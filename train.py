###############################################################
# Copyright (C) 2023, Inria
# GRAPHDECO research group, https://team.inria.fr/graphdeco
# All rights reserved.
#
# æœ¬è½¯ä»¶ä»…é™éå•†ä¸šã€ç§‘ç ”å’Œè¯„ä¼°ç”¨é€”ï¼Œå…·ä½“æ¡æ¬¾è§ LICENSE.md æ–‡ä»¶ã€‚
# å¦‚æœ‰ç–‘é—®è¯·è”ç³» george.drettakis@inria.fr
###############################################################

import os
import os.path as osp
import torch
import torch.nn.functional as F
from random import randint
import sys
from tqdm import tqdm
from argparse import ArgumentParser
import numpy as np
import yaml
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„ï¼Œå¯¼å…¥è‡ªå®šä¹‰æ¨¡å—

sys.path.append("./")
from r2_gaussian.arguments import ModelParams, OptimizationParams, PipelineParams  # å‚æ•°å®šä¹‰
from r2_gaussian.gaussian import GaussianModel, render, query, initialize_gaussian  # é«˜æ–¯æ¨¡å‹ç›¸å…³
from r2_gaussian.utils.general_utils import safe_state  # éšæœºç§å­ç­‰ç³»ç»ŸçŠ¶æ€
from r2_gaussian.utils.cfg_utils import load_config  # é…ç½®æ–‡ä»¶åŠ è½½
from r2_gaussian.utils.log_utils import prepare_output_and_logger  # æ—¥å¿—ä¸è¾“å‡º
from r2_gaussian.dataset import Scene  # æ•°æ®é›†åœºæ™¯
from r2_gaussian.utils.loss_utils import l1_loss, ssim, tv_3d_loss, loss_photometric, l1_loss_mask, depth_loss, pseudo_label_loss, depth_loss_fn, compute_graph_laplacian_loss  # æŸå¤±å‡½æ•°
from r2_gaussian.utils.depth_utils import extract_depth_from_volume_ray_casting  # æ·±åº¦æå–å‡½æ•°
from r2_gaussian.utils.warp_utils import inverse_warp  # é€†å˜å½¢å‡½æ•° - IPSMå®ç°
from r2_gaussian.utils.image_utils import metric_vol, metric_proj  # è¯„ä¼°æŒ‡æ ‡
from r2_gaussian.utils.plot_utils import show_two_slice  # å¯è§†åŒ–å·¥å…·
from r2_gaussian.utils.sghmc_optimizer import create_sss_optimizer, HybridOptimizer  # SSSä¼˜åŒ–å™¨

# FSGSä¼ªæ ‡ç­¾æ”¹è¿›æ¨¡å— (å¯é€‰ï¼Œå‘ä¸‹å…¼å®¹)
try:
    from r2_gaussian.utils.pseudo_view_utils import FSGSPseudoViewGenerator, create_fsgs_pseudo_cameras
    from r2_gaussian.utils.depth_estimator import MonocularDepthEstimator, create_depth_estimator
    # from r2_gaussian.utils.fsgs_improved import FSGSImprovedGenerator, create_improved_fsgs_pseudo_cameras
    HAS_FSGS_MODULES = True
    print("âœ… FSGS pseudo-label modules available")
except ImportError as e:
    HAS_FSGS_MODULES = False
    print(f"ğŸ“¦ FSGS modules not available: {e}")
    print("ğŸ“¦ Falling back to legacy pseudo-label implementation")

# Medical Proximity-guidedå¯†åŒ–æ¨¡å— (æ–°å¢)
try:
    from r2_gaussian.utils.realistic_proximity_guided import HighQualityMedicalProximityGuidedDensifier
    HAS_PROXIMITY_GUIDED = True
    print("âœ… Medical Proximity-guidedå¯†åŒ– modules available")
except ImportError as e:
    HAS_PROXIMITY_GUIDED = False
    print(f"ğŸ“¦ Proximity-guided modules not available: {e}")

# FSGS Proximity-guidedå¯†åŒ–æ¨¡å— (æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ - 2025-11-15)
try:
    from r2_gaussian.utils.fsgs_proximity_optimized import (
        FSGSProximityDensifierOptimized as FSGSProximityDensifier,
        add_fsgs_proximity_to_gaussian_model_optimized as add_fsgs_proximity_to_gaussian_model
    )
    HAS_FSGS_PROXIMITY = True
    print("âœ… FSGS Proximity-guided densification modules available (OPTIMIZED)")
except ImportError as e:
    HAS_FSGS_PROXIMITY = False
    print(f"ğŸ“¦ FSGS Proximity modules not available: {e}")

# ğŸŒŸğŸŒŸ FSGS å®Œæ•´ç³»ç»Ÿæ¨¡å— (å®Œæ•´å®ç° - 2025-11-15)
try:
    from r2_gaussian.utils.fsgs_complete import create_fsgs_complete_system
    from r2_gaussian.utils.fsgs_depth_renderer import FSGSDepthRenderer
    HAS_FSGS_COMPLETE = True
    print("âœ… FSGS Complete System available (Proximity + Depth Supervision + Pseudo Views)")
except ImportError as e:
    HAS_FSGS_COMPLETE = False
    print(f"ğŸ“¦ FSGS Complete System not available: {e}")


def training(
    dataset: ModelParams,
    opt: OptimizationParams,
    pipe: PipelineParams,
    tb_writer,
    testing_iterations,
    saving_iterations,
    checkpoint_iterations,
    checkpoint,
    gaussiansN=2,
    coreg=True,
    coprune=True,
    coprune_threshold=5,
    args=None,
):
    """
    è®­ç»ƒä¸»å¾ªç¯ï¼Œè´Ÿè´£é«˜æ–¯æ¨¡å‹çš„åˆå§‹åŒ–ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­ã€ç¨ å¯†åŒ–ä¸å‰ªæã€ä¿å­˜æ¨¡å‹å’Œæ–­ç‚¹ï¼Œä»¥åŠæ—¥å¿—è®°å½•ã€‚
    """
    first_iter = 0

    # åˆå§‹åŒ–æ•°æ®é›†åœºæ™¯
    scene = Scene(dataset, shuffle=False)

    # è¯»å–æ‰«æä»ªé…ç½®å’Œä½“ç´ å‚æ•°
    scanner_cfg = scene.scanner_cfg
    bbox = scene.bbox
    volume_to_world = max(scanner_cfg["sVoxel"])
    max_scale = opt.max_scale * volume_to_world if opt.max_scale else None
    densify_scale_threshold = (
        opt.densify_scale_threshold * volume_to_world
        if opt.densify_scale_threshold
        else None
    )
    scale_bound = None
    if dataset.scale_min > 0 and dataset.scale_max > 0:
        scale_bound = np.array([dataset.scale_min, dataset.scale_max]) * volume_to_world
    # æŸ¥è¯¢å‡½æ•°ï¼Œç”¨äºä½“ç´ é‡‡æ ·
    queryfunc = lambda x: query(
        x,
        scanner_cfg["offOrigin"],
        scanner_cfg["nVoxel"],
        scanner_cfg["sVoxel"],
        pipe,
    )

    # åˆå§‹åŒ–é«˜æ–¯æ¨¡å‹ (æ”¯æŒSSS)
    use_student_t = getattr(args, 'enable_sss', False)
    if use_student_t:
        print("ğŸ“ [SSS-RÂ²] Enabling Student Splatting and Scooping!")
        gaussians = GaussianModel(scale_bound, use_student_t=True)
    else:
        print("ğŸ“¦ [RÂ²] Using standard Gaussian model")
        gaussians = GaussianModel(scale_bound, use_student_t=False)
        
    initialize_gaussian(gaussians, dataset, None)
    scene.gaussians = gaussians
    gaussians.training_setup(opt)
    
    # SSS: Create hybrid optimizer if enabled
    sss_optimizer = None
    if use_student_t:
        sss_optimizer = create_sss_optimizer(gaussians, opt)
        if sss_optimizer:
            print("ğŸ”¥ [SSS-RÂ²] Created hybrid SGHMC+Adam optimizer")
    
    # FSGS Proximity-guidedå¯†åŒ–å™¨åˆå§‹åŒ– (æœ€æ–°ç‰ˆæœ¬)
    proximity_densifier = None
    enable_fsgs_proximity = dataset.enable_fsgs_proximity if hasattr(dataset, 'enable_fsgs_proximity') else False
    
    if enable_fsgs_proximity and HAS_FSGS_PROXIMITY:
        # é…ç½®FSGS proximityå‚æ•° - é’ˆå¯¹foot 3è§†è§’ä¼˜åŒ–
        proximity_threshold = dataset.proximity_threshold if hasattr(dataset, 'proximity_threshold') else 8.0
        enable_medical_constraints = dataset.enable_medical_constraints if hasattr(dataset, 'enable_medical_constraints') else True
        organ_type = dataset.proximity_organ_type if hasattr(dataset, 'proximity_organ_type') else 'foot'
        
        # ä¸ºä¸»é«˜æ–¯æ¨¡å‹æ·»åŠ FSGS proximityåŠŸèƒ½
        gaussians = add_fsgs_proximity_to_gaussian_model(
            gaussians, 
            proximity_threshold=proximity_threshold,
            enable_medical_constraints=enable_medical_constraints,
            organ_type=organ_type
        )
        print(f"ğŸŒŸ [FSGS-Proximity] Enabled for {organ_type} with threshold={proximity_threshold}")
    elif enable_fsgs_proximity:
        print("âš ï¸ [FSGS-Proximity] Module not available, falling back to standard densification")
    
    # ä¿ç•™æ—§ç‰ˆæœ¬Proximity-guidedå¯†åŒ–å™¨å…¼å®¹æ€§
    if hasattr(args, 'enable_proximity_guided') and args.enable_proximity_guided and HAS_PROXIMITY_GUIDED:
        proximity_densifier = HighQualityMedicalProximityGuidedDensifier()
        organ_type = getattr(args, 'proximity_organ_type', 'foot')
        print(f"ğŸ”¬ [Legacy Proximity-Guided] Enabling medical proximity-guided densification for {organ_type}")
    elif hasattr(args, 'enable_proximity_guided') and args.enable_proximity_guided:
        print("âš ï¸ [Legacy Proximity-Guided] Module not available, falling back to standard densification")
    
    # åˆ›å»ºé«˜æ–¯åœºå­—å…¸ - å‚è€ƒX-Gaussian-depthå®ç°
    GsDict = {}
    for i in range(gaussiansN):
        if i == 0:
            GsDict[f"gs{i}"] = gaussians
        else:
            GsDict[f"gs{i}"] = GaussianModel(scale_bound, use_student_t=use_student_t)
            initialize_gaussian(GsDict[f"gs{i}"], dataset, None)
            GsDict[f"gs{i}"].training_setup(opt)
            if use_student_t:
                print(f"ğŸ“ [SSS-RÂ²] Create gaussians{i} with Student's t distribution")
            else:
                print(f"ğŸ“¦ [RÂ²] Create gaussians{i}")
    print(f"GsDict.keys() is {GsDict.keys()}")
    
    # ğŸŒŸğŸŒŸ FSGS å®Œæ•´ç³»ç»Ÿåˆå§‹åŒ– (Proximity + Depth + Pseudo Views - 2025-11-15)
    fsgs_system = None
    enable_fsgs_complete = (
        enable_fsgs_proximity and
        HAS_FSGS_COMPLETE and
        getattr(dataset, 'enable_fsgs_depth', True)  # é»˜è®¤å¯ç”¨æ·±åº¦ç›‘ç£
    )

    if enable_fsgs_complete:
        print("\n" + "="*60)
        print("ğŸ¯ Initializing FSGS Complete System")
        print("="*60)

        try:
            # åˆ›å»º FSGS å®Œæ•´ç³»ç»Ÿ
            fsgs_system = create_fsgs_complete_system(dataset)

            # åˆå§‹åŒ–ä¼ªç›¸æœºï¼ˆåœ¨è®­ç»ƒç›¸æœºåŠ è½½åï¼‰
            train_cameras = scene.getTrainCameras()
            fsgs_system.initialize_pseudo_cameras(train_cameras)

            print("âœ… FSGS Complete System initialized successfully!")
            print("   - Proximity Unpooling: âœ…")
            print("   - Depth Supervision: âœ…" if fsgs_system.enable_depth_supervision else "   - Depth Supervision: âŒ")
            print("   - Pseudo Views: âœ…" if fsgs_system.enable_pseudo_views else "   - Pseudo Views: âŒ")
            print("="*60 + "\n")

        except Exception as e:
            print(f"âš ï¸  FSGS Complete System initialization failed: {e}")
            print("   Falling back to proximity-only mode")
            fsgs_system = None
            enable_fsgs_complete = False

    # FSGSä¼ªæ ‡ç­¾åŠŸèƒ½åˆå§‹åŒ– (å‘ä¸‹å…¼å®¹ï¼Œä»…åœ¨æœªä½¿ç”¨å®Œæ•´ç³»ç»Ÿæ—¶)
    pseudo_cameras = None
    pseudo_labels = None
    depth_estimator = None
    enable_fsgs = False  # åˆå§‹åŒ–ï¼ˆFSGS Completeæ¨¡å¼ä¸‹ä¸ä½¿ç”¨æ—§ç‰ˆæ·±åº¦ç›‘ç£ï¼‰

    if not enable_fsgs_complete:
        fsgs_generator = None
        enable_fsgs = getattr(args, 'enable_fsgs_pseudo', False) if args else False

        if dataset.multi_gaussian or dataset.pseudo_labels:
            if enable_fsgs and HAS_FSGS_MODULES:
                # é€‰æ‹©FSGSç‰ˆæœ¬: improved æˆ– original
                fsgs_version = getattr(args, 'fsgs_version', 'improved') if args else 'improved'

                # æš‚æ—¶åªä½¿ç”¨åŸç‰ˆFSGSï¼Œé¿å…å¯¼å…¥é—®é¢˜
                if fsgs_version == 'improved':
                    print("ğŸ¯ [FSGS-Original] Using original FSGS (improved temporarily disabled)...")
                    fsgs_version = 'original'

                if fsgs_version == 'original':
                    print("ğŸ¯ [FSGS-Original] Using original FSGS implementation...")

                    # åˆ›å»ºåŸç‰ˆFSGSé£æ ¼ä¼ªè§†è§’ç”Ÿæˆå™¨
                    fsgs_generator = FSGSPseudoViewGenerator(
                        noise_std=getattr(args, 'fsgs_noise_std', 0.05) if args else 0.05
                    )

                    # ç”ŸæˆåŸç‰ˆFSGSé£æ ¼ä¼ªç›¸æœº
                    pseudo_cameras = fsgs_generator.generate_pseudo_cameras(
                        scene.train_cameras,
                        num_views=dataset.num_additional_views,
                        device=gaussians._xyz.device
                    )

                    print(f"âœ… [FSGS-Original] Generated {len(pseudo_cameras)} original FSGS pseudo cameras")

                # åˆå§‹åŒ–æ·±åº¦ä¼°è®¡å™¨ (å¦‚æœéœ€è¦)
                depth_model_type = getattr(args, 'fsgs_depth_model', 'dpt_large') if args else 'dpt_large'
                if depth_model_type != 'disabled':
                    depth_estimator = create_depth_estimator(
                        model_type=depth_model_type,
                        device=gaussians._xyz.device,
                        enable_fsgs_depth=True
                    )
                    print(f"âœ… [FSGS] Depth estimator: {depth_model_type}")
                else:
                    depth_estimator = None
                    print("ğŸ“¦ [FSGS] Depth estimator disabled")

            else:
                print("ğŸ“¦ [Legacy] Using original pseudo-label implementation...")
                pseudo_cameras = scene.generate_multi_gaussian_cameras(
                    num_additional_views=dataset.num_additional_views
                )
                print(f"Generated {len(pseudo_cameras)} legacy pseudo cameras")
    # åŠ è½½æ–­ç‚¹ï¼ˆå¦‚æœ‰ï¼‰
    if checkpoint is not None:
        (model_params, first_iter) = torch.load(checkpoint)
        gaussians.restore(model_params, opt)
        print(f"Load checkpoint from {osp.basename(checkpoint)}.")

    # è®¾ç½®æŸå¤±å‡½æ•°ï¼ˆæ˜¯å¦ä½¿ç”¨ TV æŸå¤±ï¼‰
    use_tv = opt.lambda_tv > 0
    if use_tv:
        print("Use total variation loss")
        tv_vol_size = opt.tv_vol_size
        tv_vol_nVoxel = torch.tensor([tv_vol_size, tv_vol_size, tv_vol_size])
        tv_vol_sVoxel = torch.tensor(scanner_cfg["dVoxel"]) * tv_vol_nVoxel

    # è®­ç»ƒä¸»å¾ªç¯
    iter_start = torch.cuda.Event(enable_timing=True)
    iter_end = torch.cuda.Event(enable_timing=True)
    ckpt_save_path = osp.join(scene.model_path, "ckpt")
    os.makedirs(ckpt_save_path, exist_ok=True)
    viewpoint_stack = None
    progress_bar = tqdm(range(0, opt.iterations), desc="Train", leave=False)
    progress_bar.update(first_iter)
    first_iter += 1
    for iteration in range(first_iter, opt.iterations + 1):
        iter_start.record()

        # æ›´æ–°å­¦ä¹ ç‡ - ä¸ºæ¯ä¸ªé«˜æ–¯åœºæ›´æ–°
        for i in range(gaussiansN):
            GsDict[f"gs{i}"].update_learning_rate(iteration)

        # éšæœºé€‰æ‹©ä¸€ä¸ªè®­ç»ƒè§†è§’
        if not viewpoint_stack:
            viewpoint_stack = scene.getTrainCameras().copy()
        viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack) - 1))

        # ä¸ºæ¯ä¸ªé«˜æ–¯åœºæ¸²æŸ“ - å‚è€ƒX-Gaussian-depthå®ç°
        RenderDict = {}
        for i in range(gaussiansN):
            RenderDict[f"render_pkg_gs{i}"] = render(
                viewpoint_cam,
                GsDict[f'gs{i}'],
                pipe,
                enable_drop=args.enable_drop,
                drop_rate=args.drop_rate if hasattr(args, 'drop_rate') else 0.10,
                iteration=iteration,
            )
            RenderDict[f"image_gs{i}"] = RenderDict[f"render_pkg_gs{i}"]["render"]
            RenderDict[f"viewspace_point_tensor_gs{i}"] = RenderDict[f"render_pkg_gs{i}"]["viewspace_points"]
            RenderDict[f"visibility_filter_gs{i}"] = RenderDict[f"render_pkg_gs{i}"]["visibility_filter"]
            RenderDict[f"radii_gs{i}"] = RenderDict[f"render_pkg_gs{i}"]["radii"]

        # è®¡ç®—æ¯ä¸ªé«˜æ–¯åœºçš„æŸå¤±
        LossDict = {}
        gt_image = viewpoint_cam.original_image.cuda()
        
        for i in range(gaussiansN):
            LossDict[f"loss_gs{i}"] = l1_loss(RenderDict[f"image_gs{i}"], gt_image)
            
            # DSSIM æŸå¤±
        if opt.lambda_dssim > 0:
                loss_dssim = 1.0 - ssim(RenderDict[f"image_gs{i}"], gt_image)
                LossDict[f"loss_gs{i}"] += opt.lambda_dssim * loss_dssim
        
        # ååŒè®­ç»ƒ - å‚è€ƒX-Gaussian-depthå®ç°
        if coreg and gaussiansN > 1:
            for i in range(gaussiansN):
                for j in range(gaussiansN):
                    if i != j:
                        coreg_loss = l1_loss(RenderDict[f"image_gs{i}"], RenderDict[f"image_gs{j}"].detach())
                        LossDict[f"loss_gs{i}"] += coreg_loss
        
        # å¤šé«˜æ–¯è®­ç»ƒæŸå¤± - åŸå§‹ç‰ˆæœ¬ï¼ˆidentity lossï¼‰
        if dataset.multi_gaussian and pseudo_cameras is not None and gaussiansN > 1:
            for pseudo_cam in pseudo_cameras[:3]:  # é™åˆ¶æ•°é‡é¿å…è®¡ç®—è¿‡è½½
                for i in range(gaussiansN):
                    pseudo_render_pkg = render(
                        pseudo_cam,
                        GsDict[f'gs{i}'],
                        pipe,
                        enable_drop=args.enable_drop,
                        drop_rate=args.drop_rate if hasattr(args, 'drop_rate') else 0.10,
                        iteration=iteration,
                    )
                    pseudo_image = pseudo_render_pkg["render"]
                    # åŸå§‹é”™è¯¯ç‰ˆæœ¬ï¼šidentity lossï¼ˆè‡ªå·±å’Œè‡ªå·±æ¯”è¾ƒï¼‰
                    LossDict[f"loss_gs{i}"] += dataset.multi_gaussian_weight * l1_loss(pseudo_image, pseudo_image.detach())
        
        # FSGSä¼ªæ ‡ç­¾è®­ç»ƒæŸå¤± (å¯é€‰ï¼Œå‘ä¸‹å…¼å®¹)
        # FSGSå»¶è¿Ÿå¯åŠ¨: 2000æ¬¡è¿­ä»£åå¯åŠ¨ (åŸç‰ˆ: 1000æ¬¡)
        fsgs_start_iter = 2000 if enable_fsgs_proximity else 1000
        if dataset.pseudo_labels and pseudo_cameras is not None and iteration > fsgs_start_iter:
            # è·å–ä¼ªç›¸æœºå’Œå¯¹åº”çš„æœ€è¿‘çœŸå®ç›¸æœº
            pseudo_stack, closest_cam_stack = scene.getPseudoCamerasWithClosestViews(pseudo_cameras)
            
            if len(pseudo_stack) > 0:
                # åˆ›å»ºå‰¯æœ¬ï¼ˆIPSMçš„åšæ³•ï¼‰
                pseudo_stack = pseudo_stack.copy()
                closest_cam_stack = closest_cam_stack.copy()
                
                # éšæœºé€‰æ‹©ä¸€ä¸ªä¼ªç›¸æœºï¼ˆIPSMçš„dropæœºåˆ¶ï¼‰
                randint_idx = randint(0, len(pseudo_stack) - 1)
                pseudo_cam = pseudo_stack.pop(randint_idx)
                closest_cam = closest_cam_stack.pop(randint_idx)
                
                for j in range(gaussiansN):
                    # ä»ä¼ªç›¸æœºæ¸²æŸ“å›¾åƒ
                    pseudo_render_pkg = render(
                        pseudo_cam,
                        GsDict[f'gs{j}'],
                        pipe,
                    )
                    rendered_img_pseudo = pseudo_render_pkg["render"]  # (C, H, W)
                    H, W = rendered_img_pseudo.shape[1], rendered_img_pseudo.shape[2]
                    
                    # ä»ä¼ªç›¸æœºæå–æ·±åº¦å›¾ï¼ˆä½¿ç”¨ç°æœ‰çš„depthæå–æ–¹æ³•ï¼‰
                    tv_vol_center = (bbox[0] + tv_vol_sVoxel / 2) + (
                        bbox[1] - tv_vol_sVoxel - bbox[0]
                    ) * torch.rand(3)
                    vol_pred_pseudo = query(
                        GsDict[f"gs{j}"],
                        tv_vol_center,
                        tv_vol_nVoxel,
                        tv_vol_sVoxel,
                        pipe,
                    )["vol"]
                    rendered_depth_pseudo = extract_depth_from_volume_ray_casting(
                        vol_pred_pseudo,
                        pseudo_cam,
                        threshold=getattr(dataset, 'depth_threshold', 0.01)
                    )  # (H_vol, W_vol) - volumeçš„å°ºå¯¸ï¼Œä¸æ˜¯å›¾åƒå°ºå¯¸
                    
                    # ä»æœ€è¿‘çœŸå®ç›¸æœºè·å–å›¾åƒå’Œæ·±åº¦
                    closest_image_1 = closest_cam.original_image.cuda()  # (C, H_closest, W_closest)
                    closest_H, closest_W = closest_image_1.shape[1], closest_image_1.shape[2]
                    
                    # ä»æœ€è¿‘çœŸå®ç›¸æœºæå–æ·±åº¦å›¾
                    vol_pred_closest = query(
                        GsDict[f"gs{j}"],
                        tv_vol_center,
                        tv_vol_nVoxel,
                        tv_vol_sVoxel,
                        pipe,
                    )["vol"]
                    closest_depth_1 = extract_depth_from_volume_ray_casting(
                        vol_pred_closest,
                        closest_cam,
                        threshold=getattr(dataset, 'depth_threshold', 0.01)
                    )  # (H_vol, W_vol) - volumeçš„å°ºå¯¸ï¼Œä¸æ˜¯å›¾åƒå°ºå¯¸
                    
                    # ç¡®ä¿æ·±åº¦å›¾å°ºå¯¸ä¸å›¾åƒå°ºå¯¸åŒ¹é…ï¼ˆresizeæ·±åº¦å›¾åˆ°å›¾åƒå°ºå¯¸ï¼‰
                    # ä¼ªç›¸æœºæ·±åº¦å›¾resizeåˆ°ä¼ªç›¸æœºå›¾åƒå°ºå¯¸
                    pseudo_depth_H, pseudo_depth_W = rendered_depth_pseudo.shape
                    if pseudo_depth_H != H or pseudo_depth_W != W:
                        # ä½¿ç”¨åŒçº¿æ€§æ’å€¼å°†æ·±åº¦å›¾resizeåˆ°å›¾åƒå°ºå¯¸
                        rendered_depth_pseudo_resized = F.interpolate(
                            rendered_depth_pseudo.unsqueeze(0).unsqueeze(0),  # (1, 1, H_vol, W_vol)
                            size=(H, W),
                            mode='bilinear',
                            align_corners=False
                        ).squeeze(0).squeeze(0)  # (H, W)
                    else:
                        rendered_depth_pseudo_resized = rendered_depth_pseudo
                    
                    # çœŸå®ç›¸æœºæ·±åº¦å›¾resizeåˆ°çœŸå®ç›¸æœºå›¾åƒå°ºå¯¸
                    closest_depth_H, closest_depth_W = closest_depth_1.shape
                    if closest_depth_H != closest_H or closest_depth_W != closest_W:
                        closest_depth_1_resized = F.interpolate(
                            closest_depth_1.unsqueeze(0).unsqueeze(0),  # (1, 1, H_vol, W_vol)
                            size=(closest_H, closest_W),
                            mode='bilinear',
                            align_corners=False
                        ).squeeze(0).squeeze(0)  # (H_closest, W_closest)
                    else:
                        closest_depth_1_resized = closest_depth_1
                    
                    # æ„å»ºå†…å‚çŸ©é˜µï¼ˆä»FoVè®¡ç®—ï¼‰- ä½¿ç”¨ä¼ªç›¸æœºçš„å°ºå¯¸ï¼Œå› ä¸ºtarget_depthæ˜¯ä¼ªç›¸æœºçš„
                    focal_x = pseudo_cam.image_width / (2.0 * np.tan(pseudo_cam.FoVx / 2.0))
                    focal_y = pseudo_cam.image_height / (2.0 * np.tan(pseudo_cam.FoVy / 2.0))
                    intrinsic = torch.tensor([
                        [focal_x, 0, pseudo_cam.image_width / 2.0],
                        [0, focal_y, pseudo_cam.image_height / 2.0],
                        [0, 0, 1]
                    ], device=closest_image_1.device, dtype=torch.float32)
                    
                    # é€†å˜å½¢ï¼ˆinverse warpï¼‰- ä½¿ç”¨resizeåçš„æ·±åº¦å›¾
                    # æ³¨æ„ï¼šinverse_warpå†…éƒ¨ä¼šä½¿ç”¨source_imageçš„å°ºå¯¸ï¼Œæ‰€ä»¥éœ€è¦ç¡®ä¿æ·±åº¦å›¾å°ºå¯¸åŒ¹é…
                    warp_rst_1 = inverse_warp(
                        closest_image_1,
                        closest_depth_1_resized,  # ä½¿ç”¨resizeåçš„æ·±åº¦å›¾
                        rendered_depth_pseudo_resized.unsqueeze(0),  # (1, H, W) - ä½¿ç”¨resizeåçš„æ·±åº¦å›¾
                        closest_cam.world_view_transform,  # r2ä½¿ç”¨world_view_transformè€Œä¸æ˜¯extrinsic_matrix
                        pseudo_cam.world_view_transform,
                        intrinsic
                    )
                    
                    # è®¡ç®—maskedæŸå¤±ï¼ˆå®Œå…¨æŒ‰ç…§IPSMå›¾ç‰‡ä»£ç ï¼‰
                    # æ³¨æ„ï¼šmaskæ˜¯Floatç±»å‹ï¼Œä½¿ç”¨ä¹˜æ³•ä»£æ›¿ä½è¿ç®—&
                    combined_mask = (warp_rst_1["mask_warp"] * warp_rst_1["mask_depth_strict"]).unsqueeze(0)  # (1, H, W)
                    
                    warped_masked_strict_image = warp_rst_1["warped_img"] * combined_mask
                    pseudo_masked_strict_image = rendered_img_pseudo * combined_mask
                    
                    # æŸå¤±ç¼©æ”¾å› å­ï¼ˆé€æ¸å¢åŠ ï¼ŒIPSMçš„åšæ³•ï¼‰
                    loss_scale = min(iteration / 500.0, 1.0)
                    
                    # è®¡ç®—masked L1æŸå¤±
                    Ll1_masked_pseudo = l1_loss_mask(
                        pseudo_masked_strict_image,
                        warped_masked_strict_image.detach()
                    )
                    
                    # æ·»åŠ åˆ°æ€»æŸå¤±ï¼ˆä¹˜ä»¥loss_scaleï¼‰
                    LossDict[f"loss_gs{j}"] += dataset.pseudo_label_weight * loss_scale * Ll1_masked_pseudo
                    
                    # å¯é€‰ï¼šæ¯500æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡ä¿¡æ¯
                    if iteration % 500 == 0:
                        mask_valid_ratio = combined_mask.sum().item() / (H * W)
                        print(f"[IPSM Drop] Iteration {iteration}, GS{j}: masked_loss={Ll1_masked_pseudo.item():.6f}, "
                              f"loss_scale={loss_scale:.3f}, valid_mask_ratio={mask_valid_ratio:.3f}")
        
        # FSGSæ·±åº¦ç›‘ç£ (ä¼ªè§†è§’+è®­ç»ƒè§†è§’æ·±åº¦çº¦æŸ)
        if enable_fsgs and depth_estimator and depth_estimator.enabled and iteration > fsgs_start_iter:
            fsgs_depth_weight = getattr(args, 'fsgs_depth_weight', 0.05) if args else 0.05
            
            for j in range(gaussiansN):
                # 1. è®­ç»ƒè§†è§’æ·±åº¦ç›‘ç£
                try:
                    # ä¼°è®¡å½“å‰è®­ç»ƒè§†è§’çš„æ·±åº¦
                    gt_image_for_depth = gt_image.unsqueeze(0)  # [1, C, H, W]
                    estimated_depth = depth_estimator.estimate_depth(gt_image_for_depth, normalize=True)
                    
                    if estimated_depth is not None:
                        # æ¸²æŸ“å½“å‰è§†è§’çš„æ·±åº¦å›¾
                        rendered_depth = RenderDict.get(f"depth_gs{j}")
                        if rendered_depth is not None:
                            # è®¡ç®—Pearsonç›¸å…³æ€§æ·±åº¦æŸå¤±
                            depth_loss_train = depth_estimator.compute_pearson_loss(
                                rendered_depth, estimated_depth.squeeze(0)
                            )
                            LossDict[f"loss_gs{j}"] += fsgs_depth_weight * depth_loss_train
                            
                            if iteration % 500 == 0:
                                print(f"[FSGS] Iteration {iteration}, GS{j}: train_depth_loss={depth_loss_train.item():.6f}")
                                
                except Exception as e:
                    if iteration % 1000 == 0:  # å‡å°‘é”™è¯¯æ—¥å¿—é¢‘ç‡
                        print(f"Warning: FSGS train depth loss failed: {e}")
                
                # 2. ä¼ªè§†è§’æ·±åº¦ç›‘ç£ (å¦‚æœæœ‰ä¼ªç›¸æœº)
                if pseudo_cameras and len(pseudo_cameras) > 0:
                    try:
                        # éšæœºé€‰æ‹©ä¸€ä¸ªä¼ªç›¸æœºè¿›è¡Œæ·±åº¦ç›‘ç£
                        pseudo_cam = pseudo_cameras[randint(0, len(pseudo_cameras) - 1)]
                        
                        # æ¸²æŸ“ä¼ªè§†è§’
                        pseudo_render_pkg = render(pseudo_cam, GsDict[f'gs{j}'], pipe)
                        pseudo_image = pseudo_render_pkg["render"]
                        pseudo_depth = pseudo_render_pkg.get("depth")
                        
                        if pseudo_depth is not None:
                            # ä¼°è®¡ä¼ªè§†è§’æ·±åº¦
                            pseudo_image_for_depth = pseudo_image.unsqueeze(0)
                            estimated_pseudo_depth = depth_estimator.estimate_depth(pseudo_image_for_depth, normalize=True)
                            
                            if estimated_pseudo_depth is not None:
                                # è®¡ç®—ä¼ªè§†è§’æ·±åº¦æŸå¤±
                                depth_loss_pseudo = depth_estimator.compute_pearson_loss(
                                    pseudo_depth, estimated_pseudo_depth.squeeze(0)
                                )
                                LossDict[f"loss_gs{j}"] += fsgs_depth_weight * depth_loss_pseudo
                                
                                if iteration % 500 == 0:
                                    print(f"[FSGS] Iteration {iteration}, GS{j}: pseudo_depth_loss={depth_loss_pseudo.item():.6f}")
                                    
                    except Exception as e:
                        if iteration % 1000 == 0:  # å‡å°‘é”™è¯¯æ—¥å¿—é¢‘ç‡
                            print(f"Warning: FSGS pseudo depth loss failed: {e}")

        # ğŸŒŸğŸŒŸ FSGS Complete æ·±åº¦ç›‘ç£ (Proximity + Depth + Pseudo Views - 2025-11-15)
        if enable_fsgs_complete and fsgs_system is not None:
            try:
                # ä¸ºæ¯ä¸ªé«˜æ–¯åœºè®¡ç®—æ·±åº¦ç›‘ç£loss
                for i in range(gaussiansN):
                    depth_loss_dict = fsgs_system.compute_depth_loss(
                        viewpoint_cam,
                        GsDict[f'gs{i}'],
                        pipe,
                        background,
                        iteration
                    )

                    # æ·»åŠ æ·±åº¦lossåˆ°æ€»loss
                    if depth_loss_dict['depth_loss'].item() > 0:
                        LossDict[f"loss_gs{i}"] += depth_loss_dict['depth_loss']

                        # æ¯500è½®æ‰“å°ä¸€æ¬¡
                        if iteration % 500 == 0:
                            print(f"[FSGS Complete] Iteration {iteration}, GS{i}:")
                            print(f"  train_depth_loss={depth_loss_dict['train_depth_loss'].item():.6f}")
                            print(f"  pseudo_depth_loss={depth_loss_dict['pseudo_depth_loss'].item():.6f}")
                            print(f"  total_depth_loss={depth_loss_dict['depth_loss'].item():.6f}")

            except Exception as e:
                if iteration % 1000 == 0:
                    print(f"âš ï¸  [FSGS Complete] Depth loss failed: {e}")

        # DepthæŸå¤± - ä½¿ç”¨voxelizationæå–æ·±åº¦
        if dataset.enable_depth and dataset.depth_loss_weight > 0:
            for i in range(gaussiansN):
                # ä½¿ç”¨voxelizationè·å–density volume
                tv_vol_center = (bbox[0] + tv_vol_sVoxel / 2) + (
                    bbox[1] - tv_vol_sVoxel - bbox[0]
                ) * torch.rand(3)
                vol_pred = query(
                    GsDict[f"gs{i}"],
                    tv_vol_center,
                    tv_vol_nVoxel,
                    tv_vol_sVoxel,
                    pipe,
                )["vol"]
                
                # ä»volumeæå–æ·±åº¦å›¾
                depth_map = extract_depth_from_volume_ray_casting(
                    vol_pred, 
                    viewpoint_cam, 
                    threshold=dataset.depth_threshold
                )
                
                # å¦‚æœæœ‰ground truthæ·±åº¦ï¼Œè®¡ç®—æ·±åº¦æŸå¤±
                if hasattr(viewpoint_cam, 'depth_image') and viewpoint_cam.depth_image is not None:
                    gt_depth = viewpoint_cam.depth_image.cuda()
                    depth_loss_val = depth_loss_fn(
                        depth_map, 
                        gt_depth, 
                        loss_type=dataset.depth_loss_type
                    )
                    LossDict[f"loss_gs{i}"] += dataset.depth_loss_weight * depth_loss_val
                    
                # è‡ªç›‘ç£æ·±åº¦çº¦æŸï¼šè®©æ·±åº¦å¹³æ»‘ï¼Œæå‡é‡å»ºè´¨é‡
                if depth_map.shape[0] > 1 and depth_map.shape[1] > 1:
                    # è®¡ç®—æ·±åº¦å›¾ç›¸é‚»åƒç´ çš„å·®å¼‚ï¼ˆæ°´å¹³+å‚ç›´ï¼‰
                    depth_diff_h = torch.abs(depth_map[1:, :] - depth_map[:-1, :])
                    depth_diff_w = torch.abs(depth_map[:, 1:] - depth_map[:, :-1])
                    consistency_loss = (depth_diff_h.mean() + depth_diff_w.mean()) * 0.1
                    
                    # æ·»åŠ åˆ°æ€»æŸå¤±ä¸­
                    LossDict[f"loss_gs{i}"] += dataset.depth_loss_weight * consistency_loss
                    
                    # æ¯500æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡
                    if iteration % 500 == 0:
                        print(f"[æ·±åº¦çº¦æŸ] Iteration {iteration}: {consistency_loss.item():.6f}")
        
        # å›¾æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ– - å‚è€ƒCoR-GSè®ºæ–‡ï¼ˆä¸depthçº¦æŸäº’è¡¥ï¼‰
        # åœ¨depth+dropåŸºç¡€ä¸Šæ·»åŠ ï¼Œæå‡ç¨€ç–è§†è§’é‡å»ºè´¨é‡
        # æ€§èƒ½ä¼˜åŒ–ï¼šæ¯500æ¬¡è¿­ä»£è®¡ç®—ä¸€æ¬¡ï¼Œå‡å°‘è®¡ç®—é‡ï¼ˆå‚è€ƒGR-Gaussianè®ºæ–‡çš„åŠ¨æ€è¯„ä¼°ç­–ç•¥ï¼‰
        # é™ä½é¢‘ç‡ä»¥é¿å…GPUå†…å­˜é”™è¯¯ï¼ŒåŒæ—¶ä¿æŒæ­£åˆ™åŒ–æ•ˆæœ
        if dataset.enable_depth and dataset.depth_loss_weight > 0 and iteration > 5000:
            if iteration % 500 == 0:  # æ¯500æ¬¡è¿­ä»£è®¡ç®—ä¸€æ¬¡ï¼ˆé™ä½é¢‘ç‡ä»¥é¿å…GPUé”™è¯¯ï¼‰
                for i in range(gaussiansN):
                    if gaussiansN == 1:  # å•é«˜æ–¯åœºï¼ˆä½ çš„depth+dropå®éªŒä½¿ç”¨gaussiansN=1ï¼‰
                        graph_laplacian_loss = compute_graph_laplacian_loss(
                            GsDict[f"gs{i}"],
                            k=6,           # KNNé‚»å±…æ•°é‡ï¼ˆCoR-GSè®ºæ–‡æ¨èï¼‰
                            Lambda_lap=8e-4  # æ­£åˆ™åŒ–æƒé‡ï¼ˆCoR-GSè®ºæ–‡æ¨èï¼‰
                        )
                        LossDict[f"loss_gs{i}"] += graph_laplacian_loss
                        
                        # å¯é€‰ï¼šæ¯1000æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡
                        if iteration % 1000 == 0:
                            print(f"[å›¾æ‹‰æ™®æ‹‰æ–¯] Iteration {iteration}: graph_loss={graph_laplacian_loss.item():.6f}")
       
        # 3D TV æŸå¤± - ä¸ºæ¯ä¸ªé«˜æ–¯åœºè®¡ç®—
        if use_tv:
            for i in range(gaussiansN):
                # éšæœºé€‰å–ä¸€ä¸ªå°ä½“ç§¯ä¸­å¿ƒ
                tv_vol_center = (bbox[0] + tv_vol_sVoxel / 2) + (
                    bbox[1] - tv_vol_sVoxel - bbox[0]
                ) * torch.rand(3)
                vol_pred = query(
                    GsDict[f"gs{i}"],
                    tv_vol_center,
                    tv_vol_nVoxel,
                    tv_vol_sVoxel,
                    pipe,
                )["vol"]
                loss_tv = tv_3d_loss(vol_pred, reduction="mean")
                LossDict[f"loss_gs{i}"] += opt.lambda_tv * loss_tv
        
        # SSS: Add ENHANCED regularization losses for Student's t parameters
        for i in range(gaussiansN):
            if hasattr(GsDict[f"gs{i}"], 'use_student_t') and GsDict[f"gs{i}"].use_student_t:
                opacity = GsDict[f"gs{i}"].get_opacity
                nu = GsDict[f"gs{i}"].get_nu
                
                # PROGRESSIVE opacity balance: adapt target based on training phase
                if iteration < 10000:
                    # Phase 1: Strongly prefer positive (95% positive)
                    pos_target = 0.95
                    neg_penalty_weight = 10.0
                elif iteration < 20000:
                    # Phase 2: Allow some negative (85% positive)
                    pos_target = 0.85
                    neg_penalty_weight = 5.0
                else:
                    # Phase 3: More flexible (75% positive)
                    pos_target = 0.75
                    neg_penalty_weight = 2.0
                
                pos_count = (opacity > 0).float().mean()
                balance_loss = torch.abs(pos_count - pos_target)
                LossDict[f"loss_gs{i}"] += 0.003 * balance_loss
                
                # Nu regularization: encourage diversity within reasonable range
                nu_diversity_loss = -torch.std(nu) * 0.1  # Encourage diversity
                nu_range_loss = torch.mean(torch.relu(nu - 8.0)) + torch.mean(torch.relu(1.5 - nu))  # Keep in [1.5, 8]
                LossDict[f"loss_gs{i}"] += 0.001 * (nu_diversity_loss + nu_range_loss)
                
                # Adaptive negative opacity penalty
                neg_mask = opacity < 0
                if neg_mask.any():
                    extreme_neg_mask = opacity < -0.2  # Very negative values
                    if extreme_neg_mask.any():
                        extreme_penalty = torch.mean(torch.abs(opacity[extreme_neg_mask])) * neg_penalty_weight
                        LossDict[f"loss_gs{i}"] += 0.002 * extreme_penalty

        # SSS: Debug logging for ENHANCED regularization terms
        if hasattr(GsDict[f"gs0"], 'use_student_t') and GsDict[f"gs0"].use_student_t and iteration % 2000 == 0:
            opacity = GsDict[f"gs0"].get_opacity
            nu = GsDict[f"gs0"].get_nu
            pos_ratio = (opacity > 0).float().mean()
            neg_ratio = (opacity < 0).float().mean()
            nu_mean = nu.mean()
            nu_std = nu.std()
            
            # Determine current phase and targets
            if iteration < 10000:
                phase = "Early (Positive)"
                pos_target = 0.95
            elif iteration < 20000:
                phase = "Mid (Limited-Neg)"
                pos_target = 0.85
            else:
                phase = "Late (Flexible)"
                pos_target = 0.75
            
            print(f"ğŸ¯ [SSS-Enhanced] Iter {iteration} - Phase: {phase}")
            print(f"          Opacity: [{opacity.min():.3f}, {opacity.max():.3f}], Balance: {pos_ratio:.3f} pos (target: {pos_target:.2f})")
            print(f"          Nu: mean={nu_mean:.2f}, std={nu_std:.2f}, range=[{nu.min():.1f}, {nu.max():.1f}]")
            
            # Warnings based on phase
            if pos_ratio < pos_target - 0.05:
                print(f"âš ï¸  [SSS-Enhanced] Warning: {pos_ratio*100:.1f}% positive opacity (target: {pos_target*100:.0f}%)")
            
            extreme_neg = (opacity < -0.2).float().mean()
            if extreme_neg > 0.01:
                print(f"âš ï¸  [SSS-Enhanced] Warning: {extreme_neg*100:.1f}% extreme negative opacity (<-0.2)")
        
        # åå‘ä¼ æ’­ - ä¸ºæ¯ä¸ªé«˜æ–¯åœº
        for i in range(gaussiansN):
            LossDict[f"loss_gs{i}"].backward(retain_graph=(i < gaussiansN - 1))

        iter_end.record()
        torch.cuda.synchronize()

        with torch.no_grad():
            # è‡ªé€‚åº”æ§åˆ¶ï¼šæ›´æ–°é«˜æ–¯åŠå¾„å’Œç»Ÿè®¡ - ä¸ºæ¯ä¸ªé«˜æ–¯åœº
            for i in range(gaussiansN):
                viewspace_point_tensor = RenderDict[f"viewspace_point_tensor_gs{i}"]
                visibility_filter = RenderDict[f"visibility_filter_gs{i}"]
                radii = RenderDict[f"radii_gs{i}"]
                
                GsDict[f"gs{i}"].max_radii2D[visibility_filter] = torch.max(
                    GsDict[f"gs{i}"].max_radii2D[visibility_filter], radii[visibility_filter]
                )
                GsDict[f"gs{i}"].add_densification_stats(viewspace_point_tensor, visibility_filter)
            
            # é«˜æ–¯ç‚¹ç¨ å¯†åŒ–ä¸å‰ªæ - ä¸ºæ¯ä¸ªé«˜æ–¯åœº
            if iteration < opt.densify_until_iter:
                if (
                    iteration > opt.densify_from_iter
                    and iteration % opt.densification_interval == 0
                ):
                    # ğŸ”¬ Proximity-Guided Densification (åŒ»å­¦æ„ŸçŸ¥å¯†åŒ–) 
                    if (proximity_densifier is not None and 
                        hasattr(args, 'proximity_interval') and 
                        iteration % args.proximity_interval == 0):
                        
                        organ_type = getattr(args, 'proximity_organ_type', 'foot')
                        max_points = getattr(args, 'proximity_max_points', 500)
                        
                        for i in range(gaussiansN):
                            current_gaussians = GsDict[f"gs{i}"].get_xyz  # (N, 3)
                            current_opacity = GsDict[f"gs{i}"].get_opacity  # (N, 1)
                            
                            print(f"ğŸ”¬ [Proximity-Guided] Iter {iteration}: åˆ†æGS{i}çš„åŒ»å­¦åˆç†æ€§...")
                            
                            # æ‰§è¡ŒåŒ»å­¦æ„ŸçŸ¥çš„proximityå¯†åŒ–
                            densify_result = proximity_densifier.proximity_guided_densify_realistic(
                                current_gaussians, current_opacity, organ_type, max_points
                            )
                            
                            if densify_result['densified_points'] > 0:
                                new_positions = densify_result['new_positions']  # (K, 3) 
                                new_opacities = densify_result['new_opacities']  # (K, 1)
                                
                                # åˆ›å»ºæ–°é«˜æ–¯ç‚¹çš„å…¶ä»–å±æ€§ (åŸºäºè¿‘é‚»æ’å€¼)
                                device = current_gaussians.device
                                num_new = new_positions.shape[0]
                                
                                # åˆå§‹åŒ–å…¶ä»–å±æ€§
                                new_colors = torch.zeros(num_new, 3, device=device)  # RGB
                                new_rotations = torch.zeros(num_new, 4, device=device)  # å››å…ƒæ•°
                                new_rotations[:, 0] = 1.0  # wåˆ†é‡è®¾ä¸º1 (å•ä½å››å…ƒæ•°)
                                new_scales = torch.ones(num_new, 3, device=device) * 0.01  # å°å°ºåº¦
                                
                                # æ·»åŠ æ–°é«˜æ–¯ç‚¹åˆ°æ¨¡å‹
                                GsDict[f"gs{i}"].densification_postfix(
                                    new_positions, new_colors, new_rotations, new_scales, new_opacities
                                )
                                
                                print(f"âœ… [Proximity-Guided] GS{i}: æ–°å¢ {num_new} ä¸ªåŒ»å­¦åˆç†çš„é«˜æ–¯ç‚¹")
                    
                    # æ ‡å‡†å¯†åŒ–å’Œå‰ªææµç¨‹
                    for i in range(gaussiansN):
                        # SSS: Apply stricter point control for Student's t distributions
                        if hasattr(GsDict[f"gs{i}"], 'use_student_t') and GsDict[f"gs{i}"].use_student_t:
                            # Reduce max points for SSS to prevent performance issues
                            max_points_sss = min(opt.max_num_gaussians, 50000)  # Cap at 50k for SSS
                            current_points = GsDict[f"gs{i}"].get_xyz.shape[0]
                            
                            # More aggressive pruning for SSS
                            if current_points > max_points_sss * 0.8:  # Start aggressive pruning at 80% 
                                sss_grad_threshold = opt.densify_grad_threshold * 1.5  # Harder to densify
                                sss_density_threshold = opt.density_min_threshold * 0.8  # Easier to prune
                            else:
                                sss_grad_threshold = opt.densify_grad_threshold
                                sss_density_threshold = opt.density_min_threshold
                            
                            print(f"ğŸ“ [SSS-Control] Iter {iteration}: GS{i} has {current_points} points (max: {max_points_sss})")
                            
                            # ä½¿ç”¨å¢å¼ºç‰ˆå¯†åŒ–å‡½æ•° (FSGS proximity-guided)
                            if hasattr(GsDict[f"gs{i}"], 'enhanced_densify_and_prune'):
                                GsDict[f"gs{i}"].enhanced_densify_and_prune(
                                    sss_grad_threshold,
                                    sss_density_threshold,
                                    opt.max_screen_size,
                                    max_scale,
                                    max_points_sss,  # Use SSS-specific limit
                                    densify_scale_threshold,
                                    bbox,
                                    enable_proximity_densify=enable_fsgs_proximity,
                                )
                            else:
                                # å›é€€åˆ°æ ‡å‡†å¯†åŒ–
                                GsDict[f"gs{i}"].densify_and_prune(
                                    sss_grad_threshold,
                                    sss_density_threshold,
                                    opt.max_screen_size,
                                    max_scale,
                                    max_points_sss,  # Use SSS-specific limit
                                    densify_scale_threshold,
                                    bbox,
                                )
                        else:
                            # Standard densification for non-SSS gaussians
                            # ä½¿ç”¨å¢å¼ºç‰ˆå¯†åŒ–å‡½æ•° (FSGS proximity-guided)
                            if hasattr(GsDict[f"gs{i}"], 'enhanced_densify_and_prune'):
                                GsDict[f"gs{i}"].enhanced_densify_and_prune(
                                    opt.densify_grad_threshold,
                                    opt.density_min_threshold,
                                    opt.max_screen_size,
                                    max_scale,
                                    opt.max_num_gaussians,
                                    densify_scale_threshold,
                                    bbox,
                                    enable_proximity_densify=enable_fsgs_proximity,
                                )
                            else:
                                # å›é€€åˆ°æ ‡å‡†å¯†åŒ–
                                GsDict[f"gs{i}"].densify_and_prune(
                                    opt.densify_grad_threshold,
                                    opt.density_min_threshold,
                                    opt.max_screen_size,
                                    max_scale,
                                    opt.max_num_gaussians,
                                    densify_scale_threshold,
                                    bbox,
                                )
            
            # Density decayåŠŸèƒ½ - åœ¨densificationå¼€å§‹åå¯¹å¯†åº¦è¿›è¡Œè¡°å‡
            if dataset.opacity_decay and iteration > opt.densify_from_iter:
                opt.densify_until_iter = opt.iterations
                for i in range(gaussiansN):
                    GsDict[f"gs{i}"].density_decay(factor=0.995)
            
            # æ£€æŸ¥é«˜æ–¯åœºæ˜¯å¦ä¸ºç©º
            for i in range(gaussiansN):
                if GsDict[f"gs{i}"].get_density.shape[0] == 0:
                    raise ValueError(
                        f"No Gaussian left in gs{i}. Change adaptive control hyperparameters!"
                    )

            # ä¼˜åŒ–å™¨æ›´æ–° - ä¸ºæ¯ä¸ªé«˜æ–¯åœº
            if iteration < opt.iterations:
                for i in range(gaussiansN):
                    # SSS: Apply ADAPTIVE gradient clipping for enhanced stability
                    if hasattr(GsDict[f"gs{i}"], 'use_student_t') and GsDict[f"gs{i}"].use_student_t:
                        # Adaptive clipping based on training phase
                        if iteration < 10000:
                            # Phase 1: Very conservative
                            nu_clip_norm = 0.3
                            opacity_clip_norm = 0.8
                        elif iteration < 20000:
                            # Phase 2: Moderate
                            nu_clip_norm = 0.5
                            opacity_clip_norm = 1.2
                        else:
                            # Phase 3: More flexible
                            nu_clip_norm = 0.8
                            opacity_clip_norm = 1.5
                        
                        if hasattr(GsDict[f"gs{i}"], '_nu') and GsDict[f"gs{i}"]._nu.grad is not None:
                            torch.nn.utils.clip_grad_norm_(GsDict[f"gs{i}"]._nu, max_norm=nu_clip_norm)
                        if hasattr(GsDict[f"gs{i}"], '_opacity') and GsDict[f"gs{i}"]._opacity.grad is not None:
                            torch.nn.utils.clip_grad_norm_(GsDict[f"gs{i}"]._opacity, max_norm=opacity_clip_norm)
                        # Standard position gradient clipping
                        if GsDict[f"gs{i}"]._xyz.grad is not None:
                            torch.nn.utils.clip_grad_norm_(GsDict[f"gs{i}"]._xyz, max_norm=2.0)
                    
                    GsDict[f"gs{i}"].optimizer.step()
                    GsDict[f"gs{i}"].optimizer.zero_grad(set_to_none=True)

            # ä¿å­˜é«˜æ–¯æ¨¡å‹
            if iteration in saving_iterations or iteration == opt.iterations:
                tqdm.write(f"[ITER {iteration}] Saving Gaussians")
                scene.save(iteration, queryfunc)
                
                # ä¿å­˜é¢å¤–çš„é«˜æ–¯åœº
                if gaussiansN > 1:
                    for i in range(1, gaussiansN):
                        pcd_path = osp.join(scene.model_path, f"point_cloud_gs{i}/iteration_{iteration}")
                        os.makedirs(pcd_path, exist_ok=True)
                        GsDict[f"gs{i}"].save_ply(osp.join(pcd_path, "point_cloud.ply"))

            # ä¿å­˜æ–­ç‚¹
            if iteration in checkpoint_iterations:
                tqdm.write(f"[ITER {iteration}] Saving Checkpoint")
                checkpoint_data = {}
                for i in range(gaussiansN):
                    checkpoint_data[f"gs{i}"] = GsDict[f"gs{i}"].capture()
                checkpoint_data["iteration"] = iteration
                torch.save(checkpoint_data, ckpt_save_path + "/chkpnt" + str(iteration) + ".pth")

            # è¿›åº¦æ¡æ˜¾ç¤º
            if iteration % 10 == 0:
                # è®¡ç®—æ€»æŸå¤±å’Œæ€»ç‚¹æ•°
                total_loss = sum(LossDict[f"loss_gs{i}"].item() for i in range(gaussiansN))
                total_points = sum(GsDict[f"gs{i}"].get_density.shape[0] for i in range(gaussiansN))
                
                progress_bar.set_postfix(
                    {
                        "loss": f"{total_loss:.1e}",
                        "pts": f"{total_points:2.1e}",
                        "gs": f"{gaussiansN}",
                    }
                )
                progress_bar.update(10)
            if iteration == opt.iterations:
                progress_bar.close()

            # æ—¥å¿—è®°å½•ä¸è¯„ä¼°
            metrics = {}
            for i in range(gaussiansN):
                metrics[f"loss_gs{i}"] = LossDict[f"loss_gs{i}"].item()
                for param_group in GsDict[f"gs{i}"].optimizer.param_groups:
                    metrics[f"lr_gs{i}_{param_group['name']}"] = param_group["lr"]
            training_report(
                tb_writer,
                iteration,
                metrics,
                iter_start.elapsed_time(iter_end),
                testing_iterations,
                scene,
                lambda x, y, it=iteration: render(
                    x,
                    y,
                    pipe,
                    enable_drop=args.enable_drop,
                    drop_rate=args.drop_rate if hasattr(args, 'drop_rate') else 0.10,
                    iteration=it,
                ),
                queryfunc,
                gaussiansN,
            )


def training_report(
    tb_writer,
    iteration,
    metrics_train,
    elapsed,
    testing_iterations,
    scene: Scene,
    renderFunc,
    queryFunc,
    gaussiansN=1,
):
    """
    è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯„ä¼°ä¸æ—¥å¿—è®°å½•ï¼ŒåŒ…æ‹¬è®­ç»ƒç»Ÿè®¡ã€2Dæ¸²æŸ“æ€§èƒ½ã€3Dé‡å»ºæ€§èƒ½ç­‰ã€‚
    """
    # è®°å½•è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
    if tb_writer:
        for key in list(metrics_train.keys()):
            tb_writer.add_scalar(f"train/{key}", metrics_train[key], iteration)
        tb_writer.add_scalar("train/iter_time", elapsed, iteration)
        tb_writer.add_scalar(
            "train/total_points", scene.gaussians.get_xyz.shape[0], iteration
        )

    # æµ‹è¯•ä¸è¯„ä¼°
    if iteration in testing_iterations:
        # 2Dæ¸²æŸ“æ€§èƒ½è¯„ä¼°
        eval_save_path = osp.join(scene.model_path, "eval", f"iter_{iteration:06d}")
        os.makedirs(eval_save_path, exist_ok=True)
        torch.cuda.empty_cache()

        validation_configs = [
            {"name": "render_train", "cameras": scene.getTrainCameras()},
            {"name": "render_test", "cameras": scene.getTestCameras()},
        ]
        psnr_2d, ssim_2d = None, None
        for config in validation_configs:
            if config["cameras"] and len(config["cameras"]) > 0:
                images = []
                gt_images = []
                image_show_2d = []
                # æ¸²æŸ“æ‰€æœ‰è§†è§’
                show_idx = np.linspace(0, len(config["cameras"]), 7).astype(int)[1:-1]
                for idx, viewpoint in enumerate(config["cameras"]):
                    image = renderFunc(
                        viewpoint,
                        scene.gaussians,
                    )["render"]
                    gt_image = viewpoint.original_image.to("cuda")
                    images.append(image)
                    gt_images.append(gt_image)
                    
                    # ä¿å­˜å•ç‹¬çš„æ¸²æŸ“å›¾åƒï¼ˆPNGæ ¼å¼ï¼‰
                    if idx in show_idx:
                        # åˆ›å»ºå¯è§†åŒ–è¾“å‡ºç›®å½•
                        vis_output_dir = osp.join(eval_save_path, "render_images")
                        os.makedirs(vis_output_dir, exist_ok=True)
                        
                        # ä¿å­˜GTå›¾åƒ
                        gt_img_np = gt_image[0].detach().cpu().numpy()
                        gt_img_np = np.clip(gt_img_np, 0, 1) * 255
                        gt_save_path = osp.join(vis_output_dir, f"{viewpoint.image_name}_gt.png")
                        plt.imsave(gt_save_path, gt_img_np, cmap='viridis')
                        
                        # ä¿å­˜æ¸²æŸ“å›¾åƒ  
                        render_img_np = image[0].detach().cpu().numpy()
                        render_img_np = np.clip(render_img_np, 0, 1) * 255
                        render_save_path = osp.join(vis_output_dir, f"{viewpoint.image_name}_render.png")
                        plt.imsave(render_save_path, render_img_np, cmap='viridis')
                        
                        # ä¿å­˜å¯¹æ¯”å›¾ï¼ˆå·®å¼‚å›¾ï¼‰
                        diff_img = np.abs(gt_img_np - render_img_np)
                        diff_save_path = osp.join(vis_output_dir, f"{viewpoint.image_name}_diff.png")
                        plt.imsave(diff_save_path, diff_img, cmap='hot')
                        
                        print(f"ğŸ’¾ ä¿å­˜æ¸²æŸ“å›¾åƒ: {viewpoint.image_name} åˆ° {vis_output_dir}")
                    
                    if tb_writer and idx in show_idx:
                        image_show_2d.append(
                            torch.from_numpy(
                                show_two_slice(
                                    gt_image[0],
                                    image[0],
                                    f"{viewpoint.image_name} gt",
                                    f"{viewpoint.image_name} render",
                                    vmin=gt_image[0].min() if iteration != 1 else None,
                                    vmax=gt_image[0].max() if iteration != 1 else None,
                                    save=True,
                                )
                            )
                        )
                images = torch.concat(images, 0).permute(1, 2, 0)
                gt_images = torch.concat(gt_images, 0).permute(1, 2, 0)
                psnr_2d, psnr_2d_projs = metric_proj(gt_images, images, "psnr")
                ssim_2d, ssim_2d_projs = metric_proj(gt_images, images, "ssim")
                eval_dict_2d = {
                    "psnr_2d": psnr_2d,
                    "ssim_2d": ssim_2d,
                    "psnr_2d_projs": psnr_2d_projs,
                    "ssim_2d_projs": ssim_2d_projs,
                }
                with open(
                    osp.join(eval_save_path, f"eval2d_{config['name']}.yml"),
                    "w",
                ) as f:
                    yaml.dump(
                        eval_dict_2d, f, default_flow_style=False, sort_keys=False
                    )

                if tb_writer:
                    image_show_2d = torch.from_numpy(
                        np.concatenate(image_show_2d, axis=0)
                    )[None].permute([0, 3, 1, 2])
                    tb_writer.add_images(
                        config["name"] + f"/{viewpoint.image_name}",
                        image_show_2d,
                        global_step=iteration,
                    )
                    tb_writer.add_scalar(
                        config["name"] + "/psnr_2d", psnr_2d, iteration
                    )
                    tb_writer.add_scalar(
                        config["name"] + "/ssim_2d", ssim_2d, iteration
                    )

        # 3Dé‡å»ºæ€§èƒ½è¯„ä¼°
        vol_pred = queryFunc(scene.gaussians)["vol"]
        vol_gt = scene.vol_gt
        psnr_3d, _ = metric_vol(vol_gt, vol_pred, "psnr")
        ssim_3d, ssim_3d_axis = metric_vol(vol_gt, vol_pred, "ssim")
        eval_dict = {
            "psnr_3d": psnr_3d,
            "ssim_3d": ssim_3d,
            "ssim_3d_x": ssim_3d_axis[0],
            "ssim_3d_y": ssim_3d_axis[1],
            "ssim_3d_z": ssim_3d_axis[2],
        }
        with open(osp.join(eval_save_path, "eval3d.yml"), "w") as f:
            yaml.dump(eval_dict, f, default_flow_style=False, sort_keys=False)
        if tb_writer:
            image_show_3d = np.concatenate(
                [
                    show_two_slice(
                        vol_gt[..., i],
                        vol_pred[..., i],
                        f"slice {i} gt",
                        f"slice {i} pred",
                        vmin=vol_gt[..., i].min(),
                        vmax=vol_gt[..., i].max(),
                        save=True,
                    )
                    for i in np.linspace(0, vol_gt.shape[2], 7).astype(int)[1:-1]
                ],
                axis=0,
            )
            image_show_3d = torch.from_numpy(image_show_3d)[None].permute([0, 3, 1, 2])
            tb_writer.add_images(
                "reconstruction/slice-gt_pred_diff",
                image_show_3d,
                global_step=iteration,
            )
            tb_writer.add_scalar("reconstruction/psnr_3d", psnr_3d, iteration)
            tb_writer.add_scalar("reconstruction/ssim_3d", ssim_3d, iteration)
        tqdm.write(
            f"[ITER {iteration}] Evaluating: psnr3d {psnr_3d:.4f}, ssim3d {ssim_3d:.4f}, psnr2d {psnr_2d:.4f}, ssim2d {ssim_2d:.4f}"
        )

        # è®°å½•å…¶ä»–æŒ‡æ ‡
        if tb_writer:
            tb_writer.add_histogram(
                "scene/density_histogram", scene.gaussians.get_density, iteration
            )

    torch.cuda.empty_cache()


if __name__ == "__main__":
    # å‘½ä»¤è¡Œå…¥å£ï¼Œå‚æ•°è§£æä¸è®­ç»ƒå¯åŠ¨
    # fmt: off
    # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = ArgumentParser(description="Training script parameters") 
    lp = ModelParams(parser)
    op = OptimizationParams(parser)
    pp = PipelineParams(parser)
    parser.add_argument("--detect_anomaly", action="store_true", default=False)  # æ˜¯å¦å¼€å¯å¼‚å¸¸æ£€æµ‹
    parser.add_argument("--test_iterations", nargs="+", type=int, default=[5_000, 10_000, 20_000])  # æµ‹è¯•è¿­ä»£
    parser.add_argument("--save_iterations", nargs="+", type=int, default=[])  # ä¿å­˜è¿­ä»£
    parser.add_argument("--quiet", action="store_true")  # é™é»˜æ¨¡å¼
    parser.add_argument("--checkpoint_iterations", nargs="+", type=int, default=[])  # æ–­ç‚¹ä¿å­˜è¿­ä»£
    parser.add_argument("--start_checkpoint", type=str, default=None)  # èµ·å§‹æ–­ç‚¹
    parser.add_argument("--config", type=str, default=None)  # é…ç½®æ–‡ä»¶è·¯å¾„
    parser.add_argument("--enable_drop", action="store_true", default=False)  # æ˜¯å¦å¯ç”¨ drop æ–¹æ³•
    parser.add_argument("--drop_rate", type=float, default=0.10)  # drop æ¯”ä¾‹ï¼ˆ0~1ï¼‰
    
    # SSS: Student Splatting and Scooping å‚æ•°
    parser.add_argument("--enable_sss", action="store_true", default=False)  # æ˜¯å¦å¯ç”¨SSS
    parser.add_argument("--sghmc_friction", type=float, default=0.1)  # SGHMCæ‘©æ“¦ç³»æ•°
    parser.add_argument("--sghmc_burnin_steps", type=int, default=1000)  # SGHMCçƒ§å…¥æ­¥æ•°
    parser.add_argument("--nu_lr_init", type=float, default=0.001)  # nuå‚æ•°åˆå§‹å­¦ä¹ ç‡
    parser.add_argument("--opacity_lr_init", type=float, default=0.01)  # opacityå‚æ•°åˆå§‹å­¦ä¹ ç‡
    
    # FSGS Proximity-Guided Densification å‚æ•°åœ¨arguments/__init__.pyä¸­å·²å®šä¹‰
    
    # æ—§ç‰ˆæœ¬ Proximity-Guided Densification å‚æ•° (å…¼å®¹æ€§ä¿ç•™)
    parser.add_argument("--enable_proximity_guided", action="store_true", default=False)  # æ˜¯å¦å¯ç”¨æ—§ç‰ˆproximity-guidedå¯†åŒ–
    parser.add_argument("--proximity_interval", type=int, default=1000)  # proximityå¯†åŒ–é—´éš”
    parser.add_argument("--proximity_max_points", type=int, default=500)  # æ¯æ¬¡proximityå¯†åŒ–æœ€å¤§ç‚¹æ•°
    args = parser.parse_args(sys.argv[1:])
    args.save_iterations.append(args.iterations)
    args.test_iterations.append(args.iterations)
    args.test_iterations.append(1)
    # fmt: on

    # åˆå§‹åŒ–ç³»ç»ŸçŠ¶æ€ï¼ˆå¦‚éšæœºç§å­ï¼‰
    safe_state(args.quiet)

    # åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœ‰ï¼‰
    args_dict = vars(args)
    if args.config is not None:
        print(f"Loading configuration file from {args.config}")
        cfg = load_config(args.config)
        for key in list(cfg.keys()):
            args_dict[key] = cfg[key]

    # è®¾ç½®æ—¥å¿—ä¸è¾“å‡º
    tb_writer = prepare_output_and_logger(args)

    print("Optimizing " + args.model_path)

    # æ˜¯å¦å¼€å¯å¼‚å¸¸æ£€æµ‹
    torch.autograd.set_detect_anomaly(args.detect_anomaly)
    # å¯åŠ¨è®­ç»ƒä¸»å¾ªç¯
    training(
        lp.extract(args),
        op.extract(args),
        pp.extract(args),
        tb_writer,
        args.test_iterations,
        args.save_iterations,
        args.checkpoint_iterations,
        args.start_checkpoint,
        args.gaussiansN,
        args.coreg,
        args.coprune,
        args.coprune_threshold,
        args,
    )
    
    # æ³¨æ„ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ render å‡½æ•°æ—¶ï¼Œéœ€è¦ä¼ é€’ enable_drop=args.enable_drop å‚æ•°

    # è®­ç»ƒç»“æŸ
    print("Training complete.")
