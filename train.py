###############################################################
# Copyright (C) 2023, Inria
# GRAPHDECO research group, https://team.inria.fr/graphdeco
# All rights reserved.
#
# æœ¬è½¯ä»¶ä»…é™éå•†ä¸šã€ç§‘ç ”å’Œè¯„ä¼°ç”¨é€”ï¼Œå…·ä½“æ¡æ¬¾è§ LICENSE.md æ–‡ä»¶ã€‚
# å¦‚æœ‰ç–‘é—®è¯·è”ç³» george.drettakis@inria.fr
###############################################################

import os
import os.path as osp
import torch
import torch.nn.functional as F
from random import randint
import sys
from tqdm import tqdm
from argparse import ArgumentParser
import numpy as np
import yaml
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„ï¼Œå¯¼å…¥è‡ªå®šä¹‰æ¨¡å—

sys.path.append("./")
from r2_gaussian.arguments import ModelParams, OptimizationParams, PipelineParams  # å‚æ•°å®šä¹‰
from r2_gaussian.gaussian import GaussianModel, render, query, initialize_gaussian  # é«˜æ–¯æ¨¡å‹ç›¸å…³
from r2_gaussian.utils.general_utils import safe_state  # éšæœºç§å­ç­‰ç³»ç»ŸçŠ¶æ€
from r2_gaussian.utils.cfg_utils import load_config  # é…ç½®æ–‡ä»¶åŠ è½½
from r2_gaussian.utils.log_utils import prepare_output_and_logger  # æ—¥å¿—ä¸è¾“å‡º
from r2_gaussian.dataset import Scene  # æ•°æ®é›†åœºæ™¯
from r2_gaussian.utils.loss_utils import l1_loss, ssim, tv_3d_loss, loss_photometric, l1_loss_mask, depth_loss, pseudo_label_loss, depth_loss_fn, compute_graph_laplacian_loss  # æŸå¤±å‡½æ•°
from r2_gaussian.utils.depth_utils import extract_depth_from_volume_ray_casting  # æ·±åº¦æå–å‡½æ•°
from r2_gaussian.utils.warp_utils import inverse_warp  # é€†å˜å½¢å‡½æ•° - IPSMå®ç°
from r2_gaussian.utils.image_utils import metric_vol, metric_proj  # è¯„ä¼°æŒ‡æ ‡
from r2_gaussian.utils.plot_utils import show_two_slice  # å¯è§†åŒ–å·¥å…·
from r2_gaussian.utils.sghmc_optimizer import create_sss_optimizer, HybridOptimizer  # SSSä¼˜åŒ–å™¨

# FSGSä¼ªæ ‡ç­¾æ”¹è¿›æ¨¡å— (å¯é€‰ï¼Œå‘ä¸‹å…¼å®¹)
try:
    from r2_gaussian.utils.pseudo_view_utils import FSGSPseudoViewGenerator, create_fsgs_pseudo_cameras
    from r2_gaussian.utils.depth_estimator import MonocularDepthEstimator, create_depth_estimator
    # from r2_gaussian.utils.fsgs_improved import FSGSImprovedGenerator, create_improved_fsgs_pseudo_cameras
    HAS_FSGS_MODULES = True
    print("âœ… FSGS pseudo-label modules available")
except ImportError as e:
    HAS_FSGS_MODULES = False
    print(f"ğŸ“¦ FSGS modules not available: {e}")
    print("ğŸ“¦ Falling back to legacy pseudo-label implementation")

# Medical Proximity-guidedå¯†åŒ–æ¨¡å— (æ–°å¢)
try:
    from r2_gaussian.utils.realistic_proximity_guided import HighQualityMedicalProximityGuidedDensifier
    HAS_PROXIMITY_GUIDED = True
    print("âœ… Medical Proximity-guidedå¯†åŒ– modules available")
except ImportError as e:
    HAS_PROXIMITY_GUIDED = False
    print(f"ğŸ“¦ Proximity-guided modules not available: {e}")

# FSGS Proximity-guidedå¯†åŒ–æ¨¡å— (æ€§èƒ½ä¼˜åŒ–ç‰ˆæœ¬ - 2025-11-15)
try:
    from r2_gaussian.utils.fsgs_proximity_optimized import (
        FSGSProximityDensifierOptimized as FSGSProximityDensifier,
        add_fsgs_proximity_to_gaussian_model_optimized as add_fsgs_proximity_to_gaussian_model
    )
    HAS_FSGS_PROXIMITY = True
    print("âœ… FSGS Proximity-guided densification modules available (OPTIMIZED)")
except ImportError as e:
    HAS_FSGS_PROXIMITY = False
    print(f"ğŸ“¦ FSGS Proximity modules not available: {e}")

# ğŸŒŸğŸŒŸ FSGS å®Œæ•´ç³»ç»Ÿæ¨¡å— (å®Œï¿½ï¿½å®ç° - 2025-11-15)
try:
    from r2_gaussian.utils.fsgs_complete import create_fsgs_complete_system
    from r2_gaussian.utils.fsgs_depth_renderer import FSGSDepthRenderer
    HAS_FSGS_COMPLETE = True
    print("âœ… FSGS Complete System available (Proximity + Depth Supervision + Pseudo Views)")
except ImportError as e:
    HAS_FSGS_COMPLETE = False
    print(f"ğŸ“¦ FSGS Complete System not available: {e}")

# CoR-GS Stage 3 - Pseudo-view Co-regularization æ¨¡å— (2025-11-17)
try:
    from r2_gaussian.utils.pseudo_view_coreg import (
        generate_pseudo_view_medical,
        generate_random_pseudo_cameras,
        compute_pseudo_coreg_loss_medical
    )
    HAS_PSEUDO_COREG = True
    print("âœ… CoR-GS Stage 3 (Pseudo-view Co-regularization) modules available")
except ImportError as e:
    HAS_PSEUDO_COREG = False
    print(f"ğŸ“¦ CoR-GS Stage 3 modules not available: {e}")
    print("ğŸ“¦ Falling back to baseline training (no pseudo-view co-regularization)")


def training(
    dataset: ModelParams,
    opt: OptimizationParams,
    pipe: PipelineParams,
    tb_writer,
    testing_iterations,
    saving_iterations,
    checkpoint_iterations,
    checkpoint,
    gaussiansN=2,
    coreg=True,
    coprune=True,
    coprune_threshold=5,
    args=None,
):
    """
    è®­ç»ƒä¸»å¾ªç¯ï¼Œè´Ÿè´£é«˜æ–¯æ¨¡å‹çš„åˆå§‹åŒ–ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­ã€ç¨ å¯†åŒ–ä¸å‰ªæã€ä¿å­˜æ¨¡å‹å’Œæ–­ç‚¹ï¼Œä»¥åŠæ—¥å¿—è®°å½•ã€‚
    """
    first_iter = 0

    # åˆå§‹åŒ–æ•°æ®é›†åœºæ™¯
    scene = Scene(dataset, shuffle=False)

    # è¯»å–æ‰«æä»ªé…ç½®å’Œä½“ç´ å‚æ•°
    scanner_cfg = scene.scanner_cfg
    bbox = scene.bbox
    volume_to_world = max(scanner_cfg["sVoxel"])
    max_scale = opt.max_scale * volume_to_world if opt.max_scale else None
    densify_scale_threshold = (
        opt.densify_scale_threshold * volume_to_world
        if opt.densify_scale_threshold
        else None
    )
    scale_bound = None
    if dataset.scale_min > 0 and dataset.scale_max > 0:
        scale_bound = np.array([dataset.scale_min, dataset.scale_max]) * volume_to_world
    # æŸ¥è¯¢å‡½æ•°ï¼Œç”¨äºä½“ç´ é‡‡æ ·
    queryfunc = lambda x: query(
        x,
        scanner_cfg["offOrigin"],
        scanner_cfg["nVoxel"],
        scanner_cfg["sVoxel"],
        pipe,
    )

    # âŒ ç¦ç”¨ SSSï¼ˆä¸ç¡®å®šå®ç°æ˜¯å¦æ­£ç¡®ï¼‰
    # åˆå§‹åŒ–æ ‡å‡†é«˜æ–¯æ¨¡å‹
    use_student_t = False  # å¼ºåˆ¶ç¦ç”¨ SSS
    print("ğŸ“¦ [RÂ²] Using standard Gaussian model (SSS disabled)")
    gaussians = GaussianModel(scale_bound, use_student_t=False)

    initialize_gaussian(gaussians, dataset, None)
    scene.gaussians = gaussians
    gaussians.training_setup(opt)

    # âŒ ç¦ç”¨ SSS optimizer
    sss_optimizer = None

    # âŒ ç¦ç”¨ GR-Gaussianï¼ˆä¸ç¡®å®šå®ç°æ˜¯å¦æ­£ç¡®ï¼‰
    gr_graph = None
    print("âš ï¸ [RÂ²] Graph Regularization disabled (focus on FSGS)")
    
    # FSGS Proximity-guidedå¯†åŒ–å™¨åˆå§‹åŒ– (æœ€æ–°ç‰ˆæœ¬)
    proximity_densifier = None
    # âœ… ä¿®å¤ï¼šæ”¯æŒä»å‘½ä»¤è¡Œå‚æ•°å’Œ dataset ä¸¤ä¸ªæ¥æºæ¿€æ´» FSGS
    enable_fsgs_proximity = (
        getattr(args, 'enable_fsgs_proximity', False) or
        (hasattr(dataset, 'enable_fsgs_proximity') and dataset.enable_fsgs_proximity)
    )

    if enable_fsgs_proximity and HAS_FSGS_PROXIMITY:
        # é…ç½®FSGS proximityå‚æ•° - é’ˆå¯¹foot 3è§†è§’ä¼˜åŒ–
        proximity_threshold = dataset.proximity_threshold if hasattr(dataset, 'proximity_threshold') else 8.0
        enable_medical_constraints = dataset.enable_medical_constraints if hasattr(dataset, 'enable_medical_constraints') else True
        organ_type = dataset.proximity_organ_type if hasattr(dataset, 'proximity_organ_type') else 'foot'
        
        # ä¸ºä¸»é«˜æ–¯æ¨¡å‹æ·»åŠ FSGS proximityåŠŸèƒ½
        gaussians = add_fsgs_proximity_to_gaussian_model(
            gaussians, 
            proximity_threshold=proximity_threshold,
            enable_medical_constraints=enable_medical_constraints,
            organ_type=organ_type
        )
        print(f"ğŸŒŸ [FSGS-Proximity] Enabled for {organ_type} with threshold={proximity_threshold}")
    elif enable_fsgs_proximity:
        print("âš ï¸ [FSGS-Proximity] Module not available, falling back to standard densification")
    
    # ä¿ç•™æ—§ç‰ˆæœ¬Proximity-guidedå¯†åŒ–å™¨å…¼å®¹æ€§
    if hasattr(args, 'enable_proximity_guided') and args.enable_proximity_guided and HAS_PROXIMITY_GUIDED:
        proximity_densifier = HighQualityMedicalProximityGuidedDensifier()
        organ_type = getattr(args, 'proximity_organ_type', 'foot')
        print(f"ğŸ”¬ [Legacy Proximity-Guided] Enabling medical proximity-guided densification for {organ_type}")
    elif hasattr(args, 'enable_proximity_guided') and args.enable_proximity_guided:
        print("âš ï¸ [Legacy Proximity-Guided] Module not available, falling back to standard densification")
    
    # åˆ›å»ºé«˜æ–¯åœºå­—å…¸ - å‚è€ƒX-Gaussian-depthå®ç°
    GsDict = {}
    for i in range(gaussiansN):
        if i == 0:
            GsDict[f"gs{i}"] = gaussians
        else:
            GsDict[f"gs{i}"] = GaussianModel(scale_bound, use_student_t=use_student_t)
            initialize_gaussian(GsDict[f"gs{i}"], dataset, None)
            GsDict[f"gs{i}"].training_setup(opt)
            if use_student_t:
                print(f"ğŸ“ [SSS-RÂ²] Create gaussians{i} with Student's t distribution")
            else:
                print(f"ğŸ“¦ [RÂ²] Create gaussians{i}")
    print(f"GsDict.keys() is {GsDict.keys()}")
    
    # ğŸŒŸğŸŒŸ FSGS å®Œæ•´ç³»ç»Ÿåˆå§‹åŒ– (Proximity + Depth + Pseudo Views - 2025-11-15)
    fsgs_system = None
    # ğŸ”§ FIX: ä¸ä¾èµ– dataset å±æ€§,ç›´æ¥ä½¿ç”¨ args
    enable_fsgs_complete = (
        enable_fsgs_proximity and
        HAS_FSGS_COMPLETE and
        (getattr(args, 'enable_fsgs_proximity', False) or  # æœ‰ proximity å°±å¯ç”¨
         getattr(args, 'enable_fsgs_pseudo_views', False))  # æˆ–æœ‰ pseudo views
    )

    if enable_fsgs_complete:
        print("\n" + "="*60)
        print("ğŸ¯ Initializing FSGS Complete System")
        print("="*60)

        try:
            # åˆ›å»º FSGS å®Œæ•´ç³»ç»Ÿ
            fsgs_system = create_fsgs_complete_system(dataset)

            # åˆå§‹åŒ–ä¼ªç›¸æœºï¼ˆåœ¨è®­ç»ƒç›¸æœºåŠ è½½åï¼‰
            train_cameras = scene.getTrainCameras()
            fsgs_system.initialize_pseudo_cameras(train_cameras)

            print("âœ… FSGS Complete System initialized successfully!")
            print("   - Proximity Unpooling: âœ…")
            print("   - Depth Supervision: âœ…" if fsgs_system.enable_depth_supervision else "   - Depth Supervision: âŒ")
            print("   - Pseudo Views: âœ…" if fsgs_system.enable_pseudo_views else "   - Pseudo Views: âŒ")
            print("="*60 + "\n")

        except Exception as e:
            print(f"âš ï¸  FSGS Complete System initialization failed: {e}")
            print("   Falling back to proximity-only mode")
            fsgs_system = None
            enable_fsgs_complete = False

    # FSGSä¼ªæ ‡ç­¾åŠŸèƒ½åˆå§‹åŒ– (å‘ä¸‹å…¼å®¹ï¼Œä»…åœ¨æœªä½¿ç”¨å®Œæ•´ç³»ç»Ÿæ—¶)
    pseudo_cameras = None
    pseudo_labels = None
    depth_estimator = None
    enable_fsgs = False  # åˆå§‹åŒ–ï¼ˆFSGS Completeæ¨¡å¼ä¸‹ä¸ä½¿ç”¨æ—§ç‰ˆæ·±åº¦ç›‘ç£ï¼‰

    if not enable_fsgs_complete:
        fsgs_generator = None
        enable_fsgs = getattr(args, 'enable_fsgs_pseudo', False) if args else False

        if dataset.multi_gaussian or dataset.pseudo_labels:
            if enable_fsgs and HAS_FSGS_MODULES:
                # é€‰æ‹©FSGSç‰ˆæœ¬: improved æˆ– original
                fsgs_version = getattr(args, 'fsgs_version', 'improved') if args else 'improved'

                # æš‚æ—¶åªä½¿ç”¨åŸç‰ˆFSGSï¼Œé¿å…å¯¼å…¥é—®é¢˜
                if fsgs_version == 'improved':
                    print("ğŸ¯ [FSGS-Original] Using original FSGS (improved temporarily disabled)...")
                    fsgs_version = 'original'

                if fsgs_version == 'original':
                    print("ğŸ¯ [FSGS-Original] Using original FSGS implementation...")

                    # åˆ›å»ºåŸç‰ˆFSGSé£æ ¼ä¼ªè§†è§’ç”Ÿæˆå™¨
                    fsgs_generator = FSGSPseudoViewGenerator(
                        noise_std=getattr(args, 'fsgs_noise_std', 0.05) if args else 0.05
                    )

                    # ç”ŸæˆåŸç‰ˆFSGSé£æ ¼ä¼ªç›¸æœº
                    pseudo_cameras = fsgs_generator.generate_pseudo_cameras(
                        scene.train_cameras,
                        num_views=dataset.num_additional_views,
                        device=gaussians._xyz.device
                    )

                    print(f"âœ… [FSGS-Original] Generated {len(pseudo_cameras)} original FSGS pseudo cameras")

                # åˆå§‹åŒ–æ·±åº¦ä¼°è®¡å™¨ (å¦‚æœéœ€è¦)
                depth_model_type = getattr(args, 'fsgs_depth_model', 'dpt_large') if args else 'dpt_large'
                if depth_model_type != 'disabled':
                    depth_estimator = create_depth_estimator(
                        model_type=depth_model_type,
                        device=gaussians._xyz.device,
                        enable_fsgs_depth=True
                    )
                    print(f"âœ… [FSGS] Depth estimator: {depth_model_type}")
                else:
                    depth_estimator = None
                    print("ğŸ“¦ [FSGS] Depth estimator disabled")

            else:
                print("ğŸ“¦ [Legacy] Using original pseudo-label implementation...")
                pseudo_cameras = scene.generate_multi_gaussian_cameras(
                    num_additional_views=dataset.num_additional_views
                )
                print(f"Generated {len(pseudo_cameras)} legacy pseudo cameras")
    # åŠ è½½æ–­ç‚¹ï¼ˆå¦‚æœ‰ï¼‰
    if checkpoint is not None:
        (model_params, first_iter) = torch.load(checkpoint)
        gaussians.restore(model_params, opt)
        print(f"Load checkpoint from {osp.basename(checkpoint)}.")

    # è®¾ç½®æŸå¤±å‡½æ•°ï¼ˆæ˜¯å¦ä½¿ç”¨ TV æŸå¤±ï¼‰
    use_tv = opt.lambda_tv > 0
    if use_tv:
        print("Use total variation loss")

    # ğŸ¯ CoR-GS: å®šä¹‰èƒŒæ™¯é¢œè‰² (é»˜è®¤é»‘è‰²)
    background = torch.tensor([0, 0, 0], dtype=torch.float32, device="cuda")

    # ğŸ¯ CoR-GS: é¢„ç”Ÿæˆéšæœº pseudo-view ç›¸æœºï¼ˆå®˜æ–¹ç­–ç•¥ï¼‰
    pseudo_cameras_corgs = None
    if (args is not None and hasattr(args, 'enable_pseudo_coreg') and args.enable_pseudo_coreg and
        HAS_PSEUDO_COREG and gaussiansN >= 2):
        print("\n" + "="*60)
        print("ğŸ¯ [CoR-GS] ç”Ÿæˆ 10,000 ä¸ªéšæœº pseudo-view ç›¸æœº...")
        print("="*60)
        train_cameras = scene.getTrainCameras()
        pseudo_cameras_corgs = generate_random_pseudo_cameras(
            train_cameras=train_cameras,
            num_pseudo=10000,
            radius_range=(0.8, 1.2),
            seed=42
        )
        print(f"âœ… [CoR-GS] Pseudo-view ç”Ÿæˆå®Œæˆ: {len(pseudo_cameras_corgs)} ä¸ª")
        print("="*60 + "\n")

    if use_tv:
        tv_vol_size = opt.tv_vol_size
        tv_vol_nVoxel = torch.tensor([tv_vol_size, tv_vol_size, tv_vol_size])
        tv_vol_sVoxel = torch.tensor(scanner_cfg["dVoxel"]) * tv_vol_nVoxel

    # è®­ç»ƒä¸»å¾ªç¯
    iter_start = torch.cuda.Event(enable_timing=True)
    iter_end = torch.cuda.Event(enable_timing=True)
    ckpt_save_path = osp.join(scene.model_path, "ckpt")
    os.makedirs(ckpt_save_path, exist_ok=True)
    viewpoint_stack = None
    progress_bar = tqdm(range(0, opt.iterations), desc="Train", leave=False)
    progress_bar.update(first_iter)
    first_iter += 1
    for iteration in range(first_iter, opt.iterations + 1):
        iter_start.record()

        # æ›´æ–°å­¦ä¹ ç‡ - ä¸ºæ¯ä¸ªé«˜æ–¯åœºæ›´æ–°
        for i in range(gaussiansN):
            GsDict[f"gs{i}"].update_learning_rate(iteration)

        # éšæœºé€‰æ‹©ä¸€ä¸ªè®­ç»ƒè§†è§’
        if not viewpoint_stack:
            viewpoint_stack = scene.getTrainCameras().copy()
        viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack) - 1))

        # ä¸ºæ¯ä¸ªé«˜æ–¯åœºæ¸²æŸ“ - å‚è€ƒX-Gaussian-depthå®ç°
        RenderDict = {}
        for i in range(gaussiansN):
            RenderDict[f"render_pkg_gs{i}"] = render(
                viewpoint_cam,
                GsDict[f'gs{i}'],
                pipe,
                enable_drop=args.enable_drop,
                drop_rate=args.drop_rate if hasattr(args, 'drop_rate') else 0.10,
                iteration=iteration,
            )
            RenderDict[f"image_gs{i}"] = RenderDict[f"render_pkg_gs{i}"]["render"]
            RenderDict[f"viewspace_point_tensor_gs{i}"] = RenderDict[f"render_pkg_gs{i}"]["viewspace_points"]
            RenderDict[f"visibility_filter_gs{i}"] = RenderDict[f"render_pkg_gs{i}"]["visibility_filter"]
            RenderDict[f"radii_gs{i}"] = RenderDict[f"render_pkg_gs{i}"]["radii"]

        # è®¡ç®—æ¯ä¸ªé«˜æ–¯åœºçš„æŸå¤±
        LossDict = {}
        gt_image = viewpoint_cam.original_image.cuda()
        
        for i in range(gaussiansN):
            LossDict[f"loss_gs{i}"] = l1_loss(RenderDict[f"image_gs{i}"], gt_image)
            
            # DSSIM æŸå¤±
        if opt.lambda_dssim > 0:
                loss_dssim = 1.0 - ssim(RenderDict[f"image_gs{i}"], gt_image)
                LossDict[f"loss_gs{i}"] += opt.lambda_dssim * loss_dssim
        
        # ååŒè®­ç»ƒ - å‚è€ƒX-Gaussian-depthå®ç°
        if coreg and gaussiansN > 1:
            for i in range(gaussiansN):
                for j in range(gaussiansN):
                    if i != j:
                        coreg_loss = l1_loss(RenderDict[f"image_gs{i}"], RenderDict[f"image_gs{j}"].detach())
                        LossDict[f"loss_gs{i}"] += coreg_loss
        
        # å¤šé«˜æ–¯è®­ç»ƒæŸå¤± - åŸå§‹ç‰ˆæœ¬ï¼ˆidentity lossï¼‰
        if dataset.multi_gaussian and pseudo_cameras is not None and gaussiansN > 1:
            for pseudo_cam in pseudo_cameras[:3]:  # é™åˆ¶æ•°é‡é¿å…è®¡ç®—è¿‡è½½
                for i in range(gaussiansN):
                    pseudo_render_pkg = render(
                        pseudo_cam,
                        GsDict[f'gs{i}'],
                        pipe,
                        enable_drop=args.enable_drop,
                        drop_rate=args.drop_rate if hasattr(args, 'drop_rate') else 0.10,
                        iteration=iteration,
                    )
                    pseudo_image = pseudo_render_pkg["render"]
                    # åŸå§‹é”™è¯¯ç‰ˆæœ¬ï¼šidentity lossï¼ˆè‡ªå·±å’Œè‡ªå·±æ¯”è¾ƒï¼‰
                    LossDict[f"loss_gs{i}"] += dataset.multi_gaussian_weight * l1_loss(pseudo_image, pseudo_image.detach())
        
        # FSGSä¼ªæ ‡ç­¾è®­ç»ƒæŸå¤± (å¯é€‰ï¼Œå‘ä¸‹å…¼å®¹)
        # FSGSå»¶è¿Ÿå¯åŠ¨: 2000æ¬¡è¿­ä»£åå¯åŠ¨ (åŸç‰ˆ: 1000æ¬¡)
        fsgs_start_iter = 2000 if enable_fsgs_proximity else 1000
        if dataset.pseudo_labels and pseudo_cameras is not None and iteration > fsgs_start_iter:
            # è·å–ä¼ªç›¸æœºå’Œå¯¹åº”çš„æœ€è¿‘çœŸå®ç›¸æœº
            pseudo_stack, closest_cam_stack = scene.getPseudoCamerasWithClosestViews(pseudo_cameras)
            
            if len(pseudo_stack) > 0:
                # åˆ›å»ºå‰¯æœ¬ï¼ˆIPSMçš„åšæ³•ï¼‰
                pseudo_stack = pseudo_stack.copy()
                closest_cam_stack = closest_cam_stack.copy()
                
                # éšæœºé€‰æ‹©ä¸€ä¸ªä¼ªç›¸æœºï¼ˆIPSMçš„dropæœºåˆ¶ï¼‰
                randint_idx = randint(0, len(pseudo_stack) - 1)
                pseudo_cam = pseudo_stack.pop(randint_idx)
                closest_cam = closest_cam_stack.pop(randint_idx)
                
                for j in range(gaussiansN):
                    # ä»ä¼ªç›¸æœºæ¸²æŸ“å›¾åƒ
                    pseudo_render_pkg = render(
                        pseudo_cam,
                        GsDict[f'gs{j}'],
                        pipe,
                    )
                    rendered_img_pseudo = pseudo_render_pkg["render"]  # (C, H, W)
                    H, W = rendered_img_pseudo.shape[1], rendered_img_pseudo.shape[2]
                    
                    # ä»ä¼ªç›¸æœºæå–æ·±åº¦å›¾ï¼ˆä½¿ç”¨ç°æœ‰çš„depthæå–æ–¹æ³•ï¼‰
                    tv_vol_center = (bbox[0] + tv_vol_sVoxel / 2) + (
                        bbox[1] - tv_vol_sVoxel - bbox[0]
                    ) * torch.rand(3)
                    vol_pred_pseudo = query(
                        GsDict[f"gs{j}"],
                        tv_vol_center,
                        tv_vol_nVoxel,
                        tv_vol_sVoxel,
                        pipe,
                    )["vol"]
                    rendered_depth_pseudo = extract_depth_from_volume_ray_casting(
                        vol_pred_pseudo,
                        pseudo_cam,
                        threshold=getattr(dataset, 'depth_threshold', 0.01)
                    )  # (H_vol, W_vol) - volumeçš„å°ºå¯¸ï¼Œä¸æ˜¯å›¾åƒå°ºå¯¸
                    
                    # ä»æœ€è¿‘çœŸå®ç›¸æœºè·å–å›¾åƒå’Œæ·±åº¦
                    closest_image_1 = closest_cam.original_image.cuda()  # (C, H_closest, W_closest)
                    closest_H, closest_W = closest_image_1.shape[1], closest_image_1.shape[2]
                    
                    # ä»æœ€è¿‘çœŸå®ç›¸æœºæå–æ·±åº¦å›¾
                    vol_pred_closest = query(
                        GsDict[f"gs{j}"],
                        tv_vol_center,
                        tv_vol_nVoxel,
                        tv_vol_sVoxel,
                        pipe,
                    )["vol"]
                    closest_depth_1 = extract_depth_from_volume_ray_casting(
                        vol_pred_closest,
                        closest_cam,
                        threshold=getattr(dataset, 'depth_threshold', 0.01)
                    )  # (H_vol, W_vol) - volumeçš„å°ºå¯¸ï¼Œä¸æ˜¯å›¾åƒå°ºå¯¸
                    
                    # ç¡®ä¿æ·±åº¦å›¾å°ºå¯¸ä¸å›¾åƒå°ºå¯¸åŒ¹é…ï¼ˆresizeæ·±åº¦å›¾åˆ°å›¾åƒå°ºå¯¸ï¼‰
                    # ä¼ªç›¸æœºæ·±åº¦å›¾resizeåˆ°ä¼ªç›¸æœºå›¾åƒå°ºå¯¸
                    pseudo_depth_H, pseudo_depth_W = rendered_depth_pseudo.shape
                    if pseudo_depth_H != H or pseudo_depth_W != W:
                        # ä½¿ç”¨åŒçº¿æ€§æ’å€¼å°†æ·±åº¦å›¾resizeåˆ°å›¾åƒå°ºå¯¸
                        rendered_depth_pseudo_resized = F.interpolate(
                            rendered_depth_pseudo.unsqueeze(0).unsqueeze(0),  # (1, 1, H_vol, W_vol)
                            size=(H, W),
                            mode='bilinear',
                            align_corners=False
                        ).squeeze(0).squeeze(0)  # (H, W)
                    else:
                        rendered_depth_pseudo_resized = rendered_depth_pseudo
                    
                    # çœŸå®ç›¸æœºæ·±åº¦å›¾resizeåˆ°çœŸå®ç›¸æœºå›¾åƒå°ºå¯¸
                    closest_depth_H, closest_depth_W = closest_depth_1.shape
                    if closest_depth_H != closest_H or closest_depth_W != closest_W:
                        closest_depth_1_resized = F.interpolate(
                            closest_depth_1.unsqueeze(0).unsqueeze(0),  # (1, 1, H_vol, W_vol)
                            size=(closest_H, closest_W),
                            mode='bilinear',
                            align_corners=False
                        ).squeeze(0).squeeze(0)  # (H_closest, W_closest)
                    else:
                        closest_depth_1_resized = closest_depth_1
                    
                    # æ„å»ºå†…å‚çŸ©é˜µï¼ˆä»FoVè®¡ç®—ï¼‰- ä½¿ç”¨ä¼ªç›¸æœºçš„å°ºå¯¸ï¼Œå› ä¸ºtarget_depthæ˜¯ä¼ªç›¸æœºçš„
                    focal_x = pseudo_cam.image_width / (2.0 * np.tan(pseudo_cam.FoVx / 2.0))
                    focal_y = pseudo_cam.image_height / (2.0 * np.tan(pseudo_cam.FoVy / 2.0))
                    intrinsic = torch.tensor([
                        [focal_x, 0, pseudo_cam.image_width / 2.0],
                        [0, focal_y, pseudo_cam.image_height / 2.0],
                        [0, 0, 1]
                    ], device=closest_image_1.device, dtype=torch.float32)
                    
                    # é€†å˜å½¢ï¼ˆinverse warpï¼‰- ä½¿ç”¨resizeåçš„æ·±åº¦å›¾
                    # æ³¨æ„ï¼šinverse_warpå†…éƒ¨ä¼šä½¿ç”¨source_imageçš„å°ºå¯¸ï¼Œæ‰€ä»¥éœ€è¦ç¡®ä¿æ·±åº¦å›¾å°ºå¯¸åŒ¹é…
                    warp_rst_1 = inverse_warp(
                        closest_image_1,
                        closest_depth_1_resized,  # ä½¿ç”¨resizeåçš„æ·±åº¦å›¾
                        rendered_depth_pseudo_resized.unsqueeze(0),  # (1, H, W) - ä½¿ç”¨resizeåçš„æ·±åº¦å›¾
                        closest_cam.world_view_transform,  # r2ä½¿ç”¨world_view_transformè€Œä¸æ˜¯extrinsic_matrix
                        pseudo_cam.world_view_transform,
                        intrinsic
                    )
                    
                    # è®¡ç®—maskedæŸå¤±ï¼ˆå®Œå…¨æŒ‰ç…§IPSMå›¾ç‰‡ä»£ç ï¼‰
                    # æ³¨æ„ï¼šmaskæ˜¯Floatç±»å‹ï¼Œä½¿ç”¨ä¹˜æ³•ä»£æ›¿ä½è¿ç®—&
                    combined_mask = (warp_rst_1["mask_warp"] * warp_rst_1["mask_depth_strict"]).unsqueeze(0)  # (1, H, W)
                    
                    warped_masked_strict_image = warp_rst_1["warped_img"] * combined_mask
                    pseudo_masked_strict_image = rendered_img_pseudo * combined_mask
                    
                    # æŸå¤±ç¼©æ”¾å› å­ï¼ˆé€æ¸å¢åŠ ï¼ŒIPSMçš„åšæ³•ï¼‰
                    loss_scale = min(iteration / 500.0, 1.0)
                    
                    # è®¡ç®—masked L1æŸå¤±
                    Ll1_masked_pseudo = l1_loss_mask(
                        pseudo_masked_strict_image,
                        warped_masked_strict_image.detach()
                    )
                    
                    # æ·»åŠ åˆ°æ€»æŸå¤±ï¼ˆä¹˜ä»¥loss_scaleï¼‰
                    LossDict[f"loss_gs{j}"] += dataset.pseudo_label_weight * loss_scale * Ll1_masked_pseudo
                    
                    # å¯é€‰ï¼šæ¯500æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡ä¿¡æ¯
                    if iteration % 500 == 0:
                        mask_valid_ratio = combined_mask.sum().item() / (H * W)
                        print(f"[IPSM Drop] Iteration {iteration}, GS{j}: masked_loss={Ll1_masked_pseudo.item():.6f}, "
                              f"loss_scale={loss_scale:.3f}, valid_mask_ratio={mask_valid_ratio:.3f}")
        
        # FSGSæ·±åº¦ç›‘ç£ (ä¼ªè§†è§’+è®­ç»ƒè§†è§’æ·±åº¦çº¦æŸ)
        if enable_fsgs and depth_estimator and depth_estimator.enabled and iteration > fsgs_start_iter:
            fsgs_depth_weight = getattr(args, 'fsgs_depth_weight', 0.05) if args else 0.05
            
            for j in range(gaussiansN):
                # 1. è®­ç»ƒè§†è§’æ·±åº¦ç›‘ç£
                try:
                    # ä¼°è®¡å½“å‰è®­ç»ƒè§†è§’çš„æ·±åº¦
                    gt_image_for_depth = gt_image.unsqueeze(0)  # [1, C, H, W]
                    estimated_depth = depth_estimator.estimate_depth(gt_image_for_depth, normalize=True)
                    
                    if estimated_depth is not None:
                        # æ¸²æŸ“å½“å‰è§†è§’çš„æ·±åº¦å›¾
                        rendered_depth = RenderDict.get(f"depth_gs{j}")
                        if rendered_depth is not None:
                            # è®¡ç®—Pearsonç›¸å…³æ€§æ·±åº¦æŸå¤±
                            depth_loss_train = depth_estimator.compute_pearson_loss(
                                rendered_depth, estimated_depth.squeeze(0)
                            )
                            LossDict[f"loss_gs{j}"] += fsgs_depth_weight * depth_loss_train
                            
                            if iteration % 500 == 0:
                                print(f"[FSGS] Iteration {iteration}, GS{j}: train_depth_loss={depth_loss_train.item():.6f}")
                                
                except Exception as e:
                    if iteration % 1000 == 0:  # å‡å°‘é”™è¯¯æ—¥å¿—é¢‘ç‡
                        print(f"Warning: FSGS train depth loss failed: {e}")
                
                # 2. ä¼ªè§†è§’æ·±åº¦ç›‘ç£ (å¦‚æœæœ‰ä¼ªç›¸æœº)
                if pseudo_cameras and len(pseudo_cameras) > 0:
                    try:
                        # éšæœºé€‰æ‹©ä¸€ä¸ªä¼ªç›¸æœºè¿›è¡Œæ·±åº¦ç›‘ç£
                        pseudo_cam = pseudo_cameras[randint(0, len(pseudo_cameras) - 1)]
                        
                        # æ¸²æŸ“ä¼ªè§†è§’
                        pseudo_render_pkg = render(pseudo_cam, GsDict[f'gs{j}'], pipe)
                        pseudo_image = pseudo_render_pkg["render"]
                        pseudo_depth = pseudo_render_pkg.get("depth")
                        
                        if pseudo_depth is not None:
                            # ä¼°è®¡ä¼ªè§†è§’æ·±åº¦
                            pseudo_image_for_depth = pseudo_image.unsqueeze(0)
                            estimated_pseudo_depth = depth_estimator.estimate_depth(pseudo_image_for_depth, normalize=True)
                            
                            if estimated_pseudo_depth is not None:
                                # è®¡ç®—ä¼ªè§†è§’æ·±åº¦æŸå¤±
                                depth_loss_pseudo = depth_estimator.compute_pearson_loss(
                                    pseudo_depth, estimated_pseudo_depth.squeeze(0)
                                )
                                LossDict[f"loss_gs{j}"] += fsgs_depth_weight * depth_loss_pseudo
                                
                                if iteration % 500 == 0:
                                    print(f"[FSGS] Iteration {iteration}, GS{j}: pseudo_depth_loss={depth_loss_pseudo.item():.6f}")
                                    
                    except Exception as e:
                        if iteration % 1000 == 0:  # å‡å°‘é”™è¯¯æ—¥å¿—é¢‘ç‡
                            print(f"Warning: FSGS pseudo depth loss failed: {e}")

        # ğŸŒŸğŸŒŸ FSGS Complete æ·±åº¦ç›‘ç£ (Proximity + Depth + Pseudo Views - 2025-11-15)
        if enable_fsgs_complete and fsgs_system is not None:
            try:
                # ä¸ºæ¯ä¸ªé«˜æ–¯åœºè®¡ç®—æ·±åº¦ç›‘ç£loss
                for i in range(gaussiansN):
                    depth_loss_dict = fsgs_system.compute_depth_loss(
                        viewpoint_cam,
                        GsDict[f'gs{i}'],
                        pipe,
                        background,
                        iteration
                    )

                    # æ·»åŠ æ·±åº¦lossåˆ°æ€»loss
                    if depth_loss_dict['depth_loss'].item() > 0:
                        LossDict[f"loss_gs{i}"] += depth_loss_dict['depth_loss']

                        # æ¯500è½®æ‰“å°ä¸€æ¬¡
                        if iteration % 500 == 0:
                            print(f"[FSGS Complete] Iteration {iteration}, GS{i}:")
                            print(f"  train_depth_loss={depth_loss_dict['train_depth_loss'].item():.6f}")
                            print(f"  pseudo_depth_loss={depth_loss_dict['pseudo_depth_loss'].item():.6f}")
                            print(f"  total_depth_loss={depth_loss_dict['depth_loss'].item():.6f}")

            except Exception as e:
                if iteration % 1000 == 0:
                    print(f"âš ï¸  [FSGS Complete] Depth loss failed: {e}")

        # DepthæŸå¤± - ä½¿ç”¨voxelizationæå–æ·±åº¦
        if dataset.enable_depth and dataset.depth_loss_weight > 0:
            for i in range(gaussiansN):
                # ä½¿ç”¨voxelizationè·å–density volume
                tv_vol_center = (bbox[0] + tv_vol_sVoxel / 2) + (
                    bbox[1] - tv_vol_sVoxel - bbox[0]
                ) * torch.rand(3)
                vol_pred = query(
                    GsDict[f"gs{i}"],
                    tv_vol_center,
                    tv_vol_nVoxel,
                    tv_vol_sVoxel,
                    pipe,
                )["vol"]
                
                # ä»volumeæå–æ·±åº¦å›¾
                depth_map = extract_depth_from_volume_ray_casting(
                    vol_pred, 
                    viewpoint_cam, 
                    threshold=dataset.depth_threshold
                )
                
                # å¦‚æœæœ‰ground truthæ·±åº¦ï¼Œè®¡ç®—æ·±åº¦æŸå¤±
                if hasattr(viewpoint_cam, 'depth_image') and viewpoint_cam.depth_image is not None:
                    gt_depth = viewpoint_cam.depth_image.cuda()
                    depth_loss_val = depth_loss_fn(
                        depth_map, 
                        gt_depth, 
                        loss_type=dataset.depth_loss_type
                    )
                    LossDict[f"loss_gs{i}"] += dataset.depth_loss_weight * depth_loss_val
                    
                # è‡ªç›‘ç£æ·±åº¦çº¦æŸï¼šè®©æ·±åº¦å¹³æ»‘ï¼Œæå‡é‡å»ºè´¨é‡
                if depth_map.shape[0] > 1 and depth_map.shape[1] > 1:
                    # è®¡ç®—æ·±åº¦å›¾ç›¸é‚»åƒç´ çš„å·®å¼‚ï¼ˆæ°´å¹³+å‚ç›´ï¼‰
                    depth_diff_h = torch.abs(depth_map[1:, :] - depth_map[:-1, :])
                    depth_diff_w = torch.abs(depth_map[:, 1:] - depth_map[:, :-1])
                    consistency_loss = (depth_diff_h.mean() + depth_diff_w.mean()) * 0.1
                    
                    # æ·»åŠ åˆ°æ€»æŸå¤±ä¸­
                    LossDict[f"loss_gs{i}"] += dataset.depth_loss_weight * consistency_loss
                    
                    # æ¯500æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡
                    if iteration % 500 == 0:
                        print(f"[æ·±åº¦çº¦æŸ] Iteration {iteration}: {consistency_loss.item():.6f}")
        
        # ğŸŒŸ [GR-Gaussian] å›¾æ›´æ–°ä¸å›¾æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–
        if dataset.enable_graph_laplacian:
            # æ›´æ–°å›¾ç»“æ„ (æ¯ graph_update_interval æ¬¡è¿­ä»£,ä» iteration 100 å¼€å§‹)
            if gr_graph is not None and iteration > 0 and iteration % dataset.graph_update_interval == 0:
                with torch.no_grad():
                    xyz = gaussians.get_xyz.detach()
                    gr_graph.build_knn_graph(xyz)
                    gr_graph.compute_edge_weights(xyz)
                    if iteration % 500 == 0:
                        print(f"[GR-Gaussian] Rebuilt graph at iteration {iteration}: "
                              f"{gr_graph.num_nodes} nodes, {gr_graph.edge_index.shape[1]} edges")

            # è®¡ç®—å›¾æ‹‰æ™®æ‹‰æ–¯æŸå¤± - æ·»åŠ å»¶è¿Ÿå¯åŠ¨å’Œé¢‘ç‡é™åˆ¶
            if iteration > 5000 and iteration % 500 == 0:  # å»¶è¿Ÿå¯åŠ¨ + æ¯500æ¬¡è¿­ä»£è®¡ç®—ä¸€æ¬¡
                for i in range(gaussiansN):
                    graph_laplacian_loss = compute_graph_laplacian_loss(
                        GsDict[f"gs{i}"],
                        graph=gr_graph,  # ä¼ é€’é¢„æ„å»ºçš„å›¾ (å¦‚æœå­˜åœ¨)
                        k=dataset.graph_k,
                        Lambda_lap=dataset.graph_lambda_lap
                    )
                    LossDict[f"loss_gs{i}"] += graph_laplacian_loss

                    # æ—¥å¿—è®°å½•
                    if iteration % 500 == 0:
                        tb_writer.add_scalar(f'GR-Gaussian/graph_laplacian_gs{i}',
                                           graph_laplacian_loss.item(), iteration)
                    if iteration % 1000 == 0:
                        print(f"[GR-Gaussian] Iteration {iteration}, GS{i}: "
                              f"graph_loss={graph_laplacian_loss.item():.6f}")

        # 3D TV æŸå¤± - ä¸ºæ¯ä¸ªé«˜æ–¯åœºè®¡ç®—
        if use_tv:
            for i in range(gaussiansN):
                # éšæœºé€‰å–ä¸€ä¸ªå°ä½“ç§¯ä¸­å¿ƒ
                tv_vol_center = (bbox[0] + tv_vol_sVoxel / 2) + (
                bbox[1] - tv_vol_sVoxel - bbox[0]
                ) * torch.rand(3)
                vol_pred = query(
                GsDict[f"gs{i}"],
                tv_vol_center,
                tv_vol_nVoxel,
                tv_vol_sVoxel,
                pipe,
                )["vol"]
                loss_tv = tv_3d_loss(vol_pred, reduction="mean")
                LossDict[f"loss_gs{i}"] += opt.lambda_tv * loss_tv

        # === CoR-GS Stage 3: Pseudo-view Co-regularization (2025-11-17) ===
        # ğŸ”§ Bug ä¿®å¤ç‰ˆæœ¬ï¼šä¿®å¤ Bug 1/2/3/4
        if (args is not None and hasattr(args, 'enable_pseudo_coreg') and args.enable_pseudo_coreg and
            HAS_PSEUDO_COREG and iteration >= args.pseudo_start_iter and gaussiansN >= 2 and
            pseudo_cameras_corgs is not None):

            try:
                # æ­¥éª¤ 1: ä»é¢„ç”Ÿæˆçš„ pseudo-view åˆ—è¡¨ä¸­éšæœºé‡‡æ ·ï¼ˆä¿®å¤ Bug 1ï¼‰
                import random
                pseudo_camera = random.choice(pseudo_cameras_corgs)

                # æ­¥éª¤ 2: æ¸²æŸ“ç²—æ¨¡å‹å’Œç²¾ç»†æ¨¡å‹çš„ pseudo-view
                renders_pseudo = []
                for gid in range(min(2, gaussiansN)):  # ä»…å¯¹å‰ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œ co-regularization
                    render_pkg_pseudo = render(
                        pseudo_camera,
                        GsDict[f'gs{gid}'],
                        pipe,
                        scaling_modifier=1.0,
                        enable_drop=args.enable_drop if hasattr(args, 'enable_drop') else False,
                        drop_rate=args.drop_rate if hasattr(args, 'drop_rate') else 0.10,
                        iteration=iteration
                    )
                    renders_pseudo.append(render_pkg_pseudo)

                # æ­¥éª¤ 3: è®¡ç®— Co-regularization æŸå¤±ï¼ˆç²—æ¨¡å‹ vs ç²¾ç»†æ¨¡å‹ï¼‰
                # ğŸ”§ ä¿®å¤ Bug 2/3: åˆ†åˆ«è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„ disagreement lossï¼Œä½¿ç”¨ detach()
                if len(renders_pseudo) >= 2:
                    render_gs0 = renders_pseudo[0]["render"]
                    render_gs1 = renders_pseudo[1]["render"]

                    # gs0 çš„æŸå¤±ï¼šrender_gs0 å‚ä¸æ¢¯åº¦ï¼Œrender_gs1 detach
                    loss_pseudo_coreg_dict_gs0 = compute_pseudo_coreg_loss_medical(
                        render1=render_gs0,
                        render2=render_gs1.detach(),  # ä¿®å¤ Bug 2: detach
                        lambda_dssim=0.2,
                        roi_weights=None
                    )

                    # gs1 çš„æŸå¤±ï¼šrender_gs1 å‚ä¸æ¢¯åº¦ï¼Œrender_gs0 detach
                    loss_pseudo_coreg_dict_gs1 = compute_pseudo_coreg_loss_medical(
                        render1=render_gs1,
                        render2=render_gs0.detach(),  # ä¿®å¤ Bug 2: detach
                        lambda_dssim=0.2,
                        roi_weights=None
                    )

                    # æ­¥éª¤ 4: å åŠ åˆ°æ€»æŸå¤±ï¼ˆä¿®å¤ Bug 3: åˆ†åˆ«è®¡ç®—ï¼Œé¿å…æ¢¯åº¦åŠ å€ï¼‰
                    # ä¿®å¤ Bug 4: ç¡®ä¿ disagreement loss å‚ä¸ä¼˜åŒ–
                    LossDict['loss_gs0'] += args.lambda_pseudo * loss_pseudo_coreg_dict_gs0['loss']
                    LossDict['loss_gs1'] += args.lambda_pseudo * loss_pseudo_coreg_dict_gs1['loss']

                    # æ­¥éª¤ 5: TensorBoard æ—¥å¿—è®°å½•
                    if tb_writer is not None:
                        tb_writer.add_scalar(
                            "train_loss_patches/pseudo_coreg_total",
                            loss_pseudo_coreg.item(),
                            iteration
                        )
                        tb_writer.add_scalar(
                            "train_loss_patches/pseudo_coreg_l1",
                            loss_pseudo_coreg_dict['l1'].item(),
                            iteration
                        )
                        tb_writer.add_scalar(
                            "train_loss_patches/pseudo_coreg_dssim",
                            loss_pseudo_coreg_dict['d_ssim'].item(),
                            iteration
                        )
                        tb_writer.add_scalar(
                            "train_loss_patches/pseudo_coreg_ssim",
                            loss_pseudo_coreg_dict['ssim'].item(),
                            iteration
                        )
                        tb_writer.add_scalar(
                            "train_loss_patches/pseudo_coreg_weighted",
                            (args.lambda_pseudo * loss_pseudo_coreg).item(),
                            iteration
                        )

                    # æ­¥éª¤ 6: æ§åˆ¶å°æ—¥å¿—ï¼ˆæ¯ 100 iterations è¾“å‡ºä¸€æ¬¡ï¼‰
                    if iteration % 100 == 0:
                        print(f"  [Pseudo Co-reg] Loss: {loss_pseudo_coreg.item():.6f}, "
                              f"L1: {loss_pseudo_coreg_dict['l1'].item():.6f}, "
                              f"SSIM: {loss_pseudo_coreg_dict['ssim'].item():.4f}, "
                              f"Weighted: {(args.lambda_pseudo * loss_pseudo_coreg).item():.6f}")

            except Exception as e:
                # å¼‚å¸¸å¤„ç†ï¼šæ‰“å°è­¦å‘Šä½†ä¸ä¸­æ–­è®­ç»ƒï¼ˆå‘ä¸‹å…¼å®¹ï¼‰
                if iteration % 100 == 0:
                    import traceback
                    import sys
                    print(f"âš ï¸  [Pseudo Co-reg] Failed at iter {iteration}: {e}")
                    print("  â†’ Full traceback:")
                    tb_str = ''.join(traceback.format_exception(type(e), e, e.__traceback__))
                    print(tb_str, file=sys.stderr)
                    print("  â†’ Continuing training without pseudo-view loss...")

        # ğŸ¯ [SSS-RÂ²] Add regularization losses for Student's t parameters
        for i in range(gaussiansN):
            if hasattr(GsDict[f"gs{i}"], 'use_student_t') and GsDict[f"gs{i}"].use_student_t:
                opacity = GsDict[f"gs{i}"].get_opacity
                nu = GsDict[f"gs{i}"].get_nu

                # ğŸ¯ [SSS-v5-OPTIMAL] æœ€ä¼˜æ­£åˆ™åŒ– - åŸºäº v4 è¯Šæ–­ç»“æœçš„æŠ˜ä¸­æ–¹æ¡ˆ
                # v4 è¯Šæ–­ç»“è®ºï¼š
                #   - iter 5000: 23.44 dB (å³°å€¼) - balance_loss 0.001 èƒ½è¾¾åˆ°é«˜æ€§èƒ½
                #   - iter 10000: 13.13 dB (å´©æºƒ) - balance_loss 0.001 å¤ªå¼±ï¼Œæ— æ³•é•¿æœŸç¨³å®š
                # v5 ç­–ç•¥ï¼š3å€æƒé‡ + é€‚åº¦ç›®æ ‡ï¼Œå¹³è¡¡ï¿½ï¿½ï¿½èƒ½ä¸ç¨³å®šæ€§

                # Opacity balance loss - 3å€ v4 æƒé‡ (0.001 â†’ 0.003)
                pos_count = (opacity > 0).float().mean()
                pos_target = 0.75  # v4: 0.70 â†’ v5: 0.75 (å…è®¸ 25% è´Ÿå€¼ï¼Œæ¯” v4 ä¸¥æ ¼)
                balance_loss = torch.abs(pos_count - pos_target)
                LossDict[f"loss_gs{i}"] += 0.003 * balance_loss  # v5: 0.003 (3å€ v4)

                # Nu diversity loss: ä¿æŒ Î½ å¤šæ ·æ€§ (ä¸å½±å“ opacity)
                nu_diversity_loss = -torch.std(nu) * 0.1
                nu_range_loss = torch.mean(torch.relu(nu - 8.0)) + torch.mean(torch.relu(2.0 - nu))
                LossDict[f"loss_gs{i}"] += 0.001 * (nu_diversity_loss + nu_range_loss)

        # ğŸ¯ [SSS-v5-OPTIMAL] Debug logging - è§‚å¯Ÿä¼˜åŒ–åçš„ opacity åŠ¨æ€
        if hasattr(GsDict[f"gs0"], 'use_student_t') and GsDict[f"gs0"].use_student_t and iteration % 2000 == 0:
            opacity = GsDict[f"gs0"].get_opacity
            nu = GsDict[f"gs0"].get_nu
            pos_ratio = (opacity > 0).float().mean()
            neg_ratio = (opacity < 0).float().mean()
            nu_mean = nu.mean()
            nu_std = nu.std()

            pos_target = 0.75  # v5ç›®æ ‡

            print(f"ğŸ¯ [SSS-v5-OPTIMAL] Iter {iteration}")
            print(f"   Opacity: [{opacity.min():.3f}, {opacity.max():.3f}]")
            print(f"   Balance: {pos_ratio*100:.1f}% pos / {neg_ratio*100:.1f}% neg (target: {pos_target*100:.0f}% pos)")
            print(f"   Nu: mean={nu_mean:.2f}, std={nu_std:.2f}, range=[{nu.min():.1f}, {nu.max():.1f}]")

            # è®°å½•æç«¯æƒ…å†µ
            extreme_neg = (opacity < -0.5).float().mean()
            extreme_pos = (opacity > 0.9).float().mean()
            print(f"   Extremes: {extreme_pos*100:.1f}% >0.9, {extreme_neg*100:.1f}% <-0.5")
        
        # åå‘ä¼ æ’­ - ä¸ºæ¯ä¸ªé«˜æ–¯åœº
        for i in range(gaussiansN):
            LossDict[f"loss_gs{i}"].backward(retain_graph=(i < gaussiansN - 1))

        iter_end.record()
        torch.cuda.synchronize()

        with torch.no_grad():
            # è‡ªé€‚åº”æ§åˆ¶ï¼šæ›´æ–°é«˜æ–¯åŠå¾„å’Œç»Ÿè®¡ - ä¸ºæ¯ä¸ªé«˜æ–¯åœº
            for i in range(gaussiansN):
                viewspace_point_tensor = RenderDict[f"viewspace_point_tensor_gs{i}"]
                visibility_filter = RenderDict[f"visibility_filter_gs{i}"]
                radii = RenderDict[f"radii_gs{i}"]
                
                GsDict[f"gs{i}"].max_radii2D[visibility_filter] = torch.max(
                    GsDict[f"gs{i}"].max_radii2D[visibility_filter], radii[visibility_filter]
                )
                GsDict[f"gs{i}"].add_densification_stats(viewspace_point_tensor, visibility_filter)
            
            # é«˜æ–¯ç‚¹ç¨ å¯†åŒ–ä¸å‰ªæ - ä¸ºæ¯ä¸ªé«˜æ–¯åœº
            if iteration < opt.densify_until_iter:
                if (
                    iteration > opt.densify_from_iter
                    and iteration % opt.densification_interval == 0
                ):
                    # ğŸ”¬ Proximity-Guided Densification (åŒ»å­¦æ„ŸçŸ¥å¯†åŒ–) 
                    if (proximity_densifier is not None and 
                        hasattr(args, 'proximity_interval') and 
                        iteration % args.proximity_interval == 0):
                        
                        organ_type = getattr(args, 'proximity_organ_type', 'foot')
                        max_points = getattr(args, 'proximity_max_points', 500)
                        
                        for i in range(gaussiansN):
                            current_gaussians = GsDict[f"gs{i}"].get_xyz  # (N, 3)
                            current_opacity = GsDict[f"gs{i}"].get_opacity  # (N, 1)
                            
                            print(f"ğŸ”¬ [Proximity-Guided] Iter {iteration}: åˆ†æGS{i}çš„åŒ»å­¦åˆç†æ€§...")
                            
                            # æ‰§è¡ŒåŒ»å­¦æ„ŸçŸ¥çš„proximityå¯†åŒ–
                            densify_result = proximity_densifier.proximity_guided_densify_realistic(
                                current_gaussians, current_opacity, organ_type, max_points
                            )
                            
                            if densify_result['densified_points'] > 0:
                                new_positions = densify_result['new_positions']  # (K, 3) 
                                new_opacities = densify_result['new_opacities']  # (K, 1)
                                
                                # åˆ›å»ºæ–°é«˜æ–¯ç‚¹çš„å…¶ä»–å±æ€§ (åŸºäºè¿‘é‚»æ’å€¼)
                                device = current_gaussians.device
                                num_new = new_positions.shape[0]
                                
                                # åˆå§‹åŒ–å…¶ä»–å±æ€§
                                new_colors = torch.zeros(num_new, 3, device=device)  # RGB
                                new_rotations = torch.zeros(num_new, 4, device=device)  # å››å…ƒæ•°
                                new_rotations[:, 0] = 1.0  # wåˆ†é‡è®¾ä¸º1 (å•ä½å››å…ƒæ•°)
                                new_scales = torch.ones(num_new, 3, device=device) * 0.01  # å°å°ºåº¦
                                
                                # æ·»åŠ æ–°é«˜æ–¯ç‚¹åˆ°æ¨¡å‹
                                GsDict[f"gs{i}"].densification_postfix(
                                    new_positions, new_colors, new_rotations, new_scales, new_opacities
                                )
                                
                                print(f"âœ… [Proximity-Guided] GS{i}: æ–°å¢ {num_new} ä¸ªåŒ»å­¦åˆç†çš„é«˜æ–¯ç‚¹")
                    
                    # æ ‡å‡†å¯†åŒ–å’Œå‰ªææµç¨‹
                    for i in range(gaussiansN):
                        # SSS: Apply stricter point control for Student's t distributions
                        if hasattr(GsDict[f"gs{i}"], 'use_student_t') and GsDict[f"gs{i}"].use_student_t:
                            # Reduce max points for SSS to prevent performance issues
                            max_points_sss = min(opt.max_num_gaussians, 50000)  # Cap at 50k for SSS
                            current_points = GsDict[f"gs{i}"].get_xyz.shape[0]
                            
                            # More aggressive pruning for SSS
                            if current_points > max_points_sss * 0.8:  # Start aggressive pruning at 80% 
                                sss_grad_threshold = opt.densify_grad_threshold * 1.5  # Harder to densify
                                sss_density_threshold = opt.density_min_threshold * 0.8  # Easier to prune
                            else:
                                sss_grad_threshold = opt.densify_grad_threshold
                                sss_density_threshold = opt.density_min_threshold
                            
                            print(f"ğŸ“ [SSS-Control] Iter {iteration}: GS{i} has {current_points} points (max: {max_points_sss})")
                            
                            # ä½¿ç”¨å¢å¼ºç‰ˆå¯†åŒ–å‡½æ•° (FSGS proximity-guided)
                            if hasattr(GsDict[f"gs{i}"], 'enhanced_densify_and_prune'):
                                print(f"âœ… [Densify] Iter {iteration}: GS{i} ä½¿ç”¨ FSGS enhanced_densify_and_prune")
                                GsDict[f"gs{i}"].enhanced_densify_and_prune(
                                    sss_grad_threshold,
                                    sss_density_threshold,
                                    opt.max_screen_size,
                                    max_scale,
                                    max_points_sss,  # Use SSS-specific limit
                                    densify_scale_threshold,
                                    bbox,
                                    enable_proximity_densify=enable_fsgs_proximity,
                                )
                            else:
                                # å›é€€åˆ°æ ‡å‡†å¯†åŒ–
                                print(f"âš ï¸ [Densify] Iter {iteration}: GS{i} å›é€€åˆ°æ ‡å‡† densify_and_prune (æ— FSGS)")
                                GsDict[f"gs{i}"].densify_and_prune(
                                    sss_grad_threshold,
                                    sss_density_threshold,
                                    opt.max_screen_size,
                                    max_scale,
                                    max_points_sss,  # Use SSS-specific limit
                                    densify_scale_threshold,
                                    bbox,
                                )
                        else:
                            # Standard densification for non-SSS gaussians
                            # ä½¿ç”¨å¢å¼ºç‰ˆå¯†åŒ–å‡½æ•° (FSGS proximity-guided)
                            if hasattr(GsDict[f"gs{i}"], 'enhanced_densify_and_prune'):
                                print(f"âœ… [Densify] Iter {iteration}: GS{i} ä½¿ç”¨ FSGS enhanced_densify_and_prune (æ ‡å‡†æ¨¡å¼)")
                                GsDict[f"gs{i}"].enhanced_densify_and_prune(
                        opt.densify_grad_threshold,
                        opt.density_min_threshold,
                        opt.max_screen_size,
                        max_scale,
                        opt.max_num_gaussians,
                        densify_scale_threshold,
                        bbox,
                                    enable_proximity_densify=enable_fsgs_proximity,
                                )
                            else:
                                # å›é€€åˆ°æ ‡å‡†å¯†åŒ–
                                print(f"âš ï¸ [Densify] Iter {iteration}: GS{i} å›é€€åˆ°æ ‡å‡† densify_and_prune (æ— FSGS)")
                                GsDict[f"gs{i}"].densify_and_prune(
                                    opt.densify_grad_threshold,
                                    opt.density_min_threshold,
                                    opt.max_screen_size,
                                    max_scale,
                                    opt.max_num_gaussians,
                                    densify_scale_threshold,
                                    bbox,
                                )
            
            # Density decayåŠŸèƒ½ - åœ¨densificationå¼€å§‹åå¯¹å¯†åº¦è¿›è¡Œè¡°å‡
            if dataset.opacity_decay and iteration > opt.densify_from_iter:
                opt.densify_until_iter = opt.iterations
                for i in range(gaussiansN):
                    GsDict[f"gs{i}"].density_decay(factor=0.995)
            
            # æ£€æŸ¥é«˜æ–¯åœºæ˜¯å¦ä¸ºç©º
            for i in range(gaussiansN):
                if GsDict[f"gs{i}"].get_density.shape[0] == 0:
                    raise ValueError(
                        f"No Gaussian left in gs{i}. Change adaptive control hyperparameters!"
                    )

            # ä¼˜åŒ–å™¨æ›´æ–° - ä¸ºæ¯ä¸ªé«˜æ–¯åœº
            if iteration < opt.iterations:
                for i in range(gaussiansN):
                    # ğŸ¯ [SSS-RÂ²] Apply gradient clipping for stability
                    if hasattr(GsDict[f"gs{i}"], 'use_student_t') and GsDict[f"gs{i}"].use_student_t:
                        # å›ºå®šæ¢¯åº¦è£å‰ªé˜ˆå€¼,ç®€åŒ–è®­ç»ƒæµç¨‹
                        nu_clip_norm = 0.5
                        opacity_clip_norm = 1.0
                        xyz_clip_norm = 2.0

                        # Nu parameter gradient clipping
                        if hasattr(GsDict[f"gs{i}"], '_nu') and GsDict[f"gs{i}"]._nu.grad is not None:
                            torch.nn.utils.clip_grad_norm_(GsDict[f"gs{i}"]._nu, max_norm=nu_clip_norm)

                        # Opacity parameter gradient clipping
                        if hasattr(GsDict[f"gs{i}"], '_opacity') and GsDict[f"gs{i}"]._opacity.grad is not None:
                            torch.nn.utils.clip_grad_norm_(GsDict[f"gs{i}"]._opacity, max_norm=opacity_clip_norm)

                        # Position gradient clipping (standard for all models)
                        if GsDict[f"gs{i}"]._xyz.grad is not None:
                            torch.nn.utils.clip_grad_norm_(GsDict[f"gs{i}"]._xyz, max_norm=xyz_clip_norm)
                    
                    GsDict[f"gs{i}"].optimizer.step()
                    GsDict[f"gs{i}"].optimizer.zero_grad(set_to_none=True)

            # ä¿å­˜é«˜æ–¯æ¨¡å‹
            if iteration in saving_iterations or iteration == opt.iterations:
                tqdm.write(f"[ITER {iteration}] Saving Gaussians")
                scene.save(iteration, queryfunc)
                
                # ä¿å­˜é¢å¤–çš„é«˜æ–¯åœº
                if gaussiansN > 1:
                    for i in range(1, gaussiansN):
                        pcd_path = osp.join(scene.model_path, f"point_cloud_gs{i}/iteration_{iteration}")
                        os.makedirs(pcd_path, exist_ok=True)
                        GsDict[f"gs{i}"].save_ply(osp.join(pcd_path, "point_cloud.ply"))

            # ä¿å­˜æ–­ç‚¹
            if iteration in checkpoint_iterations:
                tqdm.write(f"[ITER {iteration}] Saving Checkpoint")
                checkpoint_data = {}
                for i in range(gaussiansN):
                    checkpoint_data[f"gs{i}"] = GsDict[f"gs{i}"].capture()
                checkpoint_data["iteration"] = iteration
                torch.save(checkpoint_data, ckpt_save_path + "/chkpnt" + str(iteration) + ".pth")

            # è¿›åº¦æ¡æ˜¾ç¤º
            if iteration % 10 == 0:
                # è®¡ç®—æ€»æŸå¤±å’Œæ€»ç‚¹æ•°
                total_loss = sum(LossDict[f"loss_gs{i}"].item() for i in range(gaussiansN))
                total_points = sum(GsDict[f"gs{i}"].get_density.shape[0] for i in range(gaussiansN))
                
                progress_bar.set_postfix(
                    {
                        "loss": f"{total_loss:.1e}",
                        "pts": f"{total_points:2.1e}",
                        "gs": f"{gaussiansN}",
                    }
                )
                progress_bar.update(10)
            if iteration == opt.iterations:
                progress_bar.close()

            # æ—¥å¿—è®°å½•ä¸è¯„ä¼°
            metrics = {}
            for i in range(gaussiansN):
                metrics[f"loss_gs{i}"] = LossDict[f"loss_gs{i}"].item()
                for param_group in GsDict[f"gs{i}"].optimizer.param_groups:
                    metrics[f"lr_gs{i}_{param_group['name']}"] = param_group["lr"]
            training_report(
                tb_writer,
                iteration,
                metrics,
                iter_start.elapsed_time(iter_end),
                testing_iterations,
                scene,
                lambda x, y, it=iteration: render(
                    x,
                    y,
                    pipe,
                    enable_drop=args.enable_drop,
                    drop_rate=args.drop_rate if hasattr(args, 'drop_rate') else 0.10,
                    iteration=it,
                ),
                queryfunc,
                gaussiansN,
                GsDict=GsDict,  # ğŸ¯ CoR-GS: ä¼ é€’ GsDict
                pipe=pipe,      # ğŸ¯ CoR-GS: ä¼ é€’ pipe å‚æ•°
                background=background,  # ğŸ¯ CoR-GS: ä¼ é€’èƒŒæ™¯é¢œè‰²
            )


def training_report(
    tb_writer,
    iteration,
    metrics_train,
    elapsed,
    testing_iterations,
    scene: Scene,
    renderFunc,
    queryFunc,
    gaussiansN=1,
    GsDict=None,  # ğŸ¯ CoR-GS: ä¼ é€’ GsDict ä»¥è®¿é—®å¤šä¸ªæ¨¡å‹
    pipe=None,    # ğŸ¯ CoR-GS: Pipeline å‚æ•° (ç”¨äºæ¸²æŸ“)
    background=None,  # ğŸ¯ CoR-GS: èƒŒæ™¯é¢œè‰²
):
    """
    è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯„ä¼°ä¸æ—¥å¿—è®°å½•ï¼ŒåŒ…æ‹¬è®­ç»ƒç»Ÿè®¡ã€2Dæ¸²æŸ“æ€§èƒ½ã€3Dé‡å»ºæ€§èƒ½ç­‰ã€‚
    """
    # ğŸ¯ [DEBUG] è°ƒè¯• CoR-GS æ—¥å¿—è®°å½•
    if iteration % 500 == 0:
        print(f"[DEBUG-REPORT] Iter {iteration}: gaussiansN={gaussiansN}, GsDict={GsDict is not None}, tb_writer={tb_writer is not None}", flush=True)

    # è®°å½•è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
    if tb_writer:
        for key in list(metrics_train.keys()):
            tb_writer.add_scalar(f"train/{key}", metrics_train[key], iteration)
        tb_writer.add_scalar("train/iter_time", elapsed, iteration)
        tb_writer.add_scalar(
            "train/total_points", scene.gaussians.get_xyz.shape[0], iteration
        )

        # âŒ ç¦ç”¨ CoR-GS Disagreement æ—¥å¿—è®°å½•ï¼ˆä¸ç¡®å®šå®ç°æ˜¯å¦æ­£ç¡®ï¼‰
        # ä¸“æ³¨äº FSGS åŠŸèƒ½éªŒè¯
        enable_corgs_logging = False  # å¼ºåˆ¶ç¦ç”¨
        if iteration % 500 == 0:  # è°ƒè¯•è¾“å‡º
            print(f"[DEBUG-CORGS-1] Iter {iteration}: enable_corgs_logging={enable_corgs_logging}", flush=True)
        if enable_corgs_logging:
            # ä»…åœ¨æŒ‡å®šé¢‘ç‡è®°å½• (é¿å…é¢å¤–è®¡ç®—å¼€é”€)
            log_freq = 500  # é»˜è®¤é¢‘ç‡
            if iteration % log_freq == 0:
                print(f"[DEBUG-CORGS-2] Iter {iteration}: Entering CoR-GS logging block", flush=True)
                try:
                    from r2_gaussian.utils.corgs_metrics import log_corgs_metrics
                    print(f"[DEBUG-CORGS-3] Import successful", flush=True)

                    # è·å–å‰ä¸¤ä¸ª Gaussian æ¨¡å‹
                    gaussians_1 = GsDict.get("gs0", scene.gaussians)
                    gaussians_2 = GsDict.get("gs1", None)
                    print(f"[DEBUG-CORGS-4] gs2={gaussians_2 is not None}, pipe={pipe is not None}", flush=True)

                    if gaussians_2 is not None and pipe is not None:
                        # å‡†å¤‡å‚æ•°
                        threshold = 0.3  # é»˜è®¤é˜ˆå€¼
                        test_cameras = scene.getTestCameras()
                        print(f"[DEBUG-CORGS-5] test_cameras length={len(test_cameras)}", flush=True)
                        if len(test_cameras) > 0:
                            test_camera = test_cameras[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæµ‹è¯•ç›¸æœº
                            bg_color = background if background is not None else torch.tensor([0, 0, 0], dtype=torch.float32, device="cuda")

                            # è®¡ç®—æ‰€æœ‰ Disagreement æŒ‡æ ‡
                            corgs_metrics = log_corgs_metrics(
                                gaussians_1,
                                gaussians_2,
                                test_camera,
                                pipe,
                                bg_color,
                                threshold=threshold
                            )

                            # è®°å½•åˆ° TensorBoard
                            for metric_name, metric_value in corgs_metrics.items():
                                tb_writer.add_scalar(f"corgs/{metric_name}", metric_value, iteration)

                            # æ‰“å°æ—¥å¿— (å¯é€‰)
                            print(f"[CoR-GS Metrics @ Iter {iteration}] "
                                  f"Fitness={corgs_metrics['point_fitness']:.4f}, "
                                  f"RMSE={corgs_metrics['point_rmse']:.6f}, "
                                  f"PSNR_diff={corgs_metrics['render_psnr_diff']:.2f} dB")

                except ImportError as e:
                    print(f"âš ï¸ CoR-GS metrics module not available: {e}")
                except Exception as e:
                    print(f"âš ï¸ Error computing CoR-GS metrics: {e}")

    # æµ‹è¯•ä¸è¯„ä¼°
    if iteration in testing_iterations:
        # 2Dæ¸²æŸ“æ€§èƒ½è¯„ä¼°
        eval_save_path = osp.join(scene.model_path, "eval", f"iter_{iteration:06d}")
        os.makedirs(eval_save_path, exist_ok=True)
        torch.cuda.empty_cache()

        validation_configs = [
            {"name": "render_train", "cameras": scene.getTrainCameras()},
            {"name": "render_test", "cameras": scene.getTestCameras()},
        ]
        psnr_2d, ssim_2d = None, None
        for config in validation_configs:
            if config["cameras"] and len(config["cameras"]) > 0:
                images = []
                gt_images = []
                image_show_2d = []
                # æ¸²æŸ“æ‰€æœ‰è§†è§’
                show_idx = np.linspace(0, len(config["cameras"]), 7).astype(int)[1:-1]
                for idx, viewpoint in enumerate(config["cameras"]):
                    image = renderFunc(
                        viewpoint,
                        scene.gaussians,
                    )["render"]
                    gt_image = viewpoint.original_image.to("cuda")
                    images.append(image)
                    gt_images.append(gt_image)
                    
                    # ä¿å­˜å•ç‹¬çš„æ¸²æŸ“å›¾åƒï¼ˆPNGæ ¼å¼ï¼‰
                    if idx in show_idx:
                        # åˆ›å»ºå¯è§†åŒ–è¾“å‡ºç›®å½•
                        vis_output_dir = osp.join(eval_save_path, "render_images")
                        os.makedirs(vis_output_dir, exist_ok=True)
                        
                        # ä¿å­˜GTå›¾åƒ
                        gt_img_np = gt_image[0].detach().cpu().numpy()
                        gt_img_np = np.clip(gt_img_np, 0, 1) * 255
                        gt_save_path = osp.join(vis_output_dir, f"{viewpoint.image_name}_gt.png")
                        plt.imsave(gt_save_path, gt_img_np, cmap='viridis')
                        
                        # ä¿å­˜æ¸²æŸ“å›¾åƒ  
                        render_img_np = image[0].detach().cpu().numpy()
                        render_img_np = np.clip(render_img_np, 0, 1) * 255
                        render_save_path = osp.join(vis_output_dir, f"{viewpoint.image_name}_render.png")
                        plt.imsave(render_save_path, render_img_np, cmap='viridis')
                        
                        # ä¿å­˜å¯¹æ¯”å›¾ï¼ˆå·®å¼‚å›¾ï¼‰
                        diff_img = np.abs(gt_img_np - render_img_np)
                        diff_save_path = osp.join(vis_output_dir, f"{viewpoint.image_name}_diff.png")
                        plt.imsave(diff_save_path, diff_img, cmap='hot')
                        
                        print(f"ğŸ’¾ ä¿å­˜æ¸²æŸ“å›¾åƒ: {viewpoint.image_name} åˆ° {vis_output_dir}")
                    
                    if tb_writer and idx in show_idx:
                        image_show_2d.append(
                            torch.from_numpy(
                                show_two_slice(
                                    gt_image[0],
                                    image[0],
                                    f"{viewpoint.image_name} gt",
                                    f"{viewpoint.image_name} render",
                                    vmin=gt_image[0].min() if iteration != 1 else None,
                                    vmax=gt_image[0].max() if iteration != 1 else None,
                                    save=True,
                                )
                            )
                        )
                images = torch.concat(images, 0).permute(1, 2, 0)
                gt_images = torch.concat(gt_images, 0).permute(1, 2, 0)
                psnr_2d, psnr_2d_projs = metric_proj(gt_images, images, "psnr")
                ssim_2d, ssim_2d_projs = metric_proj(gt_images, images, "ssim")
                eval_dict_2d = {
                    "psnr_2d": psnr_2d,
                    "ssim_2d": ssim_2d,
                    "psnr_2d_projs": psnr_2d_projs,
                    "ssim_2d_projs": ssim_2d_projs,
                }
                with open(
                    osp.join(eval_save_path, f"eval2d_{config['name']}.yml"),
                    "w",
                ) as f:
                    yaml.dump(
                        eval_dict_2d, f, default_flow_style=False, sort_keys=False
                    )

                if tb_writer:
                    image_show_2d = torch.from_numpy(
                        np.concatenate(image_show_2d, axis=0)
                    )[None].permute([0, 3, 1, 2])
                    tb_writer.add_images(
                        config["name"] + f"/{viewpoint.image_name}",
                        image_show_2d,
                        global_step=iteration,
                    )
                    tb_writer.add_scalar(
                        config["name"] + "/psnr_2d", psnr_2d, iteration
                    )
                    tb_writer.add_scalar(
                        config["name"] + "/ssim_2d", ssim_2d, iteration
                    )

        # 3Dé‡å»ºæ€§èƒ½è¯„ä¼°
        vol_pred = queryFunc(scene.gaussians)["vol"]
        vol_gt = scene.vol_gt
        psnr_3d, _ = metric_vol(vol_gt, vol_pred, "psnr")
        ssim_3d, ssim_3d_axis = metric_vol(vol_gt, vol_pred, "ssim")
        eval_dict = {
            "psnr_3d": psnr_3d,
            "ssim_3d": ssim_3d,
            "ssim_3d_x": ssim_3d_axis[0],
            "ssim_3d_y": ssim_3d_axis[1],
            "ssim_3d_z": ssim_3d_axis[2],
        }
        with open(osp.join(eval_save_path, "eval3d.yml"), "w") as f:
            yaml.dump(eval_dict, f, default_flow_style=False, sort_keys=False)
        if tb_writer:
            image_show_3d = np.concatenate(
                [
                    show_two_slice(
                        vol_gt[..., i],
                        vol_pred[..., i],
                        f"slice {i} gt",
                        f"slice {i} pred",
                        vmin=vol_gt[..., i].min(),
                        vmax=vol_gt[..., i].max(),
                        save=True,
                    )
                    for i in np.linspace(0, vol_gt.shape[2], 7).astype(int)[1:-1]
                ],
                axis=0,
            )
            image_show_3d = torch.from_numpy(image_show_3d)[None].permute([0, 3, 1, 2])
            tb_writer.add_images(
                "reconstruction/slice-gt_pred_diff",
                image_show_3d,
                global_step=iteration,
            )
            tb_writer.add_scalar("reconstruction/psnr_3d", psnr_3d, iteration)
            tb_writer.add_scalar("reconstruction/ssim_3d", ssim_3d, iteration)
        tqdm.write(
            f"[ITER {iteration}] Evaluating: psnr3d {psnr_3d:.4f}, ssim3d {ssim_3d:.4f}, psnr2d {psnr_2d:.4f}, ssim2d {ssim_2d:.4f}"
        )

        # è®°å½•å…¶ä»–æŒ‡æ ‡
        if tb_writer:
            tb_writer.add_histogram(
                "scene/density_histogram", scene.gaussians.get_density, iteration
            )

    torch.cuda.empty_cache()


if __name__ == "__main__":
    # å‘½ä»¤è¡Œå…¥å£ï¼Œå‚æ•°è§£æä¸è®­ç»ƒå¯åŠ¨
    # fmt: off
    # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = ArgumentParser(description="Training script parameters") 
    lp = ModelParams(parser)
    op = OptimizationParams(parser)
    pp = PipelineParams(parser)
    parser.add_argument("--detect_anomaly", action="store_true", default=False)  # æ˜¯å¦å¼€å¯å¼‚å¸¸æ£€æµ‹
    parser.add_argument("--test_iterations", nargs="+", type=int, default=[5_000, 10_000, 20_000])  # æµ‹è¯•è¿­ä»£
    parser.add_argument("--save_iterations", nargs="+", type=int, default=[])  # ä¿å­˜è¿­ä»£
    parser.add_argument("--quiet", action="store_true")  # é™é»˜æ¨¡å¼
    parser.add_argument("--checkpoint_iterations", nargs="+", type=int, default=[])  # æ–­ç‚¹ä¿å­˜è¿­ä»£
    parser.add_argument("--start_checkpoint", type=str, default=None)  # èµ·å§‹æ–­ç‚¹
    parser.add_argument("--config", type=str, default=None)  # é…ç½®æ–‡ä»¶è·¯å¾„
    parser.add_argument("--enable_drop", action="store_true", default=False)  # æ˜¯å¦å¯ç”¨ drop æ–¹æ³•
    parser.add_argument("--drop_rate", type=float, default=0.10)  # drop æ¯”ä¾‹ï¼ˆ0~1ï¼‰
    
    # SSS: Student Splatting and Scooping å‚æ•°
    parser.add_argument("--enable_sss", action="store_true", default=False)  # æ˜¯å¦å¯ç”¨SSS
    parser.add_argument("--sghmc_friction", type=float, default=0.1)  # SGHMCæ‘©æ“¦ç³»æ•°
    parser.add_argument("--sghmc_burnin_steps", type=int, default=1000)  # SGHMCçƒ§å…¥æ­¥æ•°
    parser.add_argument("--nu_lr_init", type=float, default=0.001)  # nuå‚æ•°åˆå§‹å­¦ä¹ ç‡
    parser.add_argument("--opacity_lr_init", type=float, default=0.01)  # opacityå‚æ•°åˆå§‹å­¦ä¹ ç‡
    
    # FSGS Proximity-Guided Densification å‚æ•°åœ¨arguments/__init__.pyä¸­å·²å®šä¹‰
    
    # æ—§ç‰ˆæœ¬ Proximity-Guided Densification å‚æ•° (å…¼å®¹æ€§ä¿ç•™)
    parser.add_argument("--enable_proximity_guided", action="store_true", default=False)  # æ˜¯å¦å¯ç”¨æ—§ç‰ˆproximity-guidedå¯†åŒ–
    parser.add_argument("--proximity_interval", type=int, default=1000)  # proximityå¯†åŒ–é—´éš”
    parser.add_argument("--proximity_max_points", type=int, default=500)  # æ¯æ¬¡proximityå¯†åŒ–æœ€å¤§ç‚¹æ•°

    # CoR-GS Stage 3 å‚æ•° (Pseudo-view Co-regularization - 2025-11-17)
    parser.add_argument("--enable_pseudo_coreg", action="store_true", default=False,
                        help="å¯ç”¨ CoR-GS Stage 3 Pseudo-view Co-regularization")
    parser.add_argument("--lambda_pseudo", type=float, default=1.0,
                        help="Pseudo-view co-regularization æŸå¤±æƒé‡")
    parser.add_argument("--pseudo_noise_std", type=float, default=0.02,
                        help="Pseudo-view ç›¸æœºä½ç½®éšæœºæ‰°åŠ¨æ ‡å‡†å·®")
    parser.add_argument("--pseudo_start_iter", type=int, default=0,
                        help="å¼€å§‹åº”ç”¨ pseudo-view co-reg çš„ iteration")

    args = parser.parse_args(sys.argv[1:])
    args.save_iterations.append(args.iterations)
    args.test_iterations.append(args.iterations)
    args.test_iterations.append(1)
    # fmt: on

    # åˆå§‹åŒ–ç³»ç»ŸçŠ¶æ€ï¼ˆå¦‚éšæœºç§å­ï¼‰
    safe_state(args.quiet)

    # åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœ‰ï¼‰
    args_dict = vars(args)
    if args.config is not None:
        print(f"Loading configuration file from {args.config}")
        cfg = load_config(args.config)
        for key in list(cfg.keys()):
            args_dict[key] = cfg[key]

    # è®¾ç½®æ—¥å¿—ä¸è¾“å‡º
    tb_writer = prepare_output_and_logger(args)

    print("Optimizing " + args.model_path)

    # æ˜¯å¦å¼€å¯å¼‚å¸¸æ£€æµ‹
    torch.autograd.set_detect_anomaly(args.detect_anomaly)
    # å¯åŠ¨è®­ç»ƒä¸»å¾ªç¯
    training(
        lp.extract(args),
        op.extract(args),
        pp.extract(args),
        tb_writer,
        args.test_iterations,
        args.save_iterations,
        args.checkpoint_iterations,
        args.start_checkpoint,
        args.gaussiansN,
        args.coreg,
        args.coprune,
        args.coprune_threshold,
        args,
    )
    
    # æ³¨æ„ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ render å‡½æ•°æ—¶ï¼Œéœ€è¦ä¼ é€’ enable_drop=args.enable_drop å‚æ•°

    # è®­ç»ƒç»“æŸ
    print("Training complete.")
