#
# Copyright (C) 2023, Inria
# GRAPHDECO research group, https://team.inria.fr/graphdeco
# All rights reserved.
#
# This software is free for non-commercial, research and evaluation use
# under the terms of the LICENSE.md file.
#
# For inquiries contact  george.drettakis@inria.fr
#

import torch
import torch.nn.functional as F
from torch.autograd import Variable
from math import exp
import torch.nn as nn

# æ·»åŠ torchmetricsä¾èµ–ç”¨äºæ·±åº¦ç›¸å…³æ€§è®¡ç®—
try:
    from torchmetrics.functional.regression import pearson_corrcoef
except ImportError:
    # å¦‚æœtorchmetricsä¸å¯ç”¨ï¼Œä½¿ç”¨PyTorchå†…ç½®å‡½æ•°
    def pearson_corrcoef(pred, target):
        """ç®€å•çš„çš®å°”é€Šç›¸å…³ç³»æ•°è®¡ç®—"""
        pred_mean = pred.mean()
        target_mean = target.mean()
        numerator = ((pred - pred_mean) * (target - target_mean)).sum()
        denominator = torch.sqrt(((pred - pred_mean) ** 2).sum() * ((target - target_mean) ** 2).sum())
        return numerator / (denominator + 1e-8)


def tv_3d_loss(vol, reduction="sum"):

    dx = torch.abs(torch.diff(vol, dim=0))
    dy = torch.abs(torch.diff(vol, dim=1))
    dz = torch.abs(torch.diff(vol, dim=2))

    tv = torch.sum(dx) + torch.sum(dy) + torch.sum(dz)

    if reduction == "mean":
        total_elements = (
            (vol.shape[0] - 1) * vol.shape[1] * vol.shape[2]
            + vol.shape[0] * (vol.shape[1] - 1) * vol.shape[2]
            + vol.shape[0] * vol.shape[1] * (vol.shape[2] - 1)
        )
        tv = tv / total_elements
    return tv


def l1_loss(network_output, gt):
    return torch.abs((network_output - gt)).mean()


def l2_loss(network_output, gt):
    return ((network_output - gt) ** 2).mean()


def gaussian(window_size, sigma):
    gauss = torch.Tensor(
        [
            exp(-((x - window_size // 2) ** 2) / float(2 * sigma**2))
            for x in range(window_size)
        ]
    )
    return gauss / gauss.sum()


def create_window(window_size, channel):
    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)
    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)
    window = Variable(
        _2D_window.expand(channel, 1, window_size, window_size).contiguous()
    )
    return window


def ssim(img1, img2, window_size=11, size_average=True, mask=None):
    channel = img1.size(-3)
    window = create_window(window_size, channel)

    if img1.is_cuda:
        window = window.cuda(img1.get_device())
    window = window.type_as(img1)

    ssim_value = _ssim(img1, img2, window, window_size, channel, size_average)
    
    if mask is not None:
        # Apply mask to SSIM calculation if provided
        ssim_value = ssim_value * mask.mean()
    
    return ssim_value


def loss_photometric(image, gt_image, opt, valid=None):
    """Photometric loss with mask support - å‚è€ƒX-Gaussianå®ç°"""
    Ll1 = l1_loss_mask(image, gt_image, mask=valid)
    loss = ((1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim(image, gt_image, mask=valid)))
    return Ll1, loss


def l1_loss_mask(network_output, gt, mask=None):
    """L1 loss with mask support - å‚è€ƒX-Gaussianå®ç°"""
    if mask is None:
        return l1_loss(network_output, gt)
    else:
        return torch.abs((network_output - gt) * mask).sum() / mask.sum()


def depth_loss(predicted_depth, gt_depth, depth_bounds=None):
    """Depth consistency loss - æ–°å¢æ·±åº¦æŸå¤±å‡½æ•°"""
    # æ·±åº¦ä¸€è‡´æ€§æŸå¤±
    depth_consistency = torch.abs(predicted_depth - gt_depth)
    
    if depth_bounds is not None:
        min_depth, max_depth = depth_bounds
        # æ·±åº¦èŒƒå›´æƒ©ç½š
        depth_range_penalty = torch.where(
            (predicted_depth < min_depth) | (predicted_depth > max_depth),
            torch.abs(predicted_depth - torch.clamp(predicted_depth, min_depth, max_depth)),
            torch.zeros_like(predicted_depth)
        )
        return depth_consistency.mean() + 0.1 * depth_range_penalty.mean()
    
    return depth_consistency.mean()


def pseudo_label_loss(predicted_image, pseudo_label, confidence_mask=None):
    """Pseudo label loss - ä¼ªæ ‡ç­¾æŸå¤±å‡½æ•°"""
    if confidence_mask is not None:
        # ä½¿ç”¨ç½®ä¿¡åº¦maskåŠ æƒ
        loss = torch.abs(predicted_image - pseudo_label) * confidence_mask
        return loss.sum() / confidence_mask.sum()
    else:
        return l1_loss(predicted_image, pseudo_label)


def calculate_depth_loss(rendered_depth, gt_depth):
    """è®¡ç®—æ·±åº¦ç›¸å…³æ€§æŸå¤± - å‚è€ƒX-Gaussian-depthå®ç°"""
    rendered_depth = rendered_depth.reshape(-1, 1)
    gt_depth = gt_depth.reshape(-1, 1)
    
    # ä½¿ç”¨X-Gaussian-depthä¸­çš„æ·±åº¦æŸå¤±è®¡ç®—æ–¹å¼
    depth_loss = min(
        (1 - pearson_corrcoef(-gt_depth.squeeze(), rendered_depth.squeeze())),
        (1 - pearson_corrcoef((1 / (gt_depth + 200.)).squeeze(), rendered_depth.squeeze()))
    )
    return depth_loss


def _ssim(img1, img2, window, window_size, channel, size_average=True):
    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)
    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)

    mu1_sq = mu1.pow(2)
    mu2_sq = mu2.pow(2)
    mu1_mu2 = mu1 * mu2

    sigma1_sq = (
        F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq
    )
    sigma2_sq = (
        F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq
    )
    sigma12 = (
        F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel)
        - mu1_mu2
    )

    C1 = 0.01**2
    C2 = 0.03**2

    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / (
        (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)
    )

    if size_average:
        return ssim_map.mean()
    else:
        return ssim_map.mean(1).mean(1).mean(1)


def depth_loss_fn(rendered_depth, gt_depth, loss_type='pearson'):
    """
    è®¡ç®—æ·±åº¦æŸå¤±å‡½æ•°
    
    Args:
        rendered_depth: æ¸²æŸ“çš„æ·±åº¦å›¾ (H, W)
        gt_depth: çœŸå®æ·±åº¦å›¾ (H, W)
        loss_type: æŸå¤±ç±»å‹ ('l1', 'l2', 'pearson')
    
    Returns:
        loss: æ·±åº¦æŸå¤±å€¼
    """
    if rendered_depth is None or gt_depth is None:
        return torch.tensor(0.0, device=rendered_depth.device if rendered_depth is not None else gt_depth.device)
    
    # ç¡®ä¿ä¸¤ä¸ªå¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
    if rendered_depth.device != gt_depth.device:
        gt_depth = gt_depth.to(rendered_depth.device)
    
    # ç¡®ä¿å½¢çŠ¶ä¸€è‡´
    if rendered_depth.shape != gt_depth.shape:
        # å¦‚æœå½¢çŠ¶ä¸åŒï¼Œå°è¯•è°ƒæ•´å¤§å°
        if len(rendered_depth.shape) == 2 and len(gt_depth.shape) == 2:
            gt_depth = F.interpolate(gt_depth.unsqueeze(0).unsqueeze(0), 
                                    size=rendered_depth.shape, 
                                    mode='bilinear', 
                                    align_corners=False).squeeze(0).squeeze(0)
        else:
            raise ValueError(f"Shape mismatch: {rendered_depth.shape} vs {gt_depth.shape}")
    
    if loss_type == 'l1':
        return F.l1_loss(rendered_depth, gt_depth)
    elif loss_type == 'l2':
        return F.mse_loss(rendered_depth, gt_depth)
    elif loss_type == 'pearson':
        # ä½¿ç”¨çš®å°”é€Šç›¸å…³ç³»æ•°ä½œä¸ºæŸå¤±
        correlation = pearson_corrcoef(rendered_depth.flatten(), gt_depth.flatten())
        return 1.0 - correlation  # è½¬æ¢ä¸ºæŸå¤±ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    else:
        raise ValueError(f"Unknown loss type: {loss_type}")


def depth_consistency_loss(depth_maps):
    """
    è®¡ç®—å¤šè§†è§’æ·±åº¦ä¸€è‡´æ€§æŸå¤±
    
    Args:
        depth_maps: å¤šä¸ªè§†è§’çš„æ·±åº¦å›¾åˆ—è¡¨
    
    Returns:
        consistency_loss: ä¸€è‡´æ€§æŸå¤±
    """
    if len(depth_maps) < 2:
        return torch.tensor(0.0, device=depth_maps[0].device)
    
    # ç®€åŒ–çš„å®ç°ï¼šè®¡ç®—ç›¸é‚»è§†è§’æ·±åº¦å›¾çš„å·®å¼‚
    total_loss = 0.0
    for i in range(len(depth_maps) - 1):
        diff = torch.abs(depth_maps[i] - depth_maps[i + 1])
        total_loss += diff.mean()
    
    return total_loss / (len(depth_maps) - 1)


def compute_graph_laplacian_loss(gaussians, graph=None, k=6, Lambda_lap=8e-4):
    """
    å›¾æ‹‰æ™®æ‹‰æ–¯æ­£åˆ™åŒ–æŸå¤± - GR-Gaussian å¢å¼ºç‰ˆæœ¬

    åŠŸèƒ½:
        - é¼“åŠ±ç›¸é‚»é«˜æ–¯ç‚¹çš„å¯†åº¦å¹³æ»‘
        - ä¸ depth çº¦æŸäº’è¡¥,æŠ‘åˆ¶å¯†åº¦è·³å˜

    GPU åŠ é€Ÿç‰ˆæœ¬ï¼ˆå¸¦è‡ªåŠ¨å›é€€åˆ° CPUï¼‰ï¼š
        - ä¼˜å…ˆä½¿ç”¨ GPU åŠ é€Ÿè®¡ç®—ï¼ˆtorch.cdist + topkï¼‰
        - å¦‚æœ GPU å†…å­˜ä¸è¶³æˆ–å‡ºé”™,è‡ªåŠ¨å›é€€åˆ° CPU ç‰ˆæœ¬ï¼ˆsklearnï¼‰

    Args:
        gaussians: GaussianModel å®ä¾‹
        graph: GaussianGraph å¯¹è±¡ (å¯é€‰,GR-Gaussian æ¨¡å¼)
        k: KNN é‚»å±…æ•°é‡ï¼ˆé»˜è®¤6,æ ¹æ® CoR-GS/GR-Gaussian è®ºæ–‡ï¼‰
        Lambda_lap: æ­£åˆ™åŒ–æƒé‡ï¼ˆé»˜è®¤ 8e-4ï¼‰

    Returns:
        loss: æ ‡é‡æŸå¤±å€¼

    å®ç°æ¨¡å¼:
        1. å¦‚æœæä¾› graph å¯¹è±¡ (GR-Gaussian): ä½¿ç”¨é¢„æ„å»ºçš„è¾¹ç´¢å¼•
        2. å¦åˆ™ (CoR-GS fallback): åŠ¨æ€æ„å»º KNN å›¾
    """
    import torch

    # è·å–é«˜æ–¯ç‚¹ä½ç½®å’Œå¯†åº¦
    xyz = gaussians.get_xyz  # (N, 3)
    density = gaussians.get_density  # (N,)

    N = xyz.shape[0]
    if N < k + 1:
        return torch.tensor(0.0, device=xyz.device, requires_grad=True)

    # ğŸŒŸ [GR-Gaussian] ä½¿ç”¨é¢„æ„å»ºå›¾
    if graph is not None and hasattr(graph, 'edge_index') and graph.edge_index is not None:
        # âœ… å…³é”®ä¿®å¤ï¼šæ£€æŸ¥å›¾çš„èŠ‚ç‚¹æ•°æ˜¯å¦ä¸å½“å‰é«˜æ–¯ç‚¹æ•°åŒ¹é…
        if graph.num_nodes != N:
            # é«˜æ–¯ç‚¹æ•°é‡å˜åŒ–äº†ï¼ˆdue to densification/pruningï¼‰ï¼Œå›¾å·²è¿‡æœŸ
            # è·³è¿‡æœ¬æ¬¡ Graph Laplacian æŸå¤±è®¡ç®—ï¼Œç­‰å¾…ä¸‹æ¬¡å›¾æ›´æ–°
            return torch.tensor(0.0, device=xyz.device, requires_grad=True)

        src, dst = graph.edge_index[0], graph.edge_index[1]

        # âš ï¸ é¢å¤–éªŒè¯ï¼šæ£€æŸ¥ç´¢å¼•æ˜¯å¦è¶Šç•Œ
        if src.numel() > 0 and (src.max() >= N or dst.max() >= N):
            # ç´¢å¼•è¶Šç•Œï¼Œå›¾ç»“æ„æ— æ•ˆ
            return torch.tensor(0.0, device=xyz.device, requires_grad=True)

        # âœ… ä¿®å¤ï¼šå§‹ç»ˆé‡æ–°è®¡ç®—è¾¹æƒé‡ï¼Œå› ä¸º xyz ä½ç½®åœ¨è®­ç»ƒä¸­ä¸æ–­å˜åŒ–
        # è®¡ç®—è¾¹çš„æ¬§æ°è·ç¦»
        distances = torch.norm(xyz[src] - xyz[dst], dim=1)  # (E,)

        # è®¡ç®—æƒé‡ï¼ˆé«˜æ–¯æ ¸ï¼‰
        sigma = distances.mean() + 1e-7
        weights = torch.exp(-distances / sigma)  # (E,)

        # è®¡ç®—å¯†åº¦å·®å¼‚
        density_diff = density[src] - density[dst]  # (E,)

        # âš ï¸ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½æ˜¯ 1Dï¼Œé¿å… broadcasting å¯¼è‡´ OOM
        # å¼ºåˆ¶ flatten é˜²æ­¢æ„å¤–çš„å½¢çŠ¶ï¼ˆå¦‚ (E, 1) å¯¼è‡´ broadcasting æˆ (E, E)ï¼‰
        weights = weights.flatten()  # ç¡®ä¿ (E,)
        density_diff = density_diff.flatten()  # ç¡®ä¿ (E,)

        # åŠ æƒå¹³æ–¹å·®
        weighted_loss = weights * (density_diff ** 2)  # (E,)
        loss = weighted_loss.mean() * Lambda_lap

        return loss

    # âœ… [GR-Gaussian ä¿®å¤] å½“æ²¡æœ‰é¢„æ„å»ºå›¾æ—¶ï¼Œä½¿ç”¨ GPU åŠ é€Ÿçš„åŠ¨æ€ KNN è®¡ç®—
    # å°è¯•GPUåŠ é€Ÿç‰ˆæœ¬ï¼ˆä¼˜å…ˆï¼‰
    try:
        # æ£€æŸ¥ç‚¹æ•°é‡ï¼Œé¿å…GPUå†…å­˜æº¢å‡º
        # å¦‚æœç‚¹æ•°è¿‡å¤šï¼ˆ>10ä¸‡ï¼‰ï¼Œä½¿ç”¨åˆ†æ‰¹å¤„ç†æˆ–å›é€€åˆ°CPU
        max_gpu_points = 100000
        if N > max_gpu_points:
            raise RuntimeError(f"Too many points ({N}) for GPU computation, using CPU fallback")
        
        # GPUåŠ é€ŸKNNæœç´¢ï¼šä½¿ç”¨torch.cdist + topkï¼ˆå®Œå…¨åœ¨GPUä¸Šï¼‰
        # è®¡ç®—æ‰€æœ‰ç‚¹å¯¹ä¹‹é—´çš„æ¬§æ°è·ç¦»
        dists = torch.cdist(xyz, xyz)  # (N, N) - GPUå¹¶è¡Œè®¡ç®—
        
        # è·å–æ¯ä¸ªç‚¹çš„k+1ä¸ªæœ€è¿‘é‚»ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰
        _, indices = torch.topk(dists, k+1, dim=1, largest=False)  # (N, k+1) - GPUè®¡ç®—
        
        # è·³è¿‡è‡ªå·±ï¼ˆç¬¬ä¸€ä¸ªé‚»å±…æ˜¯è‡ªå·±ï¼Œè·ç¦»ä¸º0ï¼‰
        neighbor_indices = indices[:, 1:]  # (N, k) - è·å–kä¸ªçœŸå®é‚»å±…
        
        # æ‰¹é‡è·å–é‚»å±…è·ç¦»ï¼ˆå‘é‡åŒ–æ“ä½œï¼‰
        batch_indices = torch.arange(N, device=xyz.device).unsqueeze(1).expand(-1, k)  # (N, k)
        neighbor_dists = dists[batch_indices, neighbor_indices]  # (N, k) - é‚»å±…è·ç¦»
        
        # è®¡ç®—æƒé‡ï¼ˆé«˜æ–¯æ ¸ï¼šè·ç¦»è¶Šè¿‘æƒé‡è¶Šå¤§ï¼‰
        neighbor_dists_mean = neighbor_dists.mean(dim=1, keepdim=True)  # (N, 1)
        weights = torch.exp(-neighbor_dists / (neighbor_dists_mean + 1e-7))  # (N, k)
        
        # æ‰¹é‡è®¡ç®—å¯†åº¦å·®å¼‚ï¼ˆå‘é‡åŒ–ï¼‰
        density_expanded = density.unsqueeze(1)  # (N, 1)
        density_neighbors = density[neighbor_indices]  # (N, k) - æ‰¹é‡è·å–é‚»å±…å¯†åº¦
        density_diff = density_expanded - density_neighbors  # (N, k)
        
        # åŠ æƒå¹³æ–¹å·®ï¼ˆå‘é‡åŒ–è®¡ç®—ï¼‰
        weighted_loss = weights * (density_diff ** 2)  # (N, k)
        
        # å¹³å‡æŸå¤±å¹¶ä¹˜ä»¥æƒé‡
        loss = weighted_loss.mean() * Lambda_lap
        
        return loss
        
    except RuntimeError as e:
        # GPUè®¡ç®—å¤±è´¥ï¼ˆå†…å­˜ä¸è¶³æˆ–å…¶ä»–CUDAé”™è¯¯ï¼‰ï¼Œå›é€€åˆ°CPUç‰ˆæœ¬
        # æ³¨æ„ï¼šCUDAçš„OOMé”™è¯¯ä¹Ÿä¼šæŠ›å‡ºRuntimeErrorï¼Œæ‰€ä»¥åªæ•è·RuntimeErrorå³å¯
        # åªåœ¨ç¬¬ä¸€æ¬¡å¤±è´¥æ—¶æ‰“å°è­¦å‘Šï¼Œé¿å…æ—¥å¿—è¿‡å¤š
        import warnings
        if not hasattr(compute_graph_laplacian_loss, '_cpu_fallback_warned'):
            warnings.warn(f"GPU computation failed ({str(e)}), falling back to CPU version. "
                        f"This may be slower but will work correctly.")
            compute_graph_laplacian_loss._cpu_fallback_warned = True
        
        # CPUå›é€€ç‰ˆæœ¬ï¼ˆåŸå§‹å®ç°ï¼‰
        from sklearn.neighbors import NearestNeighbors
        
        device = xyz.device
        
        # æ„å»ºKNNå›¾ï¼ˆCPUè®¡ç®—ï¼‰
        xyz_np = xyz.detach().cpu().numpy()
        nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree').fit(xyz_np)
        distances, indices = nbrs.kneighbors(xyz_np)
        
        # è®¡ç®—æ‹‰æ™®æ‹‰æ–¯æŸå¤±ï¼šL = sum_{i,j} w_{ij} * (d_i - d_j)^2
        total_loss = 0.0
        count = 0
        
        for i in range(len(xyz_np)):
            # è·å–kä¸ªé‚»å±…ï¼ˆè·³è¿‡è‡ªå·±ï¼‰
            neighbors = indices[i][1:]
            neighbor_dists = distances[i][1:]
            
            if len(neighbors) == 0:
                continue
                
            # è®¡ç®—æƒé‡ï¼ˆé«˜æ–¯æ ¸ï¼šè·ç¦»è¶Šè¿‘æƒé‡è¶Šå¤§ï¼‰
            weights = torch.exp(-torch.tensor(neighbor_dists, device=device) / (neighbor_dists.mean() + 1e-7))
            
            # è®¡ç®—å¯†åº¦å·®å¼‚
            density_i = density[i]
            density_neighbors = density[neighbors]
            density_diff = density_i - density_neighbors
            
            # åŠ æƒå¹³æ–¹å·®
            weighted_loss = weights * (density_diff ** 2)
            total_loss += weighted_loss.sum()
            count += len(neighbors)
        
        if count == 0:
            return torch.tensor(0.0, device=device, requires_grad=True)
        
        # å¹³å‡æŸå¤±å¹¶ä¹˜ä»¥æƒé‡
        loss = (total_loss / count) * Lambda_lap
        return loss
