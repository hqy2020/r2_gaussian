"""
å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å— - FSGSé£æ ¼æ·±åº¦ç›‘ç£
æ”¯æŒDPTå’ŒMiDaSæ·±åº¦ä¼°è®¡å™¨ï¼Œå®ç°Pearsonç›¸å…³æ€§æ·±åº¦æŸå¤±

å‚è€ƒè®ºæ–‡: FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting
æ ¸å¿ƒåŠŸèƒ½: å•ç›®æ·±åº¦ä¼°è®¡ + å°ºåº¦ä¸å˜æ·±åº¦æŸå¤±
"""

import torch
import torch.nn.functional as F
import numpy as np
from typing import Optional, Tuple, Union
import warnings

# å°è¯•å¯¼å…¥æ·±åº¦ä¼°è®¡åº“
try:
    import transformers
    from transformers import DPTImageProcessor, DPTForDepthEstimation
    HAS_DPT = True
except ImportError:
    HAS_DPT = False
    print("Warning: transformers not available, DPT depth estimation disabled")

try:
    import midas
    from midas.model_loader import default_models, load_model
    HAS_MIDAS = True
except ImportError:
    HAS_MIDAS = False
    print("Warning: MiDaS not available, using fallback depth estimation")


class MonocularDepthEstimator:
    """
    å•ç›®æ·±åº¦ä¼°è®¡å™¨ - æ”¯æŒå¤šç§é¢„è®­ç»ƒæ¨¡å‹
    å®ç°FSGSè®ºæ–‡ä¸­çš„æ·±åº¦ç›‘ç£æœºåˆ¶
    """
    
    def __init__(self, 
                 model_type: str = "dpt_large",
                 device: str = "cuda",
                 enable_depth_estimation: bool = True):
        """
        åˆå§‹åŒ–æ·±åº¦ä¼°è®¡å™¨
        
        Args:
            model_type: æ·±åº¦ä¼°è®¡æ¨¡å‹ç±»å‹
                       - "dpt_large": Dense Prediction Transformer Large (æ¨è)
                       - "dpt_hybrid": DPT Hybrid
                       - "midas_small": MiDaS Small
                       - "midas_large": MiDaS Large
                       - "disabled": ç¦ç”¨æ·±åº¦ä¼°è®¡ (å‘ä¸‹å…¼å®¹)
            device: è®¡ç®—è®¾å¤‡
            enable_depth_estimation: æ˜¯å¦å¯ç”¨æ·±åº¦ä¼°è®¡ (å‘ä¸‹å…¼å®¹å¼€å…³)
        """
        self.model_type = model_type
        self.device = device
        self.enabled = enable_depth_estimation and model_type != "disabled"
        
        # å‘ä¸‹å…¼å®¹æ£€æŸ¥
        if not self.enabled:
            print("ğŸ“¦ [Compatibility] Depth estimation disabled - using legacy pseudo-label mode")
            self.model = None
            self.processor = None
            return
            
        self.model = None
        self.processor = None
        
        # åˆå§‹åŒ–é€‰å®šçš„æ¨¡å‹
        if self.enabled:
            self._initialize_model()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ·±åº¦ä¼°è®¡æ¨¡å‹"""
        try:
            if self.model_type.startswith("dpt") and HAS_DPT:
                self._initialize_dpt_model()
            elif self.model_type.startswith("midas") and HAS_MIDAS:
                self._initialize_midas_model()
            else:
                # å¤‡ç”¨æ–¹æ¡ˆ: ç¦ç”¨æ·±åº¦ä¼°è®¡
                print(f"âš ï¸  Model type {self.model_type} not available, disabling depth estimation")
                self.enabled = False
                
        except Exception as e:
            print(f"âš ï¸  Failed to initialize {self.model_type}: {e}")
            print("ğŸ“¦ Falling back to legacy mode without depth estimation")
            self.enabled = False
    
    def _initialize_dpt_model(self):
        """åˆå§‹åŒ–DPTæ¨¡å‹"""
        if self.model_type == "dpt_large":
            model_name = "Intel/dpt-large"
        elif self.model_type == "dpt_hybrid":
            model_name = "Intel/dpt-hybrid-midas"
        else:
            raise ValueError(f"Unknown DPT model: {self.model_type}")
        
        print(f"ğŸ¯ Loading DPT model: {model_name}")
        self.processor = DPTImageProcessor.from_pretrained(model_name)
        self.model = DPTForDepthEstimation.from_pretrained(model_name)
        self.model.to(self.device)
        self.model.eval()
        print("âœ… DPT model loaded successfully")
    
    def _initialize_midas_model(self):
        """åˆå§‹åŒ–MiDaSæ¨¡å‹"""
        if self.model_type == "midas_small":
            model_type = "MiDaS_small"
        elif self.model_type == "midas_large":
            model_type = "MiDaS"
        else:
            raise ValueError(f"Unknown MiDaS model: {self.model_type}")
        
        print(f"ğŸ¯ Loading MiDaS model: {model_type}")
        self.model, self.processor, _ = load_model(
            device=torch.device(self.device),
            model_type=model_type,
            optimize=True
        )
        print("âœ… MiDaS model loaded successfully")
    
    def estimate_depth(self, image: torch.Tensor, 
                      normalize: bool = True) -> Optional[torch.Tensor]:
        """
        ä¼°è®¡å›¾åƒæ·±åº¦
        
        Args:
            image: è¾“å…¥å›¾åƒ [C, H, W] æˆ– [B, C, H, W]ï¼Œå€¼åŸŸ[0,1]
            normalize: æ˜¯å¦å½’ä¸€åŒ–æ·±åº¦å€¼
        
        Returns:
            æ·±åº¦å›¾ [H, W] æˆ– [B, H, W]ï¼Œå¦‚æœç¦ç”¨åˆ™è¿”å›None
        """
        if not self.enabled or self.model is None:
            return None
        
        try:
            with torch.no_grad():
                if self.model_type.startswith("dpt"):
                    return self._estimate_depth_dpt(image, normalize)
                elif self.model_type.startswith("midas"):
                    return self._estimate_depth_midas(image, normalize)
                else:
                    return None
                    
        except Exception as e:
            warnings.warn(f"Depth estimation failed: {e}", RuntimeWarning)
            return None
    
    def _estimate_depth_dpt(self, image: torch.Tensor, normalize: bool) -> torch.Tensor:
        """DPTæ·±åº¦ä¼°è®¡"""
        # è½¬æ¢ä¸ºPILæ ¼å¼è¿›è¡Œé¢„å¤„ç†
        if image.dim() == 3:
            image = image.unsqueeze(0)  # [1, C, H, W]
        
        # è½¬æ¢åˆ°CPUè¿›è¡Œé¢„å¤„ç†
        image_np = image.cpu().numpy().transpose(0, 2, 3, 1)  # [B, H, W, C]
        image_np = (image_np * 255).astype(np.uint8)
        
        batch_size = image_np.shape[0]
        depth_maps = []
        
        for i in range(batch_size):
            # é¢„å¤„ç†å•å¼ å›¾åƒ
            inputs = self.processor(images=image_np[i], return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # æ·±åº¦é¢„æµ‹
            outputs = self.model(**inputs)
            predicted_depth = outputs.predicted_depth
            
            # æ’å€¼åˆ°åŸå§‹åˆ†è¾¨ç‡
            original_shape = image.shape[-2:]  # [H, W]
            depth = F.interpolate(
                predicted_depth.unsqueeze(1),
                size=original_shape,
                mode='bilinear',
                align_corners=False
            ).squeeze(1)  # [1, H, W] -> [H, W]
            
            depth_maps.append(depth.squeeze(0))  # [H, W]
        
        depth_tensor = torch.stack(depth_maps, dim=0)  # [B, H, W]
        
        # å½’ä¸€åŒ–
        if normalize:
            depth_tensor = self._normalize_depth(depth_tensor)
        
        return depth_tensor.squeeze(0) if batch_size == 1 else depth_tensor
    
    def _estimate_depth_midas(self, image: torch.Tensor, normalize: bool) -> torch.Tensor:
        """MiDaSæ·±åº¦ä¼°è®¡"""
        if image.dim() == 3:
            image = image.unsqueeze(0)
        
        # MiDaSé¢„å¤„ç†
        input_batch = []
        for i in range(image.shape[0]):
            img = image[i].permute(1, 2, 0).cpu().numpy()  # [H, W, C]
            img = (img * 255).astype(np.uint8)
            processed = self.processor(img)
            input_batch.append(torch.from_numpy(processed).to(self.device))
        
        input_tensor = torch.stack(input_batch, dim=0)
        
        # æ·±åº¦é¢„æµ‹
        depth_batch = self.model(input_tensor)
        
        # å½’ä¸€åŒ–
        if normalize:
            depth_batch = self._normalize_depth(depth_batch)
        
        return depth_batch.squeeze(0) if depth_batch.shape[0] == 1 else depth_batch
    
    def _normalize_depth(self, depth: torch.Tensor) -> torch.Tensor:
        """å½’ä¸€åŒ–æ·±åº¦å€¼"""
        # å¯¹æ¯ä¸ªæ ·æœ¬åˆ†åˆ«å½’ä¸€åŒ–
        if depth.dim() == 3:  # [B, H, W]
            normalized_depth = []
            for i in range(depth.shape[0]):
                d = depth[i]
                d_min, d_max = d.min(), d.max()
                if d_max > d_min:
                    d_norm = (d - d_min) / (d_max - d_min)
                else:
                    d_norm = torch.zeros_like(d)
                normalized_depth.append(d_norm)
            return torch.stack(normalized_depth, dim=0)
        else:  # [H, W]
            d_min, d_max = depth.min(), depth.max()
            if d_max > d_min:
                return (depth - d_min) / (d_max - d_min)
            else:
                return torch.zeros_like(depth)
    
    def compute_pearson_loss(self, 
                           rendered_depth: torch.Tensor,
                           estimated_depth: torch.Tensor,
                           mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®—Pearsonç›¸å…³æ€§æ·±åº¦æŸå¤± (FSGSæ ¸å¿ƒæŸå¤±å‡½æ•°)
        
        Args:
            rendered_depth: æ¸²æŸ“æ·±åº¦å›¾ [H, W]
            estimated_depth: ä¼°è®¡æ·±åº¦å›¾ [H, W] 
            mask: å¯é€‰é®ç½© [H, W]
        
        Returns:
            Pearsonç›¸å…³æ€§æŸå¤± (è¶Šå°è¶Šå¥½)
        """
        if not self.enabled or estimated_depth is None:
            return torch.tensor(0.0, device=rendered_depth.device)
        
        # ç¡®ä¿æ·±åº¦å›¾åœ¨åŒä¸€è®¾å¤‡ä¸Š
        if estimated_depth.device != rendered_depth.device:
            estimated_depth = estimated_depth.to(rendered_depth.device)
        
        # åº”ç”¨é®ç½©
        if mask is not None:
            rendered_flat = rendered_depth[mask > 0.5]
            estimated_flat = estimated_depth[mask > 0.5]
        else:
            rendered_flat = rendered_depth.reshape(-1)
            estimated_flat = estimated_depth.reshape(-1)
        
        # è¿‡æ»¤æ— æ•ˆå€¼
        valid_mask = torch.isfinite(rendered_flat) & torch.isfinite(estimated_flat)
        rendered_flat = rendered_flat[valid_mask]
        estimated_flat = estimated_flat[valid_mask]
        
        if len(rendered_flat) < 10:  # éœ€è¦è¶³å¤Ÿçš„æœ‰æ•ˆåƒç´ 
            return torch.tensor(0.0, device=rendered_depth.device)
        
        # è®¡ç®—Pearsonç›¸å…³ç³»æ•°
        corr = self._pearson_correlation(rendered_flat, estimated_flat)
        
        # FSGSæŸå¤±: 1 - |correlation|ï¼Œä½¿æŸå¤±æœ€å°åŒ–å¯¹åº”ç›¸å…³æ€§æœ€å¤§åŒ–
        loss = 1.0 - torch.abs(corr)
        
        return loss
    
    def _pearson_correlation(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—Pearsonç›¸å…³ç³»æ•°"""
        # è®¡ç®—å‡å€¼
        mean_x = torch.mean(x)
        mean_y = torch.mean(y)
        
        # è®¡ç®—åæ–¹å·®å’Œæ–¹å·®
        cov_xy = torch.mean((x - mean_x) * (y - mean_y))
        var_x = torch.mean((x - mean_x) ** 2)
        var_y = torch.mean((y - mean_y) ** 2)
        
        # è®¡ç®—ç›¸å…³ç³»æ•°
        correlation = cov_xy / (torch.sqrt(var_x * var_y) + 1e-8)
        
        return correlation
    
    def compute_scale_invariant_loss(self,
                                   rendered_depth: torch.Tensor,
                                   estimated_depth: torch.Tensor,
                                   mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è®¡ç®—å°ºåº¦ä¸å˜æ·±åº¦æŸå¤± (å¤‡é€‰æŸå¤±å‡½æ•°)
        
        Args:
            rendered_depth: æ¸²æŸ“æ·±åº¦å›¾
            estimated_depth: ä¼°è®¡æ·±åº¦å›¾
            mask: å¯é€‰é®ç½©
        
        Returns:
            å°ºåº¦ä¸å˜æ·±åº¦æŸå¤±
        """
        if not self.enabled or estimated_depth is None:
            return torch.tensor(0.0, device=rendered_depth.device)
        
        # å¯¹æ•°æ·±åº¦å·®å¼‚
        log_rendered = torch.log(rendered_depth + 1e-6)
        log_estimated = torch.log(estimated_depth + 1e-6)
        
        if mask is not None:
            log_diff = (log_rendered - log_estimated)[mask > 0.5]
        else:
            log_diff = log_rendered - log_estimated
        
        # å°ºåº¦ä¸å˜æŸå¤±
        loss = torch.mean(log_diff ** 2) - 0.5 * (torch.mean(log_diff) ** 2)
        
        return loss


def create_depth_estimator(model_type: str = "dpt_large", 
                         device: str = "cuda",
                         enable_fsgs_depth: bool = True) -> MonocularDepthEstimator:
    """
    åˆ›å»ºæ·±åº¦ä¼°è®¡å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        model_type: æ¨¡å‹ç±»å‹
        device: è®¾å¤‡
        enable_fsgs_depth: æ˜¯å¦å¯ç”¨FSGSæ·±åº¦åŠŸèƒ½ (å‘ä¸‹å…¼å®¹å¼€å…³)
    
    Returns:
        æ·±åº¦ä¼°è®¡å™¨å®ä¾‹
    """
    if not enable_fsgs_depth:
        model_type = "disabled"
        print("ğŸ“¦ [Compatibility] FSGS depth estimation disabled by user setting")
    
    return MonocularDepthEstimator(
        model_type=model_type,
        device=device,
        enable_depth_estimation=enable_fsgs_depth
    )