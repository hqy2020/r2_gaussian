# 3D Student Splatting and Scooping 

Jialin Zhu ${ }^{1}$, Jiangbei Yue ${ }^{2}$, Feixiang $\mathrm{He}^{1}$, He Wang ${ }^{1,3 *}$<br>${ }^{1}$ University College London, UK ${ }^{2}$ University of Leeds, UK<br>${ }^{3}$ AI Centre, University College London, UK


#### Abstract

Recently, 3D Gaussian Splatting (3DGS) provides a new framework for novel view synthesis, and has spiked a new wave of research in neural rendering and related applications. As 3DGS is becoming a foundational component of many models, any improvement on 3DGS itself can bring huge benefits. To this end, we aim to improve the fundamental paradigm and formulation of 3DGS. We argue that as an unnormalized mixture model, it needs to be neither Gaussians nor splatting. We subsequently propose a new mixture model consisting of flexible Student's $t$ distributions, with both positive (splatting) and negative (scooping) densities. We name our model Student Splatting and Scooping, or SSS. When providing better expressivity, SSS also poses new challenges in learning. Therefore, we also propose a new principled sampling approach for optimization. Through exhaustive evaluation and comparison, across multiple datasets, settings, and metrics, we demonstrate that SSS outperforms existing methods in terms of quality and parameter efficiency, e.g. achieving matching or better quality with similar numbers of components, and obtaining comparable results while reducing the component number by as much as $82 \%$.


## 1. Introduction

Presented initially as a neural rendering technique, 3D Gaussian Splatting (3DGS) [14] has quickly become a versatile component in various systems, e.g. geometry reconstruction, autonomous driving [3, 6]. Given its importance as a foundational component, very recently researchers start to investigate possible alternatives to the basic framework of 3DGS, e.g. more expressive distributions instead of Gaussians [8, 19], more principled optimization [15], which all focus on improving the model expressivity. Our research is among these attempts.

The key to 3DGS' success lies in its name: Gaussian and splatting. 3DGS can be seen as a (unnormalized) Gaus-

[^0]sian mixture, which provides two advantages. As a generalpurpose distribution, Gaussians can approximate an arbitrary density function hence good expressivity. Also, Gaussians have analytical forms under e.g. affine transformation, enabling quick evaluation in 3D-2D projection, thus quick learning from images. Meanwhile, splatting provides a flexible way of identifying only the relevant Gaussians to an image for learning. Despite the success, the framework can still suffer from insufficient expressivity [17, 35], and low parameter efficiency, i.e. needing a large number of components [18, 29]. Therefore, we re-examine the three key components in 3DGS: Gaussian, splatting, and the optimization. Since its underlying principle is essentially to fit a 3D mixture model to a radiance field, we argue it needs not to be restricted to Gaussians or splatting.

To this end, we propose a simple yet effective generalization of 3DGS. We first replace Gaussians with Student's t distribution with one degree of freedom, referred to simply as t-distribution. Similar to Gaussians, t-distribution also enjoys good properties such as analytical forms under affine transformation. More importantly, t-distribution can be seen as a generalization of Gaussians and therefore is more expressive. t-distribution has a control parameter for the tail fatness, representing distributions ranging from Cauchy distribution to Gaussian distribution, and any distribution inbetween. Compared with Gaussians, Cauchy is fat-tailed, i.e. a single Cauchy can cover a larger area with comparatively higher densities than a Gaussian. Furthermore, by making this control parameter learnable, we learn components with a wide range of varying tail thicknesses.

Next, we extend the splatting scheme which only operates in the positive density space. Inspired by mixture models with negative components [21], we propose to employ both positive and negative components to splat (adding) and scoop (subtracting) densities from the model. This leads to more complex mathematical forms than 3DGS but we derive their close-form gradients for learning.

Finally, as the increased model complexity, optimization methods based on naive stochastic gradient descent become insufficient, due to parameter coupling. Therefore, we propose a principled sampling scheme based on Stochastic


[^0]:    *Corresponding author, he_wang@ucl.ac.uk

---

Gradient Hamiltonian Monte Carlo (SGHMC).

We refer to our model as Student Splatting and Scooping (SSS). SSS is evaluated on multiple datasets and compared with existing methods. Experiments show that SSS achieves higher quality often with fewer number of components, demonstrating more expressivity and higher parameter efficiency. Formally, our contributions include:

- A new model named Student Splatting and Scooping (SSS), which is highly expressive and parameter efficient.
- A new mixture model with flexible components learned from a set of distribution families for neural rendering.
- A mixture model with negative components, which extends the learning into the negative density space.
- A principled sampling approach to tackle parameter coupling during learning.


## 2. Related Work

3D Reconstruction and Novel View Synthesis 3D reconstruction and Novel View Synthesis have been longstanding research topics in computer vision. Traditional methods mainly include Multi-View Stereo (MVS) [28] and Structure from Motion (SFM) [31]. Recently, the advent of Deep Learning has brought important changes to the field. In particular, the techniques based on Neural Radiance Field (NeRF) [23] and 3D Gaussian Splatting (3DGS) [14] have set new state-of-the-art (SOTA) benchmarks.

NeRF methods NeRF [23] proposes to implicitly encode the radiance field of a 3D object/scene into a neural network and renders the 3D geometry and textures through a continuous volume rendering function. Since then, a large number of methods based on NeRF have been proposed, namely NeRF++ [40], Mip-NeRF [2] and Mip-NeRF360 [2] to improve rendering quality, Plenoxels [7] and Instant-NGP [24] to accelerate NeRF training, D-NeRF [26] to extend NeRFs to dynamic scenes, DreamFusion [25] and Zero-1-to-3 [20] to employ it for text-to-3d generation models, etc. However, the biggest drawback of NeRF is that the ray casting process for rendering is time consuming. Despite the effort in improving its rendering efficiency e.g. SNeRG [11] and mobileNeRF [5], it still cannot be used for real-time rendering in most cases.

Splatting methods 3DGS [14] solves the above problem for real-time rendering, by replacing the volume rendering with a differentiable rasterization method, which achieves the SOTA render quality. 3DGS uses 3D Gaussian as the primitive for the splatting method [42, 43]. It directly projects 3D Gaussians onto the 2D image plane through view/projective transformation for rasterization. Similar to NeRFs, prolific follow up research has been conducted based on 3DGS. GS++ [13] and Mip-Splatting [38] aim
to improve rendering quality, 4D Gaussian Splatting [34] and Deformable 3D Gaussians [36] extend 3DGS to dynamic scenes, Dreamgaussian [30] employs 3DGS for text-to-3D tasks. One particular line of research is to improve the fundamental paradigm of 3DGS. This includes FreGS [39], 3DGS-MCMC [15] and Bulò et al. [27] which optimize the training process and adaptive density control in 3DGS, Scaffold-GS [22] and Implicit Gaussian Splatting [35] which combine grid representation with 3DGS for better rendering quality. More recently, there is also research exploring different primitives other than 3D Gaussians. 2DGS [12] obtains better surface reconstruction by changing the primitives from 3D Gaussian to 2D Gaussian for aligning the 3D scene. GES [8] uses a generalized exponential kernel to increase the expression ability of primitives and reduce memory cost. 3DHGS [19] decomposes one Gaussian into two half-Gaussians to obtain asymmetry and better expressivity.

Our research is among the few recent efforts in improving the fundamental formulation of 3DGS. Different from them, we propose to use more expressive and flexible distributions, 3D Student's t distribution, as the basic primitive. In addition, we also use both positive and negative densities to extend the optimization into the negative density space for better representation. Finally, we propose a principled sampling approach for learning, deviating from most of the above research.

## 3. Methodology

### 3.1. Preliminaries: 3DGS as a mixture model

3DGS essentially fits a (unnormalized) 3D Gaussian mixture model to a radiance field [14]:

$$
P(x)=\sum w_{i} G_{i}(x), G(x)=e^{-\frac{1}{2}(x-\mu)^{T} \Sigma^{-1}(x-\mu)}
$$

where $w_{i}>0 . \mu$ is the center position of Gaussian. $\Sigma \in$ $\mathbb{R}^{3 \times 3}$ is the covariance matrix, parameterized by a scaling matrix $S$ and a rotation matrix $R$ to maintain its positive semi-definiteness: $\Sigma=R S S^{T} R^{T}$. Additionally, every 3D Gaussian is associated with opacity $o \in[0,1]$ and color $c \in$ $\mathbb{R}^{27}$ which is represented by spherical harmonics and viewdependent [14]. $w_{i}$ is determined by $o, c$ and compositing values after projection.

Since the mixture can only be evaluated in 2D, when rendering an image, a 3D Gaussian is projected onto the 2D image plane, $G^{2 D}$, via integrating it in the camera view space along a ray, for computing a pixel color:

$$
C(u)=\sum_{i=1}^{N} c_{i} o_{i} G_{i}^{2 D}(u) \prod_{j=1}^{i-1}\left(1-o_{j} G_{j}^{2 D}(u)\right)
$$

where $N$ is the number of the Gaussians that intersect with the ray cast from the pixel $u$. Finally, the Gaussian parameters, opacity, and colors are learned based on the observed

---

![img-0.jpeg](2503.10148v4-2_img-0.jpeg)

Figure 1. Student's t with varying degrees of freedom ν. (standard deviation is 5).

2D images. Eq. (2) reveals that *C(u)* can be seen as a 2D Gaussian mixture, except that now a component weight is also a function of other components, introducing additional cross-component interactions.

Numerically, Gaussians, as the mixture component, are closed under affine transformation and marginalization of variables, so that the forward/backward pass can be quickly computed. 3DGS is a *monotonic* mixture as it is additive, *i.e*. *w<sub>i</sub>* > 0. Due to the success of 3DGS, existing works have since followed this paradigm [13, 22, 38, 39].

## 3.2. Student's t as a basic component

We propose an unnormalized t-distribution mixture model, where a t-distribution is defined by a mean (location) μ ∈ ℝ<sup>3</sup>, a covariance matrix (shape) Σ = *RSS<sup>T</sup>* *R<sup>T</sup>* ∈ ℝ<sup>3×3</sup>, a degree of freedom (tail-fatness) ν ∈ [1, +∞), associated with opacity o, and color c:

$$P(x) = \sum w_i T_i(x), \ w_i > 0$$

$$T(x|\nu) = [1 + \frac{1}{\nu}(x - \mu)^T \Sigma^{-1}(x - \mu)]^{-\frac{\nu+3}{2}}, \qquad (3)$$

where we can drop the scalar $$\frac{\Gamma(\nu+3)/2}{\Gamma(\nu/2)\nu^{\frac{3}{2}} \pi^{\frac{3}{2}} | \Sigma |^{\frac{1}{2}}}$$ in the original t-distribution safely to facilitate learning. The choice is driven by two main factors. First, t-distribution is a strong generalization of Gaussians. As shown in Fig. 1, when ν → 1, T → *Cauchy*; when ν → ∞, T → *Gaussian*. So t-distribution can capture what Gaussians capture and beyond. Furthermore, since Cauchy is fat-tailed, it can cover larger areas with higher densities than Gaussians therefore potentially reducing the number of components. As ν, μ and Σ are learnable, SSS becomes a mixture of components learned from an infinite number of distribution families, instead of one family [14], providing further flexibility.

The second reason for t-distribution is it also provides good properties similar to Gaussians, *e.g*. close under affine transformation and marginalization of variables. Rendering a pixel requires an affine transformation, then a projective transformation, followed by an integration along a ray, to be applied to a component, which has a simple form in 3DGS. In SSS, t-distribution also has a close form:

$$T^{2D}(u) = [1 + \frac{1}{\nu}(u - \mu^{2D})^T(\Sigma^{2D})^{-1}(u - \mu^{2D})]^{-\frac{\nu+2}{2}}$$

$$\mu^{2D} = (W\mu + t)_{1:2}/((W\mu + t)_{3})$$

$$\Sigma^{2D} = (JW\Sigma W^T J^T)_{1:2,1:2},\tag{4}$$

where the subscripts select the corresponding rows and columns. *W*, *t* and *J* are the affine transformation (*i.e*. scale, translation) and (approximated) projective transformation [42, 43]. This enables us to easily derive the key gradients for learning shown in the supplementary material (SM), unlike existing research also using alternative mixture components but requiring approximation [8].

In summary, the mixture of learnable t-distributions enhances the representational power and provides good mathematical properties for learning.

## 3.3. Splatting and Scooping

While monotonic mixture models are powerful, a *non-monotonic* mixture model recently has been proposed by introducing negative components [21], arguing that it is suboptimal to only operate in the positive density space:

$$P(x) = (\sum w_i T_i(x))^2 = \sum_{i=1}^{K} \sum_{j=1}^{K} w_i w_j T_i(x) T_j(x) \qquad (5)$$

where w ∈ ℝ. In our problem, a negative density makes good sense as it can be seen as subtracting a color. However, our experiments using Eq. (5) show that it is not ideal as it introduces interactions between every pair of components, increasing the model evaluation complexity to *O(n<sup>2</sup>)* where *n* is the number of components in the model, making it significantly slower than before. Therefore, we still use Eq. (3) but with w ∈ ℝ instead of *w* > 0 where *w<sub>i</sub>* = *c<sub>i</sub>o<sub>i</sub>* ∏_{k=1}^{i-1}(1 − *o<sub>k</sub>T<sub>k</sub><sup>2D</sup>(u)) and o ∈ [−1, 1]. Normally this might cause issues as Eq. (3) is then not well defined with negative components. However, we can parameterize the density in an energy-based form explained later which is well defined. In learning, we constrain the opacity by a *tanh* function so that positive and negative components can dynamically change signs while being bounded. Introducing negative t-distribution can enhance the representation power and the parameter efficiency. We show a simple experiment in Fig. 2, where fewer components are needed to fit the shape topology of a torus. In SSS, a component with negative densities is equivalent to removing its color from the mixture. Negative components are particularly useful in subtracting colors.

## 3.4. Learning via sampling

Recently, it is argued that principled sampling is better in 3DGS, *e.g*. Markov Chain Monte Carlo [15], instead of

---

![img-1.jpeg](2503.10148v4-2_img-1.jpeg)

Figure 2. High parameter efficiency by negative components.
We use a torus with only ambient lighting and frontal views (a), where the challenge is to capture the shape topology with as few components as possible. We initialize the component means near the center. Only using positive densities either underfits if two components are used (b), or requires at least 5 components to capture the topology correctly (c). In contrast, in (d), we only need two components (one positive and one negative), to capture the topology of the shape. Both components are co-located at the center of the torus. The positive component covers the torus but also the hole, while the negative component subtracts densities in the middle to make a hole.

naive stochastic gradient descent (SGD). Empirically, we found training SSS involves learning more tightly coupled parameters compared with 3DGS, namely among $\nu$, $\mu$, and $\Sigma$. We speculate that this is because changing $\nu$ in learning is changing the family of distributions within which we optimize $\mu$ and $\Sigma$. Therefore, we propose a sampling scheme that mitigates such coupling, based on Stochastic Gradient Hamiltonian Monte Carlo (SGHMC).

Starting from the Hamiltonian Monte Carlo, we first parameterize the posterior distribution as:

$$P(\theta, r) \propto \exp(-L_{\theta}(x) - \frac{1}{2}r^T Ir) \tag{6}$$

where $L_{\theta}(x)$ is our loss function, $I$ is an identity matrix, $r$ is a momentum auxiliary variable, and $\theta$ is the learnable parameters. This is because Eq. (3) with $w \in \mathbb{R}$ is not a well-defined distribution, which makes direct sampling difficult. Using an energy function circumvents this issue and prescribes the high density regions of good $\theta$. Intuitively, we would like to sample $\theta$ to minimize $L_{\theta}(x)$. In addition, to decouple parameters during learning, the momentum term $\frac{1}{2}r^T Ir$ creates frictions for each dimension of the parameter space, enabling adaptive learning for each parameter.

For $L_{\theta}(x)$, our rendering function computes the pixel value based on the $N$ components associated with a ray:

$$C(u) = \sum_{i=1}^{N} c_i o_i T_i^2D(u) \prod_{j=1}^{i-1} (1 - o_j T_j^2D(u)) . \tag{7}$$

where $u$ is the pixel. $c$ and $o$ are the color and opacity associated with a component $T$. We then employ the following loss function [15]:

$$\begin{split}
L &= (1 - \epsilon_{D-SSIM})L_1 + \epsilon_{D-SSIM}L_{D-SSIM} \\
&+ \epsilon_{o} \sum_{i} |o_i|_{1} + \epsilon_{\Sigma} \sum_{i} \sum_{j} |\sqrt{\lambda_{i,j}}|_{1}
\end{split} \tag{8}$$

where the $L_1$ norm, and the structural similarity $L_{D-SSIM}$ loss aim to reconstruct images, while the last two terms act as regularization, with $\lambda$ being the eigenvalues of $\Sigma$. The regularization applied to the opacity ensures that the opacity is big only when a component is absolutely needed. The regularization on $\lambda$ ensures the model uses components as spiky as possible (i.e. small variances). Together, they minimize the needed number of components [15].

Furthermore, directly sampling from Eq. (6) requires the full gradient of $U = L_{\theta}(x) - \frac{1}{2}r^T Ir$ which is not possible given the large number of training samples. Therefore, replacing the full gradient with stochastic gradient will introduce a noise term: $\nabla \dot{U} = \nabla U + \mathcal{N}(0, V)$, where $\mathcal{N}$ is Normal and $V$ is the covariance of the stochastic gradient noise. Under mild assumptions [4], sampling Eq. (6) using stochastic gradients becomes (with detailed derivation in the SM):

$$\begin{split}
d\theta &= M^{-1}rdt \\
dr &= -\nabla U(\theta)dt - CM^{-1}rdt + \mathcal{N}(0, 2Cdt)
\end{split} \tag{9}$$

where $\mathcal{N}$ is Gaussian noise, $M$ is a mass matrix, and $C$ is a control parameter dictating the friction term $CM^{-1}rdt$ and noise $\mathcal{N}(0, 2Cdt)$. In our problem, it is crucial to design good friction and noise scheduling. The effect of this principled sampling method is further discussed in SM.

### 3.4.1. Friction and Noise Scheduling

We first use SGHMC on $\mu$ and Adam on the other parameters. To learn $\mu$, we modify Eq. (9) to:

$$\begin{split}
\mu_{t+1} &= \mu_t - \varepsilon^2 \left[\frac{\partial L}{\partial \mu}\right]_t + F + N \\
F &= \sigma(o)\varepsilon(1 - \varepsilon C)r_{t-1} \\
N &= \sigma(o)\mathcal{N}(0, 2\varepsilon^{\frac{3}{2}}C) \\
r_{t+1} &= r_t - \varepsilon \left[\frac{\partial L}{\partial \mu}\right]_{t+1} - \varepsilon Cr_{t-1} + \mathcal{N}(0, 2\varepsilon C) \\
&\text{where } \sigma(o) = \sigma(-k(o - t))
\end{split} \tag{10}$$

where $\varepsilon$ is the learning rate and decays during learning. $\mathcal{N}$ is Gaussian noise. $o$ is the opacity. The main difference between Eq. (10) and Eq. (9) is we now have adaptive friction $F$ and noise $N$ for $\mu$. $\sigma$ (sigmoid function) switches on/off the friction and noise. We use $k = 100$ and $t = 0.995$, so that it only activates for components with opacity lower than 0.005. When it is activated, friction and noise are added to these components. Note that if $F$ is disabled, Eq. (10)

---

is simplified to a Stochastic Gradient Langevin Dynamics scheme [33].

When learning, we initialize with a sparse set of (SfM) points without normals [14], run Eq. (10) without $F$ for burn-in for exploration, then run the full sampling for exploitation until convergence. During burn-in stage, we multiply $N$ by the covariance $\Sigma$ of the component to maintain the anisotropy profile of the t-distribution. After the burnin, $\Sigma$ is removed, and the anisotropy is then maintained by $F$ due to the momentum $r$.

Key gradients Overall, the key learnable parameters for each component in SSS include the mean $\mu$, covariance $\Sigma$ (i.e. $S$ and $R$ ), color $c$, opacity $o$, and degree parameter $\nu$. To compute Eq. (9), the key gradients are $\frac{\partial L}{\partial \mu}, \frac{\partial L}{\partial S}, \frac{\partial L}{\partial R}, \frac{\partial L}{\partial c}$, $\frac{\partial L}{\partial o}$, and $\frac{\partial L}{\partial \nu}$. For simplicity, we give them in the SM.

### 3.4.2. Adding and Recycling Components

Components can become nearly transparent during sampling, i.e. near zero opacity. In 3DGS, they are removed. Recently, it is argued that they should be recycled [15], by relocating them to a high opacity component. However, careful consideration needs to be taken as the overall distribution before and after relocation should be the same [27]. When moving some components to the location of another component, this is ensured by:

$$
\min \int_{-\infty}^{\infty}\left\|C_{n e w}(\mu)-C_{o l d}(\mu)\right\|_{2}^{2} d \mu
$$

where $C_{\text {new }}$ and $C_{\text {old }}$ are the color after and before relocation respectively. Minimizing this integral ensures the smallest possible pixel-wise color changes over the whole domain. Minimizing Eq. (11) in SSS leads to:

$$
\begin{aligned}
& \mu_{\text {new }}=\mu_{\text {old }},\left(1-o_{\text {new }}\right)^{N}=\left(1-o_{\text {old }}\right) \\
& \Sigma_{\text {new }}=\left(o_{\text {old }}\right)^{2} \frac{\nu_{\text {old }}}{\nu_{\text {new }}}\left(\frac{\beta\left(\frac{1}{2}, \frac{\nu_{\text {old }}+2}{2}\right)}{K}\right)^{2} \Sigma_{\text {old }} \\
& K=\sum_{i=1}^{N} \sum_{k=0}^{i-1}\binom{i-1}{k}(-1)^{k}\left(o_{\text {new }}\right)^{k+1} Z) \\
& Z=\beta\left(\frac{1}{2}, \frac{(k+1)\left(\nu_{\text {new }}+3\right)-1}{2}\right)
\end{aligned}
$$

$\mu_{\text {new }}$ and $\mu_{\text {old }}, o_{\text {new }}$ and $o_{\text {old }}, \Sigma_{\text {new }}$ and $\Sigma_{\text {old }}$ are the mean, opacity, and covariance after and before relocation respectively. $N$ is the total number of components after relocation, i.e. moving $N-1$ low opacity components to the location of 1 high opacity component. $\beta$ is the beta function. We leave the detailed derivation in the SM. Note we do not distinguish between positive and negative components during relocation. This introduces a perturbation on the sign of the opacity. In Eq. (12), if $o_{\text {old }}>0$ then all $o_{\text {new }}>0$, or otherwise $o_{\text {new }}<0$ if $o_{\text {old }}<0$, regardless their original opacity
signs. This sign perturbation in practice helps the mixing of the sampling. Furthermore, to ensure the sampling stability, we limit the relocation to a maximum of $5 \%$ of the total components at a time. Finally, we also add new components when needed, but do not use the adaptive density control (clone and split) in 3DGS [14]. Instead, we add 5\% new components with zero opacity and then recycle them.

## 4. Experiments

### 4.1. Experimental setting

Datasets and Metrics Following existing research, we employ 11 scenes from 3 datasets, including 7 public scenes from Mip-NeRF 360 [2], 2 outdoor scenes from Tanks \& Temples [16], and 2 indoor scenes from Deep Blending [10]. Also, following the previous evaluation metrics, we use Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Metric (SSIM) [32], and Learned Perceptual Image Patch Similarity (LPIPS) [41]. We provide average scores of each dataset, and detailed scores are in the SM.

Baselines Due to there being many publications based on 3DGS, we only choose the original 3DGS [14] and the most recent work that focuses on improving the fundamental paradigm of 3DGS and has achieved the best performances. The methods include Generalized Exponential Splatting (GES) [8] and 3D Half-Gaussian Splatting (3DHGS) [19] which also use different (positive only) mixture components, Scaffold-GS [22] and Fre-GS [39] which optimize the training procedure to achieve faster convergence and better results, 3DGS-MCMC [15] which proposes a principled MCMC sampling process, and MipNeRF [1] which is a state-of-the-art method with Neural Radiance Field (NeRF) [23]. Overall, our baselines comprehensively include methods with new mixture components, new optimization approaches, and the SOTA quality.

The results of all baselines in general benchmarking are from their papers. In addition, we run their codes with other settings for more comparison. Since not all baseline methods are implemented in exactly the same setting, we need to adapt them for comparison. These details are in the SM.

### 4.2. General Benchmarks

We first compare SSS with the baselines on all scenes in their default settings, shown in Tab. 1. SSS achieves overall the best results on 6 of the 9 metrics, and the second best on 2 metrics. The only exception is the LPIPS in Deep Blending, where the difference between SSS and the best is $7 \times 10^{-3}$. Furthermore, when investigating individual scenes, SSS achieves the largest leading margin on Train. It achieves $23.23 / 0.844 / 0.170$, where the second best method 3DHGS achieves 22.95/0.827/0.197, in PSNR, SSIM, and LPIPS, which is a $1.22 \% / 2.05 \% / 13.7 \%$ improvement. De-

---

| Dataset | Mip-NeRF360 Dataset | | | Tanks\&Temples | | | Deep Blending | | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Method-Metric | PSNR $\uparrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$ | PSNR $\uparrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$ | PSNR $\uparrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$ |
| Mip-NeRF | 29.23 | 0.844 | 0.207 | 22.22 | 0.759 | 0.257 | 29.40 | 0.901 | 0.245 |
| 3DGS | 28.69 | 0.870 | 0.182 | 23.14 | 0.841 | 0.183 | 29.41 | 0.903 | 0.243 |
| GES | 26.91 | 0.794 | 0.250 | 23.35 | 0.836 | 0.198 | 29.68 | 0.901 | 0.252 |
| 3DHGS | 29.56 | 0.873 | 0.178 | 24.49 | 0.857 | 0.169 | 29.76 | 0.905 | 0.242 |
| Fre-GS | 27.85 | 0.826 | 0.209 | 23.96 | 0.841 | 0.183 | 29.93 | 0.904 | 0.240 |
| Scaffold-GS | 28.84 | 0.848 | 0.220 | 23.96 | 0.853 | 0.177 | 30.21 | 0.906 | 0.254 |
| 3DGS-MCMC | 29.89 | 0.900 | 0.190 | 24.29 | 0.860 | 0.190 | 29.67 | 0.890 | 0.320 |
| Ours | 29.90 | 0.893 | 0.145 | 24.87 | 0.873 | 0.138 | 30.07 | 0.907 | 0.247 |

Table 1. Comparison. The red, orange and yellow colors represent the top three results. Competing metrics are extracted from respective papers, and ours are reported as the average of three runs.
tailed scores for each scene are in the SM. We show qualitative comparison in Figure 3.

### 4.3. Parameter Efficiency

SSS has stronger representation power than 3DGS and its variants. The varying tail-thickness of its components enables SSS to fit the data with fewer components, i.e. higher parameter efficiency. We show this via experiments under different component numbers.

Since the SfM initialization gives different components in different scenes and a method normally increases the component number during learning, we introduce a coefficient to describe the latter as a multiplicity of the former. Denoting the initial component number as $\delta$, we test $\delta, 1.4 \delta$, $1.8 \delta, 2.2 \delta$, and $2.6 \delta$ as the maximum component number for comparison. Note even with $2.6 \delta$, the component number is still much smaller than the experiments in Tab. 1. Specifically, the $2.6 \delta$ vs the original 3DGS component number are $468 \mathrm{k} / 1.1 \mathrm{~m}, 364 \mathrm{k} / 2.6 \mathrm{~m}, 208 \mathrm{k} / 3.4 \mathrm{~m}, 96 \mathrm{k} / 2.5 \mathrm{~m}, 140 \mathrm{k} / 5.9 \mathrm{~m}$, $520 \mathrm{k} / 1.3 \mathrm{~m}, 416 \mathrm{k} / 1.2 \mathrm{~m}, 364 \mathrm{k} / 5.2 \mathrm{~m}, 624 \mathrm{k} / 1.8 \mathrm{~m}, 286 \mathrm{k} / 1.5 \mathrm{~m}$, $83 \mathrm{k} / 4.75 \mathrm{~m}$, in Train, Truck, DrJohnson, Playroom, Bicycle, Bonsai, Counter, Garden, Kitchen, Room, Stump, corresponding to merely $42.5 \%, 14 \%, 6.1 \%, 3.8 \%, 2.4 \%, 40 \%$, $34.7 \%, 7 \%, 34.7 \%, 19.1 \%, 1.7 \%$ of the original components, a maximum of $98.3 \%$ reduction.

PSNR is averaged over the scenes in each dataset and shown in Figure 4. First, SSS outperforms all other methods in 15 out of 15 settings, demonstrating strong expressivity across all scenarios. Furthermore, when the component number decreases, all methods deteriorate, but SSS deteriorates slowly comparatively, demonstrating that SSS can fit the data much more efficiently than the rest. One specific example is the Tanks \& Temples in Tab. 1. SSS achieves 23.6 PSNR with merely 180k components, already surpassing Mip-NeFR, 3DGS and GES. With around 300k components, SSS achieves 24.4 PSNR, which is only slightly worse than 3DHGS and 3DGS-MCMC, by a margin at the scale of $10^{-2}$. Note this is a comparison with the methods
in Tab. 1 where they use at least 1 m components, e.g. 3DGS employs around 1.1 m and 2.6 m in Train and Truck, while SSS employs only around 364 k and 468 k , a maximum reduction of $82 \%$ of the components.

### 4.4. Qualitative Comparison

We further show a qualitative comparison in one scene across the different component numbers in Fig. 5. The ground-truth is one view from the Train. When restricting the component number to around 180k, the original 3DGS and one of the state-of-the-art methods 3DHGS show significant blur. This is likely to be caused by the struggle between stretching Gaussians to cover large areas and narrowing them to reconstruct details, given a limited number of components at proposal. Due to the optimization relying on stochastic gradient descent, a local minimum is sought where the distribution of the Gaussians is sub-optimal. Intuitively, the issue can be mitigated by more flexible components and/or better optimization. As expected, this is shown in GES and 3DGS-MCMC, where the former employs a more flexible component (generalized exponential function) while the latter improves the optimization itself by MCMC. The improvements by both methods suggest that these are the correct directions to improve the paradigm of 3DGS.

Next, SSS outperforms GES and 3DGS-MCMC visually when using 180k components. One example is the sky and the hill in the background in the left half of the image. GES creates a blurry background mixing the sky and the hill, with no discernible details, suggesting it uses a few components that are stretched to cover large areas. In contrast, 3DGS-MCMC can separate the sky from the hill. But it creates random white patches in the sky, which do not exist in the ground truth. This suggests that 3DGS-MCMC employs a relatively larger number of slim Gaussians to fit the details but meanwhile introduces additional noises. SSS not only successfully separates the sky and the hill, but simultaneously retains the homogeneous color in the sky and reconstructs the details on the hill, e.g. the trees and lawns.

---

![img-2.jpeg](2503.10148v4-2_img-2.jpeg)

Figure 3. Visual comparison Zoom-in for better visualization. (a) SSS restores better the indentations of the box lid; (b) SSS is the best at detailing windows in the upper center; (c) Only the image rendered by SSS contains the green track detail in the upper right corner; (d) SSS is the best at restoring the reflection in the front window of the truck; (e) SSS perfectly restores the light switch next to the stairs.

![img-3.jpeg](2503.10148v4-2_img-3.jpeg)

Figure 4. All methods with reduced component numbers.

This is attributed to SSS' capability of learning components with varying tail-fatness to adaptively capture large homogeneous areas and small heterogeneous regions.

Furthermore, when increasing the component number to 468k, all results are improved, as expected. 3DGS and 3DHGS still cannot work as well as other methods, suggesting they need more components. In addition, the difference between GES, 3DGS-MCMC, and SSS starts to nar-

---

![img-4.jpeg](2503.10148v4-2_img-4.jpeg)

Figure 5. Visual results of all methods with varying component numbers. In addition to the well-reconstructed main body of the train compared to other baselines, our method can use a small number of components to restore more details, such as plants on distant mountains, rocks on the ground nearby, etc. Besides, our sky has fewer noises and appears more similarly to the ground truth. Zoom in for details. Note that ours with 252k components has already achieved SOTA quality and beats most baselines.

row. GES can separate the sky and the hill. Both GES and 3DGS-MCMC have fewer artifacts. However, as the component number increases, there are still noticeable noises introduced to the sky, which suggests that it is a common issue for both methods when using more components to cover an area with mixed homogeneous and heterogeneous regions. Comparatively, SSS gives consistent performance across all component numbers, *i.e*. clearly separating and reconstructing the homogeneous sky and the heterogeneous hill. Also, the visual quality from 180k to 468k does not change significantly for SSS, but is noticeably improved for GES and 3DGS-MCMC, suggesting a higher parameter efficiency of SSS in perception.

### Ablation Study
We conduct an ablation study to show the effectiveness of various components in SSS. These results prove how each component contributes to the final performance improvement. We give details in the SM.

## 5. Conclusion, Discussion, and Future Work

We proposed Student Splatting and Scooping (SSS), a new non-monotonic mixture model, consisting of positive and negative Student's t distributions, learned by a principled SGHMC sampling. SSS contains a simple yet strong and non-trivial generalization of 3DGS and its variants. SSS outperforms existing methods in rendering quality, and shows high parameter efficiency, *e.g*. achieving comparable quality with less than ½ of components.

SSS has limitations. Its primitives are restricted to symmetric and smooth t-distributions, limiting its representation. The sampling also needs hyperparameter tuning such as the percentage of negative components. In the future, we will combine other distribution families (*e.g*. Laplace) with t-distribution to further enhance the expressivity, and make the SGHMC self-adaptive to achieve better balances between the positive and negative components.

---

## Acknowledgement

This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 899739 CrowdDNA.

## References

[1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5855-5864, 2021. 5, 1
[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5470-5479, 2022. 2, 5, 7, 1
[3] Guikun Chen and Wenguan Wang. A survey on 3d gaussian splatting. arXiv preprint arXiv:2401.03890, 2024. 1
[4] Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In International conference on machine learning, pages 1683-1691. PMLR, 2014. 4, 13, 14
[5] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1656916578, 2023. 2
[6] Ben Fei, Jingyi Xu, Rui Zhang, Qingyuan Zhou, Weidong Yang, and Ying He. 3d gaussian as a new vision era: A survey. arXiv preprint arXiv:2402.07181, 2024. 1
[7] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5501-5510, 2022. 2
[8] Abdullah Hamdi, Luke Melas-Kyriazi, Jinjie Mai, Guocheng Qian, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, and Andrea Vedaldi. Ges: Generalized exponential splatting for efficient radiance field rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19812-19822, 2024. 1, 2, 3, 5
[9] N Alan Heckert, James J Filliben, C M Croarkin, B Hembree, William F Guthrie, P Tobias, and J Prinz. Handbook 151: Nist/sematech e-handbook of statistical methods. 2002. 7
[10] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. ACM Transactions on Graphics (ToG), 37(6):1-15, 2018. 5, 7, 1, 2
[11] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. In Proceedings of the IEEE/CVF international conference on computer vision, pages 5875-5884, 2021. 2
[12] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically ac-
curate radiance fields. In ACM SIGGRAPH 2024 Conference Papers, pages 1-11, 2024. 2
[13] Letian Huang, Jiayang Bai, Jie Guo, and Yanwen Guo. Gs++: Error analyzing and optimal gaussian splatting. arXiv preprint arXiv:2402.00752, 2024. 2, 3
[14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):139-1, 2023. $1,2,3,5$
[15] Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Yang-Che Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. 3d gaussian splatting as markov chain monte carlo. Advances in Neural Information Processing Systems, 37:80965-80986, 2024. 1, 2, 3, 4, 5, 8, 11,13
[16] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (ToG), 36(4): $1-13,2017.5,7,1,2$
[17] Jonas Kulhanek, Songyou Peng, Zuzana Kukelova, Marc Pollefeys, and Torsten Sattler. WildGaussians: 3D gaussian splatting in the wild. NeurIPS, 2024. 1
[18] Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. Compact 3d gaussian representation for radiance field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2171921728, 2024. 1
[19] Haolin Li, Jinyang Liu, Mario Sznaier, and Octavia Camps. 3d-hgs: 3d half-gaussian splatting. arXiv preprint arXiv:2406.02720, 2024. 1, 2, 5, 3
[20] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9298-9309, 2023. 2
[21] Lorenzo Loconte, Aleksanteri Sladek, Stefan Mengel, Martin Trapp, Arno Solin, Nicolas Gillis, and Antonio Vergari. Subtractive mixture models via squaring: Representation and learning. In International Conference on Learning Representations (ICLR), 2024. 1, 3
[22] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20654-20664, 2024. 2, 3, 5, 1
[23] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99-106, 2021. 2, 5
[24] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):1-15, 2022. 2
[25] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations. 2

---

[26] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1031810327, 2021. 2
[27] Samuel Rota Bulò, Lorenzo Porzi, and Peter Kontschieder. Revising densification in gaussian splatting. In European Conference on Computer Vision, pages 347-362. Springer, 2024. 2, 5, 8, 11
[28] Steven M Seitz, Brian Curless, James Diebel, Daniel Scharstein, and Richard Szeliski. A comparison and evaluation of multi-view stereo reconstruction algorithms. In 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06), pages 519-528. IEEE, 2006. 2
[29] Xiangyu Sun, Joo Chan Lee, Daniel Rho, Jong Hwan Ko, Usman Ali, and Eunbyung Park. F-3dgs: Factorized coordinates and representations for 3d gaussian splatting. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 7957-7965, 2024. 1
[30] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In The Twelfth International Conference on Learning Representations. 2
[31] Shimon Ullman. The interpretation of structure from motion. Proceedings of the Royal Society of London. Series B. Biological Sciences, 203(1153):405-426, 1979. 2, 3
[32] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600-612, 2004. 5, 1
[33] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 681-688. Citeseer, 2011. 5
[34] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20310-20320, 2024. 2
[35] Minye Wu and Tinne Tuytelaars. Implicit gaussian splatting with efficient multi-level tri-plane representation. arXiv preprint arXiv:2408.10041, 2024. 1, 2
[36] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for highfidelity monocular dynamic scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20331-20341, 2024. 2
[37] Vickie Ye and Angjoo Kanazawa. Mathematical supplement for the gsplat library. arXiv preprint arXiv:2312.02121, 2023. 11
[38] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-splatting: Alias-free 3d gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19447-19456, 2024. 2, 3
[39] Jiahui Zhang, Fangneng Zhan, Muyu Xu, Shijian Lu, and Eric Xing. Fregs: 3d gaussian splatting with progressive frequency regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21424-21433, 2024. 2, 3, 5
[40] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492, 2020. 2
[41] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586-595, 2018. 5, 1
[42] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In Proceedings Visualization, 2001. VIS'01., pages 29-538. IEEE, 2001. 2, 3, 5, 9, 10
[43] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa splatting. IEEE Transactions on Visualization and Computer Graphics, 8(3):223-238, 2002. 2, 3, 5, 9,10

---

# 3D Student Splatting and Scooping Supplementary Material 

Jialin Zhu ${ }^{1}$, Jiangbei Yue ${ }^{2}$, Feixiang $\mathrm{He}^{1}$, He Wang ${ }^{1,3}$<br>${ }^{1}$ University College London, UK ${ }^{2}$ University of Leeds, UK<br>${ }^{3}$ AI Centre, University College London, UK

## 1. Additional Experimental Results

We show more visual results of general benchmarks for Student Splatting and Scooping (SSS) against selected baselines: 3D Gaussian Splatting (3DGS) [14], 3D HalfGaussian Splatting (3DHGS) [19], Generalized Exponential Splattin (GES) [8] and 3D Gaussian Splatting as Markov Chain Monte Carlo (3DGS-MCMC) [15] in Fig. 1. For (a) in Fig. 1, there are a few food residues in the metal bowl. Only 3DGS-MCMC and SSS can successfully restore these details. Furthermore, SSS also achieves a better reconstruction of colors than 3DGS-MCMC with these residues. SSS is the only method that can reproduce the sharp details of the chair refracted in the transparent water glass in Fig. 1 (b). For (c) in Fig. 1, the difficulty lies in reconstructing the details of the upper wall edge and the items on the cabinet (texts, etc.). Considering these two difficulties, SSS performs best in this scenario. SSS can also ensure that the details at the edge of the image are restored to the greatest extent, which is reflected in (d) and (e) of Fig. 1. In addition, SSS is also the best for reconstructing pure color areas (sky, wall, etc.).

We have illustrated the results of the Train scene in the Tanks \& Temples dataset [16] in the main context for the varying component numbers experiment. Here we further show two more scenes (room from Mip-MeRF 360 [2] and drjohnson from Deep Blending [10]) in Figs. 2 and 3. In Fig. 2, the difficulty of 3D reconstruction lies in the texture of the carpet. Both 3DGS, 3DHGS, and GES can only restore the approximate shape but cannot take into account the details when the number of components is small. 3DGSMCMC and SSS can restore the details of the carpet. However, because SSS has a scooping operation, the carpet it reconstructs has a more realistic texture. 3DGS, 3DHGS, and GES are completely unable to reconstruct quality results in the setting of 80 k to 208 k components in Fig. 3. The reconstruction of 3DGS-MCMC is good enough but loses some details (window gaps inside the basket marked area), while SSS is still the best method for capturing details.

### 1.1. Detailed Results on Each Scene

We show detailed comparisons between our method and the baselines on every scene on Peak Signal-toNoise Ratio (PSNR), Structural Similarity Index Metric (SSIM) [32], and Learned Perceptual Image Patch Similarity (LPIPS) [41] metrics among three datasets (MipNeRF 360 [2], Tanks \& Temples [16], Deep Blending [10])

| Method $\backslash$ Scene | bicycle | bonsai | counter | garden | kitchen | room | stump | average |
| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| 3DGS | 25.25 | 31.98 | 28.70 | 27.41 | 30.32 | 30.63 | 26.55 | 28.69 |
| Mip-NeRF | 24.37 | 33.46 | 29.55 | 26.98 | 32.23 | 31.64 | 26.40 | 29.23 |
| Scaffold-GS | 24.50 | 32.70 | 29.34 | 27.17 | 31.30 | 31.93 | 26.27 | 28.84 |
| 3DHGS | 25.39 | 33.30 | 29.62 | 27.68 | 32.17 | 32.12 | 26.64 | 29.56 |
| 3DGS-MCMC | 26.15 | 32.88 | 29.51 | 28.16 | 32.27 | 32.48 | 27.80 | 29.89 |
| SSS | 25.68 | 33.50 | 29.87 | 28.09 | 32.43 | 32.57 | 27.17 | 29.90 |

Table 1. PSNR results for every scene in Mip-NeRF 360 dataset. The red, orange, and yellow colors represent the top three results. Competing metrics are extracted from respective papers, and ours are reported as the average of three runs.

| Method $\backslash$ Scene | bicycle | bonsai | counter | garden | kitchen | room | stump | average |
| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| 3DGS | 0.771 | 0.938 | 0.905 | 0.868 | 0.922 | 0.914 | 0.775 | 0.870 |
| Mip-NeRF | 0.685 | 0.941 | 0.894 | 0.813 | 0.920 | 0.913 | 0.744 | 0.844 |
| Scaffold-GS | 0.705 | 0.946 | 0.914 | 0.842 | 0.928 | 0.925 | 0.784 | 0.848 |
| 3DHGS | 0.768 | 0.950 | 0.909 | 0.868 | 0.930 | 0.921 | 0.770 | 0.873 |
| 3DGS-MCMC | 0.810 | 0.950 | 0.920 | 0.890 | 0.940 | 0.940 | 0.820 | 0.900 |
| SSS | 0.798 | 0.956 | 0.926 | 0.882 | 0.939 | 0.938 | 0.813 | 0.893 |

Table 2. SSIM results for every scene in Mip-NeRF 360 dataset. The red, orange, and yellow colors represent the top three results. Competing metrics are extracted from respective papers, and ours are reported as the average of three runs.
in Tabs. 1 to 6 .
Note we only include baselines that provide detailed evaluation scores on each scene. These baselines are MipNeRF [1], 3DGS [14], Scaffold-GS [22], 3DHGS [19] and 3DGS-MCMC [15]. This is due to the intrinsic randomness in the training of these methods. When we re-train the models ourselves and often obtain slightly different results from their papers. Therefore, we use the results reported in their original papers.

Overall, our method outperforms all the baselines. A close second is 3DGS-MCMC, which is a state-of-the-art model recently. There are some baselines that outperform both our method and 3DGS-MCMC on individual scenes under some metrics, but overall, our method and 3DGSMCMC are the best and second best methods.

Beyond the selected methods for baseline, we do realize that there might be other methods that are not included here but achieve higher scores on certain scenes and metrics, especially when it comes to specific application settings, e.g. 3D reconstruction. However, our goal here is to restrict our comparison to the methods that aim to improve 3DGS on its fundamental formulation and can be potentially used as a generic-purposed component.

---

![img-5.jpeg](2503.10148v4-2_img-5.jpeg)

Figure 1. Visual comparison. (a) SSS restores the best details inside the metal bowl. (b) SSS is the only one that can reconstruct the details of the chair refracted in the transparent cup. (c) The reconstruction of the wall edge (bright blue box) and the font were both done best by SSS. (d) SSS's details on the distant woods and the reconstruction of the sky are the best. (e) The reconstruction of the pattern on the wall is SSS at its best.

|  Method \ Scene | bicycle | bonsai | counter | garden | kitchen | room | stump | average  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  3DGS | 0.205 | 0.205 | 0.204 | 0.103 | 0.129 | 0.220 | 0.210 | 0.182  |
|  Mip-NeRF | 0.301 | 0.176 | 0.204 | 0.170 | 0.127 | 0.211 | 0.261 | 0.207  |
|  Scaffold-GS | 0.306 | 0.185 | 0.191 | 0.146 | 0.126 | 0.202 | 0.284 | 0.220  |
|  3DHGS | 0.202 | 0.180 | 0.201 | 0.104 | 0.125 | 0.220 | 0.215 | 0.178  |
|  3DGS-MCMC | 0.180 | 0.220 | 0.220 | 0.100 | 0.140 | 0.250 | 0.190 | 0.190  |
|  SSS | 0.173 | 0.151 | 0.156 | 0.009 | 0.104 | 0.167 | 0.174 | 0.145  |

Table 3. LPIPS results for every scene in Mip-NeRF 360 dataset. The red, orange, and yellow colors represent the top three results. Competing metrics are extracted from respective papers, and ours are reported as the average of three runs.

# 1.2. Detailed Results on Varying Component Numbers

We show detailed results of varying component numbers, on every scene with PSNR, SSIM, and LPIPS in Mip-NeRF

|  Dataset - Scene | Tanks\&Temples |  |  | Deep Blending |  |   |
| --- | --- | --- | --- | --- | --- | --- |
|  Method | train | truck | average | drjohnson | playroom | average  |
|  3DGS | 21.09 | 25.18 | 23.14 | 28.77 | 30.04 | 29.41  |
|  Mip-NeRF | 19.52 | 24.91 | 22.22 | 29.14 | 29.66 | 29.40  |
|  Scaffold-GS | 22.15 | 25.77 | 23.96 | 29.80 | 30.62 | 30.21  |
|  3DHGS | 22.95 | 26.04 | 24.49 | 29.32 | 30.20 | 29.76  |
|  3DGS-MCMC | 22.47 | 26.11 | 24.29 | 29.00 | 30.33 | 29.67  |
|  SSS | 23.32 | 26.41 | 24.87 | 29.66 | 30.47 | 30.07  |

Table 4. PSNR results for every scene in Tanks \& Temples and Deep Blending dataset. The red, orange, and yellow colors represent the top three results. Competing metrics are extracted from respective papers, and ours are reported as the average of three runs.

360, Tanks \& Temples, and Deep Blending datasets. Since we need to re-train all methods for comparison, we ensure

---

| Dataset - Scene | Tanks\&Temples | | | Deep Blending | | |
| --- | --- | --- | --- | --- | --- | --- |
| Method | train | truck | average | drjohnson | playroom | average |
| 3DGS | 0.802 | 0.879 | 0.841 | 0.899 | 0.906 | 0.903 |
| Mip-NeRF | 0.660 | 0.857 | 0.759 | 0.901 | 0.900 | 0.901 |
| Scaffold-GS | 0.822 | 0.883 | 0.853 | 0.907 | 0.904 | 0.906 |
| 3DHGS | 0.827 | 0.887 | 0.857 | 0.904 | 0.907 | 0.905 |
| 3DGS-MCMC | 0.830 | 0.890 | 0.860 | 0.890 | 0.900 | 0.890 |
| SSS | 0.850 | 0.897 | 0.873 | 0.905 | 0.909 | 0.907 |

Table 5. SSIM results for every scene in Tanks \& Temples and Deep Blending dataset. The red, orange, and yellow colors represent the top three results. Competing metrics are extracted from respective papers, and ours are reported as the average of three runs.

| Dataset - Scene | Tanks\&Temples | | | Deep Blending | | |
| --- | --- | --- | --- | --- | --- | --- |
| Method | train | truck | drjohnson | playroom |  |  |
| 3DGS | 0.218 | 0.148 | 0.183 | 0.244 | 0.241 | 0.243 |
| Mip-NeRF | 0.354 | 0.159 | 0.257 | 0.237 | 0.252 | 0.245 |
| Scaffold-GS | 0.206 | 0.147 | 0.177 | 0.250 | 0.258 | 0.254 |
| 3DHGS | 0.197 | 0.141 | 0.169 | 0.240 | 0.243 | 0.242 |
| 3DGS-MCMC | 0.240 | 0.140 | 0.860 | 0.330 | 0.310 | 0.320 |
| SSS | 0.166 | 0.109 | 0.138 | 0.249 | 0.245 | 0.247 |

Table 6. LPIPS results for every scene in Tanks \& Temples and Deep Blending dataset. The red, orange, and yellow colors represent the top three results. Competing metrics are extracted from respective papers, and ours are reported as the average of three runs. the implementation is as fair as possible. Based on whether codes are open sourced and furthermore how easily they can be adapted for comparison (explained later), we selected 3DGS [14], GES [8], 3DHGS [19] and 3DGS-MCMC [15] as the baselines. We use $\delta$ to represent the number of the initial components from Structure from Motion (SFM) reconstruction [31] for different scenes.

The results are shown in Tabs. 7 to 12. In total, there are 11 (scenes) $\times 5$ (component numbers) $\times 3$ (metrics) $=$ 165 comparisons. For the absolute majority, SSS achieves the best. Furthermore, when it is not the best method, it is the close second to 3DGS-MCMC. This is an exhaustive comparison of many datasets, metrics, and more importantly different numbers of components.

### 1.3. Ablation Study

We conduct an ablation study to show the effectiveness of various components in SSS. We report the results on one dataset in Tab. 13. Universally, by only replacing Gaussians with Student's t distributions (SGD+t-distribution), it already outperforms Mip-NeRF, 3DGS, and GES, demonstrating improved expressivity. Further with SGHMC, it is already the best method, showing the advantage of the proposed sampling and the importance of a good sampler in the optimization process. Finally, adding negative components further improves the results.

The detailed results of Ablation Study (effect of each contribution in SSS) on every scene with PSNR, SSIM, and

| Scene - Components | Mip-NeRF 360 - average | | | | | | Method | $\delta$ | $1.4 \delta$ | $1.8 \delta$ | $2.2 \delta$ | $2.6 \delta$ | | 3DGS | 19.16 | 20.32 | 20.50 | 21.22 | 21.75 | | GES | 20.59 | 21.60 | 22.78 | 23.52 | 23.86 | | 3DHGS | 19.63 | 20.33 | 20.39 | 20.99 | 22.12 | | 3DGS-MCMC | 27.47 | 27.89 | 28.18 | 28.39 | 28.56 | | SSS | 27.72 | 28.14 | 28.43 | 28.66 | 28.84 | | Scene - Components | Mip-NeRF 360 - bicycle | | | | | | | Method | 54 k | 75 k | 97 k | 118 k | 140 k | | 3DGS | 18.32 | 18.53 | 18.62 | 18.75 | 18.52 | | GES | 18.31 | 18.29 | 18.20 | 18.13 | 18.19 | | 3DHGS | 18.45 | 18.72 | 18.62 | 18.61 | 18.50 | | 3DGS-MCMC | 23.04 | 23.42 | 23.74 | 23.97 | 24.17 | | SSS | 22.97 | 23.29 | 23.53 | 23.71 | 23.95 | | Scene - Components | Mip-NeRF 360 - bonsai | | | | | | | Method | 206 k | 280 k | 360 k | 440 k | 520 k | | 3DGS | 20.50 | 22.93 | 23.39 | 23.97 | 24.54 | | GES | 22.47 | 24.12 | 25.51 | 27.02 | 29.27 | | 3DHGS | 21.37 | 22.90 | 23.45 | 24.62 | 24.82 | | 3DGS-MCMC | 31.14 | 31.56 | 31.85 | 32.03 | 32.17 | | SSS | 31.67 | 32.21 | 32.52 | 32.75 | 32.94 | | Scene - Components | Mip-NeRF 360 - counter | | | | | | | Method | 155 k | 224 k | 288 k | 352 k | 416 k | | 3DGS | 18.26 | 19.07 | 18.84 | 20.78 | 23.73 | | GES | 19.68 | 23.64 | 26.35 | 26.74 | 27.45 | | 3DHGS | 17.77 | 18.36 | 18.89 | 20.48 | 23.59 | | 3DGS-MCMC | 28.38 | 28.68 | 28.82 | 28.93 | 29.03 | | SSS | 28.71 | 29.06 | 29.21 | 29.39 | 29.48 | | Scene - Components | Mip-NeRF 360 - garden | | | | | | | Method | 138 k | 196 k | 252 k | 308 k | 364 k | | 3DGS | 16.73 | 17.34 | 17.49 | 17.81 | 18.05 | | GES | 16.62 | 16.94 | 17.24 | 17.58 | 17.92 | | 3DHGS | 17.36 | 17.44 | 17.70 | 18.25 | 18.49 | | 3DGS-MCMC | 25.23 | 25.70 | 26.02 | 26.26 | 26.47 | | SSS | 25.32 | 25.80 | 26.17 | 26.41 | 26.64 | | Scene - Components | Mip-NeRF 360 - kitchen | | | | | | | Method | 241 k | 336 k | 432 k | 528 k | 624 k | | 3DGS | 21.19 | 22.70 | 22.10 | 22.63 | 22.70 | | GES | 24.78 | 25.74 | 27.65 | 29.34 | 29.54 | | 3DHGS | 23.19 | 24.20 | 21.40 | 21.61 | 22.60 | | 3DGS-MCMC | 30.11 | 30.78 | 31.05 | 31.26 | 31.45 | | SSS | 30.84 | 31.33 | 31.64 | 31.77 | 31.97 | | Scene - Components | Mip-NeRF 360 - room | | | | | | | Method | 112 k | 154 k | 198 k | 242 k | 286 k | | 3DGS | 18.89 | 21.41 | 22.42 | 23.78 | 23.76 | | GES | 22.60 | 22.93 | 24.98 | 26.60 | 25.51 | | 3DHGS | 19.20 | 20.37 | 22.27 | 22.83 | 26.31 | | 3DGS-MCMC | 30.72 | 30.98 | 31.26 | 31.45 | 31.54 | | SSS | 30.98 | 31.34 | 31.65 | 31.92 | 31.96 | | Scene - Components | Mip-NeRF 360 - stump | | | | | | | Method | 32 k | 44.8 k | 57.6 k | 70.4 k | 83.2 k | | 3DGS | 20.21 | 20.26 | 20.61 | 20.82 | 20.92 | | GES | 19.67 | 19.55 | 19.51 | 19.25 | 19.13 | | 3DHGS | 20.08 | 20.31 | 20.42 | 20.52 | 20.53 | | 3DGS-MCMC | 23.64 | 24.08 | 24.51 | 24.84 | 25.09 | | SSS | 23.57 | 23.95 | 24.31 | 24.66 | 24.91 |

Table 7. PSNR results of varying component numbers experiments for every scene in the Mip-NeRF 360 dataset.

---

![img-6.jpeg](2503.10148v4-2_img-6.jpeg)

Figure 2. Visual results of all methods with varying component numbers of room scene from Mip-NeRF 360. Only 3DGS-MCMC and SSS can restore the details of the carpet, but the result of SSS obviously has a more realistic carpet texture.

LPIPS metrics among Mip-NeRF 360, Tanks & Temples and Deep Blending datasets are in Tabs. 14 to 19.

More ablation We also show comparison of applying SGHMC with vanilla 3DGS and positive t-distributions only with Tanks & Temples and Deep Blending datasets to the ablations (Tab. 20). Replacing SGD with SGHMC already improves the results. Replacing Gaussians with positive t-distributions further improves the PSNR but slightly reduces SSIM and LPIPS. Nonetheless, our Full model is obviously the best. While individual techniques alone might provide merely small improvements, SSS as a whole is the SOTA.

## 2. Sampling Effects on Learning

When trying the original SGD with only positive Student’s t components, the learned ν values were not ideal. Considering Gaussian is simply a Student’s t distribution with fixed ν = ∞, it shows that ν introduces undesirable local minima. This was mitigated by SGHMC as the friction term decouples parameters, but the sampling became slow compared to vanilla 3DGS.

We compare the learned ν distributions between SGHMC (decoupling) and the 3DGS optimization (no decoupling) in Fig. 4. SGHMC learned a distribution across a wide range, with no mode collapse and fully utilizing the representation power of t-distribution. 3DGS optimization learns a distribution heavily concentrated in some areas (near 1, likely mode collapse), unable to explore the full space of t-distribution.

---

![img-7.jpeg](2503.10148v4-2_img-7.jpeg)

Figure 3. Visual results of all methods with varying component numbers of drjohnson scene from Deep Blending. 3DGS, 3DHSGS, and GES are completely unable to reconstruct quality results. The results of 3DGS-MCMC and SSS are relatively better. SSS can restore more details (such as the window gaps in the blue box) than 3DGS-MCMC.
![img-8.jpeg](2503.10148v4-2_img-8.jpeg)

Figure 4. Sampling effects on learning .

## 3. Implementation Details

The implementation of SSS is based on open-source codes of the vanilla 3D Gaussian Splatting (3DGS) [14] and 3DGS-MCMC [15]. We modified various parts to replace Gaussians with Student's t distributions for splatting algorithm [42, 43]. These modifications are adapted to both forward and backward procedures. We show the formulae of the forward process (transformation and marginalization) in Sec. 4.1. For backward propagation with training, we derive the relevant partial derivatives with respect to Student's t distribution, which is given in Sec. 4.2. Theoretically, the value of $\nu$ of Student's t distribution can be infinite, but in practice, to avoid numerical issues, we limit the range of $\nu$ from 1 to 10000 .

There are two major differences between using Student's

---

| Scene - Components | Mip-NeRF 360 - average | | | | | | Method | $\delta$ | $1.4 \delta$ | $1.8 \delta$ | $2.2 \delta$ | $2.6 \delta$ | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | 3DGS | 0.584 | 0.609 | 0.610 | 0.626 | 0.640 | | | | | | | GES | 0.612 | 0.630 | 0.652 | 0.667 | 0.674 | | | | | | | 3DHGS | 0.591 | 0.603 | 0.599 | 0.611 | 0.634 | | | | | | | 3DGS-MCMC | 0.803 | 0.818 | 0.829 | 0.837 | 0.844 | | | | | | | SSS | 0.802 | 0.816 | 0.827 | 0.835 | 0.842 | | | | | | | Scene - Components | Mip-NeRF 360 - bicycle | | | | | | | | Method | 54 k | 75 k | 97 k | 118 k | 140 k | | | | | | | 3DGS | 0.367 | 0.375 | 0.377 | 0.382 | 0.377 | | | | | | | GES | 0.372 | 0.369 | 0.368 | 0.368 | 0.370 | | | | | | | 3DHGS | 0.369 | 0.374 | 0.372 | 0.376 | 0.373 | | | | | | | 3DGS-MCMC | 0.603 | 0.630 | 0.650 | 0.666 | 0.679 | | | | | | | SSS | 0.597 | 0.620 | 0.639 | 0.652 | 0.666 | | | | | | | Scene - Components | Mip-NeRF 360 - bonsai | | | | | | | | Method | 206k | 280k | 360k | 440k | 520k | | | | | | | 3DGS | 0.790 | 0.822 | 0.820 | 0.828 | 0.838 | | | | | | | GES | 0.811 | 0.836 | 0.861 | 0.882 | 0.911 | | | | | | | 3DHGS | 0.802 | 0.821 | 0.827 | 0.837 | 0.839 | | | | | | | 3DGS-MCMC | 0.938 | 0.942 | 0.945 | 0.947 | 0.948 | | | | | | | SSS | 0.938 | 0.944 | 0.948 | 0.950 | 0.951 | | | | | | | Scene - Components | Mip-NeRF 360 - counter | | | | | | | | Method | 155k | 224k | 288k | 352k | 416k | | | | | | | 3DGS | 0.663 | 0.686 | 0.682 | 0.731 | 0.799 | | | | | | | GES | 0.696 | 0.785 | 0.854 | 0.866 | 0.882 | | | | | | | 3DHGS | 0.650 | 0.667 | 0.680 | 0.719 | 0.788 | | | | | | | 3DGS-MCMC | 0.901 | 0.907 | 0.911 | 0.914 | 0.916 | | | | | | | SSS | 0.898 | 0.906 | 0.911 | 0.915 | 0.917 | | | | | | | Scene - Components | Mip-NeRF 360 - garden | | | | | | | | Method | 138k | 196k0 | 252k | 308k | 364k | | | | | | | 3DGS | 0.366 | 0.386 | 0.399 | 0.412 | 0.424 | | | | | | | GES | 0.373 | 0.390 | 0.402 | 0.416 | 0.430 | | | | | | | 3DHGS | 0.383 | 0.394 | 0.410 | 0.423 | 0.433 | | | | | | | 3DGS-MCMC | 0.757 | 0.784 | 0.802 | 0.814 | 0.823 | | | | | | | SSS | 0.758 | 0.785 | 0.803 | 0.815 | 0.825 | | | | | | | Scene - Components | Mip-NeRF 360 - kitchen | | | | | | | | Method | 241k | 336k | 432k | 528k | 624k | | | | | | | 3DGS | 0.762 | 0.796 | 0.760 | 0.769 | 0.776 | | | | | | | GES | 0.856 | 0.838 | 0.875 | 0.908 | 0.912 | | | | | | | 3DHGS | 0.811 | 0.801 | 0.698 | 0.706 | 0.734 | | | | | | | 3DGS-MCMC | 0.919 | 0.925 | 0.928 | 0.930 | 0.932 | | | | | | | SSS | 0.921 | 0.926 | 0.929 | 0.932 | 0.934 | | | | | | | Scene - Components | Mip-NeRF 360 - room | | | | | | | | Method | 112k | 154k | 198k | 242k | 286k | | | | | | | 3DGS | 0.720 | 0.769 | 0.791 | 0.813 | 0.815 | | | | | | | GES | 0.786 | 0.797 | 0.824 | 0.851 | 0.839 | | | | | | | 3DHGS | 0.721 | 0.746 | 0.783 | 0.793 | 0.844 | | | | | | | 3DGS-MCMC | 0.911 | 0.916 | 0.920 | 0.923 | 0.925 | | | | | | | SSS | 0.912 | 0.918 | 0.923 | 0.926 | 0.928 | | | | | | | Scene - Components | Mip-NeRF 360 - stump | | | | | | | | Method | 32k | 44.8k | 57.6k | 70.4k | 83.2k | | | | | | | 3DGS | 0.421 | 0.428 | 0.439 | 0.447 | 0.450 | | | | | | | GES | 0.391 | 0.391 | 0.384 | 0.376 | 0.371 | | | | | | | 3DHGS | 0.404 | 0.414 | 0.424 | 0.423 | 0.424 | | | | | | | 3DGS-MCMC | 0.594 | 0.624 | 0.650 | 0.668 | 0.683 | | | | | | | SSS | 0.588 | 0.616 | 0.638 | 0.657 | 0.670 | | | | | | | | | |

Table 8. SSIM results of varying component numbers experiments for every scene in the Mip-NeRF 360 dataset. t distribution and Gaussian distribution. The first is that a

| Scene - Components | Mip-NeRF 360 - average | | | | | | | Method | $\delta$ | $1.4 \delta$ | $1.8 \delta$ | $2.2 \delta$ | $2.6 \delta$ | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | 3DGS | 0.516 | 0.494 | 0.493 | 0.476 | 0.461 | | | | | | | GES | 0.485 | 0.466 | 0.439 | 0.419 | 0.409 | | | | | | | 3DHGS | 0.511 | 0.502 | 0.507 | 0.493 | 0.461 | | | | | | | 3DGS-MCMC | 0.284 | 0.264 | 0.250 | 0.239 | 0.231 | | | | | | | SSS | 0.284 | 0.264 | 0.249 | 0.237 | 0.228 | | | | | | | Scene - Components | Mip-NeRF 360 - bicycle | | | | | | | | Method | 54 k | 75 k | 97 k | 118 k | 140 k | | | | | | | 3DGS | 0.640 | 0.637 | 0.632 | 0.631 | 0.636 | | | | | | | GES | 0.640 | 0.642 | 0.648 | 0.646 | 0.642 | | | | | | | 3DHGS | 0.645 | 0.642 | 0.644 | 0.639 | 0.644 | | | | | | | 3DGS-MCMC | 0.430 | 0.405 | 0.387 | 0.371 | 0.360 | | | | | | | SSS | 0.434 | 0.412 | 0.394 | 0.379 | 0.367 | | | | | | | Scene - Components | Mip-NeRF 360 - bonsai | | | | | | | | Method | 206k | 280k | 360k | 440k | 520k | | | | | | | 3DGS | 0.388 | 0.360 | 0.361 | 0.352 | 0.340 | | | | | | | GES | 0.370 | 0.342 | 0.311 | 0.284 | 0.242 | | | | | | | 3DHGS | 0.384 | 0.366 | 0.360 | 0.347 | 0.343 | | | | | | | 3DGS-MCMC | 0.200 | 0.189 | 0.183 | 0.178 | 0.175 | | | | | | | SSS | 0.192 | 0.181 | 0.173 | 0.168 | 0.164 | | | | | | | Scene - Components | Mip-NeRF 360 - counter | | | | | | | | Method | 155k | 224k | 288k | 352k | 416k | | | | | | | 3DGS | 0.499 | 0.476 | 0.477 | 0.427 | 0.352 | | | | | | | GES | 0.461 | 0.359 | 0.284 | 0.266 | 0.242 | | | | | | | 3DHGS | 0.513 | 0.495 | 0.480 | 0.438 | 0.364 | | | | | | | 3DGS-MCMC | 0.212 | 0.200 | 0.192 | 0.187 | 0.183 | | | | | | | SSS | 0.214 | 0.197 | 0.188 | 0.181 | 0.176 | | | | | | | Scene - Components | Mip-NeRF 360 - garden | | | | | | | | Method | 138k | 196k | 252k | 308k | 364k | | | | | | | 3DGS | 0.661 | 0.640 | 0.627 | 0.613 | 0.601 | | | | | | | GES | 0.652 | 0.635 | 0.623 | 0.607 | 0.594 | | | | | | | 3DHGS | 0.649 | 0.638 | 0.623 | 0.606 | 0.541 | | | | | | | 3DGS-MCMC | 0.296 | 0.256 | 0.229 | 0.209 | 0.194 | | | | | | | SSS | 0.297 | 0.257 | 0.228 | 0.207 | 0.191 | | | | | | | Scene - Components | Mip-NeRF 360 - kitchen | | | | | | | | Method | 241k | 336k | 432k | 528k | 624k | | | | | | | 3DGS | 0.347 | 0.312 | 0.353 | 0.344 | 0.334 | | | | | | | GES | 0.235 | 0.260 | 0.211 | 0.163 | 0.156 | | | | | | | 3DHGS | 0.295 | 0.308 | 0.413 | 0.406 | 0.378 | | | | | | | 3DGS-MCMC | 0.145 | 0.134 | 0.129 | 0.125 | 0.122 | | | | | | | SSS | 0.142 | 0.132 | 0.125 | 0.119 | 0.115 | | | | | | | Scene - Components | Mip-NeRF 360 - room | | | | | | | | Method | 112k | 154k | 198k | 242k | 286k | | | | | | | 3DGS | 0.471 | 0.429 | 0.403 | 0.376 | 0.375 | | | | | | | GES | 0.404 | 0.396 | 0.361 | 0.325 | 0.342 | | | | | | | 3DHGS | 0.467 | 0.449 | 0.413 | 0.401 | 0.343 | | | | | | | 3DGS-MCMC | 0.235 | 0.223 | 0.215 | 0.209 | 0.203 | | | | | | | SSS | 0.230 | 0.217 | 0.207 | 0.200 | 0.195 | | | | | | | Scene - Components | Mip-NeRF 360 - stump | | | | | | | | Method | 32k | 44.8k | 57.6k | 70.4k | 83.2k | | | | | | | 3DGS | 0.610 | 0.605 | 0.595 | 0.590 | 0.588 | | | | | | | GES | 0.634 | 0.632 | 0.639 | 0.642 | 0.647 | | | | | | | 3DHGS | 0.627 | 0.619 | 0.612 | 0.612 | 0.613 | | | | | | | 3DGS-MCMC | 0.474 | 0.442 | 0.415 | 0.395 | 0.379 | | | | | | | SSS | 0.481 | 0.451 | 0.427 | 0.406 | 0.390 | | | |

Table 8. SSIM results of varying component numbers experiments for every scene in the Mip-NeRF 360 dataset. t distribution and Gaussian distribution. The first is that a

---

| Scene - Components | Tanks\&Temples - average |  |  |  |  |
| :--: | :--: | :--: | :--: | :--: | :--: |
| Method | $\delta$ | $1.4 \delta$ | $1.8 \delta$ | $2.2 \delta$ | $2.6 \delta$ |
| 3DGS | 15.70 | 15.79 | 15.88 | 16.37 | 16.64 |
| GES | 16.74 | 17.47 | 17.80 | 18.38 | 18.71 |
| 3DHGS | 15.13 | 15.26 | 15.36 | 15.55 | 15.69 |
| 3DGS-MCMC | 23.18 | 23.53 | 23.65 | 23.84 | 23.93 |
| SSS | 23.67 | 24.03 | 24.19 | 24.35 | 24.40 |
| Scene - Components | Tanks\&Temples - train |  |  |  |  |
| Method | 182k | 252k | 324k | 396k | 468k |
| 3DGS | 15.20 | 15.53 | 15.58 | 16.45 | 16.92 |
| GES | 18.15 | 19.39 | 19.94 | 21.14 | 21.58 |
| 3DHGS | 14.42 | 14.62 | 14.75 | 15.05 | 15.24 |
| 3DGS-MCMC | 21.72 | 22.07 | 22.03 | 22.24 | 22.34 |
| SSS | 22.46 | 22.74 | 22.84 | 22.92 | 22.97 |
| Scene - Components | Tanks\&Temples - truck |  |  |  |  |
| Method | 136k | 196k | 252k | 308k | 364k |
| 3DGS | 16.20 | 16.06 | 16.18 | 16.29 | 16.36 |
| GES | 15.32 | 15.54 | 15.66 | 15.62 | 15.85 |
| 3DHGS | 15.84 | 15.90 | 15.97 | 16.05 | 16.13 |
| 3DGS-MCMC | 24.64 | 24.99 | 25.27 | 25.44 | 25.53 |
| SSS | 24.88 | 25.33 | 25.53 | 25.74 | 25.88 |
| Scene - Components | Deep Blending - average |  |  |  |  |
| Method | $\delta$ | $1.4 \delta$ | $1.8 \delta$ | $2.2 \delta$ | $2.6 \delta$ |
| 3DGS | 15.95 | 16.05 | 16.33 | 16.60 | 17.06 |
| GES | 15.99 | 16.10 | 16.55 | 17.38 | 17.23 |
| 3DHGS | 16.22 | 16.63 | 16.54 | 18.02 | 18.07 |
| 3DGS-MCMC | 28.38 | 28.82 | 29.11 | 29.17 | 29.35 |
| SSS | 28.69 | 29.08 | 29.29 | 29.41 | 29.67 |
| Scene - Components | Deep Blending - drjohnson |  |  |  |  |
| Method | 80k | 112k | 144k | 176k | 208k |
| 3DGS | 16.66 | 16.64 | 17.05 | 16.91 | 17.34 |
| GES | 16.43 | 16.84 | 17.13 | 17.41 | 18.05 |
| 3DHGS | 17.17 | 17.29 | 17.37 | 19.19 | 19.82 |
| 3DGS-MCMC | 28.15 | 28.62 | 28.83 | 28.90 | 28.94 |
| SSS | 28.79 | 29.05 | 29.19 | 29.43 | 29.42 |
| Scene - Components | Deep Blending - playroom |  |  |  |  |
| Method | 37k | 51.8k | 66.6k | 81.4k | 96.2k |
| 3DGS | 15.23 | 15.45 | 15.61 | 16.28 | 16.78 |
| GES | 15.56 | 15.36 | 15.96 | 17.34 | 16.42 |
| 3DHGS | 15.26 | 15.96 | 15.71 | 16.22 | 16.94 |
| 3DGS-MCMC | 28.61 | 29.02 | 29.39 | 29.45 | 29.76 |
| SSS | 28.58 | 29.10 | 29.40 | 29.40 | 29.91 |

Table 10. PSNR results of varying component numbers experiments for every scene in Tanks\&Temples and Deep Blending dataset.
t-distribution is not a t-distribution anymore after convolution with another t-distribution. So, we did not add a lowpass filter like in vanilla 3DGS (adding 0.3 to the diagonal values of the projected covariance matrix) in our final model. However, we do add the low-pass filter as a practical solution in SGD + positive t-distribution (replacing Gaussian with Student's t but without negative components and SGHMC) in the Ablation Study for fair comparison with similar component numbers. This will be discussed in Sec. 3.2. Another difference is that Student's t distribution does not have a general "empirical rule" as Gaussian

| Scene - Components | Tanks\&Temples - average |  |  |  |  |
| :--: | :--: | :--: | :--: | :--: | :--: |
| Method | $\delta$ | $1.4 \delta$ | $1.8 \delta$ | $2.2 \delta$ | $2.6 \delta$ |
| 3DGS | 0.528 | 0.537 | 0.543 | 0.566 | 0.580 |
| GES | 0.589 | 0.623 | 0.642 | 0.668 | 0.680 |
| 3DHGS | 0.504 | 0.511 | 0.518 | 0.526 | 0.533 |
| 3DGS-MCMC | 0.826 | 0.836 | 0.842 | 0.848 | 0.851 |
| SSS | 0.827 | 0.839 | 0.846 | 0.852 | 0.856 |
| Scene - Components | Tanks\&Temples - train |  |  |  |  |
| Method | 182k | 252k | 324k | 396k | 468k |
| 3DGS | 0.506 | 0.521 | 0.523 | 0.561 | 0.584 |
| GES | 0.645 | 0.699 | 0.727 | 0.774 | 0.788 |
| 3DHGS | 0.473 | 0.479 | 0.485 | 0.494 | 0.505 |
| 3DGS-MCMC | 0.798 | 0.807 | 0.814 | 0.820 | 0.824 |
| SSS | 0.799 | 0.812 | 0.821 | 0.828 | 0.833 |
| Scene - Components | Tanks\&Temples - truck |  |  |  |  |
| Method | 136k | 196k | 252k | 308k | 364k |
| 3DGS | 0.549 | 0.554 | 0.563 | 0.571 | 0.576 |
| GES | 0.532 | 0.548 | 0.557 | 0.562 | 0.572 |
| 3DHGS | 0.535 | 0.543 | 0.550 | 0.557 | 0.561 |
| 3DGS-MCMC | 0.855 | 0.865 | 0.871 | 0.876 | 0.879 |
| SSS | 0.854 | 0.865 | 0.872 | 0.877 | 0.880 |
| Scene - Components | Deep Blending - average |  |  |  |  |
| Method | $\delta$ | $1.4 \delta$ | $1.8 \delta$ | $2.2 \delta$ | $2.6 \delta$ |
| 3DGS | 0.702 | 0.706 | 0.711 | 0.715 | 0.726 |
| GES | 0.704 | 0.707 | 0.717 | 0.732 | 0.732 |
| 3DHGS | 0.704 | 0.713 | 0.710 | 0.737 | 0.737 |
| 3DGS-MCMC | 0.881 | 0.887 | 0.890 | 0.893 | 0.895 |
| SSS | 0.881 | 0.887 | 0.891 | 0.893 | 0.897 |
| Scene - Components | Deep Blending - drjohnson |  |  |  |  |
| Method | 80k | 112k | 144k | 176k | 208k |
| 3DGS | 0.698 | 0.701 | 0.707 | 0.707 | 0.717 |
| GES | 0.696 | 0.705 | 0.715 | 0.721 | 0.735 |
| 3DHGS | 0.706 | 0.711 | 0.711 | 0.752 | 0.744 |
| 3DGS-MCMC | 0.880 | 0.885 | 0.888 | 0.890 | 0.891 |
| SSS | 0.880 | 0.885 | 0.890 | 0.894 | 0.896 |
| Scene - Components | Deep Blending - playroom |  |  |  |  |
| Method | 37k | 51.8k | 66.6k | 81.4k | 96.2k |
| 3DGS | 0.706 | 0.711 | 0.714 | 0.723 | 0.735 |
| GES | 0.713 | 0.709 | 0.719 | 0.742 | 0.730 |
| 3DHGS | 0.701 | 0.714 | 0.709 | 0.722 | 0.731 |
| 3DGS-MCMC | 0.883 | 0.890 | 0.893 | 0.896 | 0.899 |
| SSS | 0.882 | 0.888 | 0.892 | 0.891 | 0.897 |

Table 11. SSIM results of varying component numbers experiments for every scene in Tanks\&Temples and Deep Blending dataset.
distribution. In vanilla 3DGS, they use the "empirical rule" (also known as "68-95-99.7 rule" or "three-sigma rule") to truncate the projected Gaussian in 2D space. The "threesigma rule" can be applied to t-distributions with thin tails (high $\nu$ degree). Besides, we obtain different critical values of different $\nu$ degrees [9] and interpolate these to obtain appropriate truncated values for fat-tailed t-distributions with lower $\nu$ values.

Our training process does not use the adaptive density control in vanilla 3DGS. Instead, we recycle components with low opacity to high opacity components every $n$ iter-

---

| Scene - Components | Tanks\&Temples - average |  |  |  |  |  |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| Method | $\delta$ | $1.4 \delta$ | $1.8 \delta$ | $2.2 \delta$ | $2.6 \delta$ |  |
| 3DGS | 0.569 | 0.558 | 0.552 | 0.527 | 0.510 |  |
| GES | 0.500 | 0.460 | 0.441 | 0.410 | 0.397 |  |
| 3DHGS | 0.590 | 0.582 | 0.574 | 0.566 | 0.558 |  |
| 3DGS-MCMC | 0.231 | 0.214 | 0.203 | 0.195 | 0.187 |  |
| SSS | 0.229 | 0.210 | 0.195 | 0.185 | 0.177 |  |
| Scene - Components | Tanks\&Temples - train |  |  |  |  |  |
| Method | 182k | 252k | 324k | 396k | 468k |  |
| 3DGS | 0.564 | 0.548 | 0.543 | 0.504 | 0.476 |  |
| GES | 0.415 | 0.352 | 0.321 | 0.267 | 0.251 |  |
| 3DHGS | 0.597 | 0.592 | 0.583 | 0.575 | 0.561 |  |
| 3DGS-MCMC | 0.255 | 0.240 | 0.229 | 0.220 | 0.212 |  |
| SSS | 0.254 | 0.235 | 0.220 | 0.208 | 0.199 |  |
| Scene - Components | Tanks\&Temples - truck |  |  |  |  |  |
| Method | 136k | 196k | 252k | 308k | 364k |  |
| 3DGS | 0.575 | 0.569 | 0.560 | 0.551 | 0.545 |  |
| GES | 0.585 | 0.569 | 0.561 | 0.552 | 0.543 |  |
| 3DHGS | 0.583 | 0.571 | 0.564 | 0.558 | 0.555 |  |
| 3DGS-MCMC | 0.206 | 0.189 | 0.178 | 0.170 | 0.163 |  |
| SSS | 0.204 | 0.184 | 0.170 | 0.161 | 0.154 |  |
| Scene - Components | Deep Blending - average |  |  |  |  |  |
| Method | $\delta$ | $1.4 \delta$ | $1.8 \delta$ | $2.2 \delta$ | $2.6 \delta$ |  |
| 3DGS | 0.525 | 0.522 | 0.515 | 0.510 | 0.500 |  |
| GES | 0.522 | 0.522 | 0.509 | 0.495 | 0.491 |  |
| 3DHGS | 0.522 | 0.515 | 0.515 | 0.490 | 0.488 |  |
| 3DGS-MCMC | 0.325 | 0.310 | 0.301 | 0.295 | 0.290 |  |
| SSS | 0.319 | 0.305 | 0.296 | 0.289 | 0.282 |  |
| Scene - Components | Deep Blending - drjohnson |  |  |  |  |  |
| Method | 80k | 112k | 144k | 176k | 208k |  |
| 3DGS | 0.526 | 0.523 | 0.514 | 0.512 | 0.504 |  |
| GES | 0.529 | 0.522 | 0.508 | 0.503 | 0.487 |  |
| 3DHGS | 0.515 | 0.515 | 0.511 | 0.476 | 0.482 |  |
| 3DGS-MCMC | 0.316 | 0.290 | 0.290 | 0.284 | 0.280 |  |
| SSS | 0.310 | 0.297 | 0.288 | 0.280 | 0.275 |  |
| Scene - Components | Deep Blending - playroom |  |  |  |  |  |
| Method | 37k | 51.8 k | 66.6 k | 81.4 k | 96.2 k |  |
| 3DGS | 0.524 | 0.521 | 0.515 | 0.508 | 0.496 |  |
| GES | 0.516 | 0.522 | 0.509 | 0.488 | 0.494 |  |
| 3DHGS | 0.530 | 0.515 | 0.519 | 0.505 | 0.494 |  |
| 3DGS-MCMC | 0.333 | 0.320 | 0.312 | 0.306 | 0.301 |  |
| SSS | 0.329 | 0.314 | 0.304 | 0.298 | 0.289 |  |

Table 12. LPIPS results of varying component numbers experiments for every scene in Tanks\&Temples and Deep Blending dataset.
ations. This is done by creating a Multinomial distribution of opacity values and sampling all components that have relatively high probability. Recycle can only be achieved if certain conditions are met, i.e. the change to the current state (rendering result) after the recycle is minimal [15, 27]. We give detailed formulae of recycling in Sec. 5. The result of calculating the new covariance matrix after recycling includes the $\beta()$ function. Because CUDA native implementation does not include the support for the $\beta()$ function, we decompose the $\beta()$ function into $\Gamma()$ function for computation. Further, to prevent excessive val-

| Ablation Setup — Metric | PSNR $\uparrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$ |
| :-- | :--: | :--: | :--: |
| Mip-NeRF | 22.22 | 0.759 | 0.257 |
| 3DGS | 23.14 | 0.841 | 0.183 |
| GES | 23.35 | 0.836 | 0.198 |
| SGD + positive t-dis | 23.80 | 0.838 | 0.191 |
| SGHMC + positive t-dis | 24.53 | 0.864 | 0.155 |
| Full model | 24.87 | 0.873 | 0.138 |

Table 13. Ablation Study on Tanks\&Temples with the same component numbers as in baselines. More details are in the SM.

| Method $\backslash$ Scene | bicycle | bonsai | counter | garden | kitchen | room | stump | average |
| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| SGD + positive t-dis | 25.52 | 31.80 | 28.82 | 27.60 | 30.93 | 31.60 | 26.72 | 29.00 |
| SGHMC + positive t-dis | 0.801 | 0.952 | 0.920 | 0.879 | 0.936 | 0.933 | 0.817 | 0.891 |
| Full SSS model | 0.798 | 0.956 | 0.926 | 0.882 | 0.939 | 0.938 | 0.813 | 0.893 |

Table 15. SSIM results of ablation study for every scene in Mip-NeRF 360 dataset.

| Method $\backslash$ Scene | bicycle | bonsai | counter | garden | kitchen | room | stump | average |
| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| SGD + positive t-dis | 0.225 | 0.185 | 0.188 | 0.110 | 0.121 | 0.199 | 0.234 | 0.180 |
| SGHMC + positive t-dis | 0.182 | 0.159 | 0.168 | 0.099 | 0.112 | 0.181 | 0.183 | 0.155 |
| Full SSS model | 0.173 | 0.151 | 0.156 | 0.009 | 0.104 | 0.167 | 0.174 | 0.145 |

Table 16. LPIPS results of ablation study for every scene in Mip-NeRF 360 dataset.

| Dataset - Scene | Tanks\&Temples |  |  |  | Deep Blending |  |  |
| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| Method | train | truck | average | drjohnson | playroom | average |  |
| SGD + positive t-dis | 22.18 | 25.42 | 23.80 | 29.22 | 29.93 | 29.57 |  |
| SGHMC + positive t-dis | 22.92 | 26.15 | 24.53 | 29.45 | 30.04 | 29.75 |  |
| Full SSS model | 23.32 | 26.41 | 24.87 | 29.66 | 30.47 | 30.07 |  |

Table 17. PSNR results of ablation study for every scene in Tanks\&Temples and Deep Blending dataset.

| Dataset - Scene | Tanks\&Temples |  |  |  | Deep Blending |  |  |
| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| Method | train | truck | average | drjohnson | playroom | average |  |
| SGD + positive t-dis | 0.803 | 0.874 | 0.838 | 0.900 | 0.901 | 0.901 |  |
| SGHMC + positive t-dis | 0.838 | 0.891 | 0.864 | 0.902 | 0.905 | 0.903 |  |
| Full SSS model | 0.850 | 0.897 | 0.873 | 0.905 | 0.909 | 0.907 |  |

Table 18. SSIM results of ablation study for every scene in Tanks\&Temples and Deep Blending dataset.

| Dataset - Scene | Tanks\&Temples |  |  |  | Deep Blending |  |  |
| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: |
| Method | train | truck | average | drjohnson | playroom | average |  |
| SGD + positive t-dis | 0.226 | 0.156 | 0.191 | 0.248 | 0.247 | 0.247 |  |
| SGHMC + positive t-dis | 0.186 | 0.124 | 0.155 | 0.262 | 0.257 | 0.260 |  |
| Full SSS model | 0.166 | 0.109 | 0.138 | 0.249 | 0.245 | 0.247 |  |

Table 19. LPIPS results of ablation study for every scene in Tanks\&Temples and Deep Blending dataset.
ues by the $\Gamma()$ function with a large input number, we further use the $\ln (\Gamma())$ function in practice. These are shown

---

|  Dataset | Tanks\&Temples |  |  | Deep Blending |  |   |
| --- | --- | --- | --- | --- | --- | --- |
|  Method—Metric | PSNR $\uparrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$ | PSNR $\uparrow$ | SSIM $\uparrow$ | LPIPS $\downarrow$  |
|  3DGS | 23.14 | 0.841 | 0.183 | 29.41 | 0.903 | 0.243  |
|  SGD + positive t-dis | 23.80 | 0.838 | 0.191 | 29.57 | 0.901 | 0.247  |
|  SGHMC + 3DGS | 24.52 | 0.869 | 0.150 | 29.56 | 0.906 | 0.239  |
|  SGHMC + positive t-dis | 24.53 | 0.864 | 0.155 | 29.75 | 0.903 | 0.259  |
|  Full model | 24.87 | 0.873 | 0.138 | 30.07 | 0.907 | 0.247  |

Table 20. More Ablation Study.

|  Dataset | Mip-NeRF360 |  | Tanks\&Temples |  | Deep Blending |   |
| --- | --- | --- | --- | --- | --- | --- |
|  Method—Metric | FPS | Training time | FPS | Training time | FPS | Training time  |
|  3DGS | 99 | 21 min | 120 | 12 min | 119 | 21 min  |
|  3DGS-MCMC | 79 | 32 min | 105 | 17 min | 138 | 29 min  |
|  SSS | 71 | 45 min | 87 | 33 min | 100 | 21 min  |

Table 21. Training time and rendering efficiency. with Eqs. (34) and (35). Finally, we employ the Adam gradient in SGHMC. The hyperparameters in our training process are largely the same as those of the original 3DGS and 3DGS-MCMC (e.g. we down-scale images to the same resolution for large scenes in the Mip-NeRF360 dataset as most works did), but because we use Student's t, negative components, and SGHMC, there are some new parameters. Detailed parameter values can be found in our code. The code is available at https://github.com/realcrane/3D-student-splating-and-scooping.

All our experiments are running with one NVIDIA RTX 4090 GPU. We show training/rendering time in Tab. 21. SSS is slower compared to vanilla 3DGS but still achieves real-time rendering ( $>70 \mathrm{fps}$ ).

### 3.1. Adaptation of Baselines

To illustrate the parameter efficiency of our model, we did experiments with fewer components amount all baselines and our model. Because other baselines except 3DGSMCMC do not support setting the maximum number of components, we modified their code. These modifications are minimal and do not involve any adjustment of hyperparameters to ensure a fair comparison. Specifically, we modified the adaptive density control used by most baselines. We stop densification (cloning and splitting) if the current number of components has reached the maximum setting. Note that due to the existence of the pruning strategy, the number of components can also be decreased, which will allow densification to continue until the preset densification iteration is reached.

### 3.2. Ablation Study Implementation

We show our Ablation study results in Sec. 1.3. These include three settings with one or more components in SSS to evaluate their effectiveness. Setting 1 (SGD + positive t-dis) uses adaptive density control and SGD optimization in vanilla 3DGS with positive Student's t distribution. Setting 2 (SGHMC + positive t-dis) uses SGHMC only with positive Student's t distribution. Setting 3 (full SSS model) uses SGHMC with both positive and negative Student's t distribution. Since Student's t distribution does not have an analytical form after convolution with another Student's t , we remove the low-pass filter in the splatting algorithm [42, 43]. The main function of the low-pass filter is to ensure that the minimum scale of each component is close to 1 pixel. Removing the low-pass filter does not affect our final result, because our principled sampler can ensure that the final components are of the appropriate size. However, for setting 1 , the absence of a low-pass filter will lead to an increase in the number of components (considering that it will use more tiny components to reconstruct more details). In order to ensure the fairness of the comparison (to make our number of components roughly equal to that of vanilla 3DGS), we added a small value ( 0.3 ) to the covariance matrix of the t-distribution after projection as a low-pass filter from an engineering perspective. Furthermore, we do not present the results of using adaptive density control and SGD optimization in vanilla 3DGS with both positive and negative t-distributions. This is because our negative components are designed to be used with our principled sampler. The densification of positive components in vanilla 3DGS is based on the size of the components and their gradients, which is not applicable to negative components. Adding negative components directly to adaptive density control will lead to worse results and is not the focus of our study. Finally, we also test our SGHMC sampler with Gaussian distributions to show its advantage. We use the same hyperparameters related to SGHMC and keep all other hyperparameters the same as vanilla 3DGS in this experiment.

## 4. Forward and Backward Passes

Although the general forward/backward passes of SSS are similar to 3DGS, the equations are different. This is mainly due to the introduction of t-distribution. Below we give details of mathematical derivation.

### 4.1. Forward Pass

Affine transformation of 3D Student's t distribution 3D Student's t distribution is:

$$ \begin{gathered} T(x ; \mu, \Sigma, \nu)=\frac{\Gamma\left(\frac{\nu+3}{2}\right)}{(\nu \pi)^{\frac{3}{2}} \Gamma\left(\frac{\nu}{2}\right)|\Sigma|^{\frac{1}{2}}} \cdot\left[1+\frac{1}{\nu}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right]^{-\frac{\nu+3}{2}} \end{gathered} $$

where $\nu \geq 1, x, \mu \in \mathbb{R}^{3} \Sigma \in \mathbb{R}^{3 \times 3}$ are the control parameter, the mean and the covariance matrix, which describes the spread and orientation of the distribution in 3D space. To render an image, Eq. (1) needs to be projected into a 2D image plane, which goes through a view transformation $W$,

---

$d$, and a (approximate) projective transformation $J[42,43]$, so a 3D point $x$ after transformation becomes $u$ :

$$
\begin{array}{r}
u=m^{-1}(x)=A^{-1}(x-b) \\
\text { where } A=J W \text { and } b=x+J(d-t)
\end{array}
$$

where $t$ is the camera coordinates. Applying the transformations Eq. (2) to Eq. (1) gives:

$$
\begin{aligned}
& T(x)=\frac{\Gamma\left(\frac{\nu+3}{2}\right)}{(\nu \pi)^{\frac{3}{2}} \Gamma\left(\frac{\nu}{2}\right)|\Sigma|^{\frac{1}{2}}} \\
& \cdot\left[1+\frac{1}{\nu}\left(A^{-1}(x-b)-\mu\right)^{T} \Sigma^{-1}\left(A^{-1}(x-b)-\mu\right)\right]^{-\frac{\nu+3}{2}} \\
& =\frac{\Gamma\left(\frac{\nu+3}{2}\right)}{(\nu \pi)^{\frac{3}{2}} \Gamma\left(\frac{\nu}{2}\right)|\Sigma|^{\frac{1}{2}}} \\
& \cdot\left[1+\frac{1}{\nu}\left(A^{-1} x-A^{-1} b-A^{-1} A \mu\right)^{T}\right. \\
& \left.\cdot \Sigma^{-1}\left(A^{-1} x-A^{-1} b-A^{-1} A \mu\right)\right]^{-\frac{\nu+3}{2}} \\
& =\frac{\Gamma\left(\frac{\nu+3}{2}\right)}{(\nu \pi)^{\frac{3}{2}} \Gamma\left(\frac{\nu}{2}\right)|\Sigma|^{\frac{1}{2}}} \\
& \cdot\left[1+\frac{1}{\nu}(x-b-A \mu)^{T}\left(A^{-1}\right)^{T} \Sigma^{-1} A^{-1}(x-b-A \mu)\right]^{-\frac{\nu+3}{2}} \\
& =\frac{\Gamma\left(\frac{\nu+3}{2}\right)}{(\nu \pi)^{\frac{3}{2}} \Gamma\left(\frac{\nu}{2}\right)|\Sigma|^{\frac{1}{2}}} \\
& \cdot\left[1+\frac{1}{\nu}(x-m(\mu))^{T}\left(A \Sigma A^{T}\right)^{-1}(x-m(\mu))\right]^{-\frac{\nu+3}{2}} \\
& =\frac{\Gamma\left(\frac{\nu+3}{2}\right)|A|}{(\nu \pi)^{\frac{3}{2}} \Gamma\left(\frac{\nu}{2}\right)|\Sigma|^{\frac{1}{2}}|A|^{\frac{1}{2}}\left|A^{T}\right|^{\frac{1}{2}}} \\
& \cdot\left[1+\frac{1}{\nu}(x-m(\mu))^{T}\left(A \Sigma A^{T}\right)^{-1}(x-m(\mu))\right]^{-\frac{\nu+3}{2}} \\
& =|A| \frac{\Gamma\left(\frac{\nu+3}{2}\right)}{(\nu \pi)^{\frac{3}{2}} \Gamma\left(\frac{\nu}{2}\right)\left|A \Sigma A^{T}\right|^{\frac{1}{2}}} \\
& \cdot\left[1+\frac{1}{\nu}(x-m(\mu))^{T}\left(A \Sigma A^{T}\right)^{-1}(x-m(\mu))\right]^{-\frac{\nu+3}{2}} \\
& =|A| T(x, m(\mu), A \Sigma A^{T}, \nu)
\end{aligned}
$$

So for $r(x)=T(x ; \mu, \Sigma, \nu)$ at $x$, its transformed density is:

$$
\begin{aligned}
& r_{k}^{\prime}(x)=\frac{1}{\left|A^{-1}\right|} T(x ; m(\mu), A \Sigma A^{T}, \nu) \\
& =\frac{1}{\left|W^{-1} J_{k}^{-1}\right|} T(x ; m(\mu), J W \Sigma W^{T} J^{T}, \nu) \\
& =\frac{1}{\left|W^{-1} J^{-1}\right|} T(x ; m(\mu), \Sigma^{\prime}, \nu)
\end{aligned}
$$

where $\Sigma^{\prime}=J W \Sigma W^{T} J^{T}$.
This result still holds after we drop the normalization constant $\frac{\Gamma\left(\frac{\nu+3}{2}\right)}{(\nu \pi)^{\frac{3}{2}} \Gamma\left(\frac{\nu}{2}\right)|\Sigma|^{\frac{1}{2}}}$.

Integral along a ray (marginalization) After deriving the view and projective transformation for Student's $t$ distribution, we need to integrate it along the ray that intersects with it for rendering. i.e. splatting it. This is actually the marginalization of Student's $t$ distribution along one dimension, and it can be done by another affine transformation $y=W x$, where $W=\left[\begin{array}{l}\mathbf{I}, 0 \\ 0,0\end{array}\right]$ with $\mathbf{I}$ is a $2 \times 2$ identify matrix. To see this, imagine $x$ can be divided into two parts $x=\left[\begin{array}{l}x_{1} \\ x_{2}\end{array}\right]$, so that:

$$
T(W x ; \mu, \Sigma, \nu, p)=T\left(y ; W \mu, W \Sigma W^{T}, \nu, p\right)
$$

and if $\mu=\left[\begin{array}{ll}
\mu_{a} \\
\mu_{b}
\end{array}\right]$ and $\Sigma=\left[\begin{array}{ll}
\Sigma_{a a} & \Sigma_{a b} \\
\Sigma_{b a} & \Sigma_{b b}
\end{array}\right]$ then

$$
T(x ; \mu, \Sigma, \nu, p)=T\left(\binom{x_{a}}{x_{b}} ;\binom{\mu_{a}}{\mu_{b}} ;\binom{\Sigma_{a a}}{\Sigma_{b a}} \frac{\Sigma_{a b}}{\Sigma_{b b}}, \nu, p\right)
$$

where means by transforming $x$ with $W=\left[\begin{array}{l}\mathbf{I}, 0 \\ 0,0\end{array}\right]$, we will get the marginal distribution of $x_{a}$ :

$$
\begin{aligned}
T(W x ; \mu, \Sigma, \nu, p) & =T\left(y ; W \mu, W \Sigma W^{T}, \nu, p\right) \\
& =T\left(x_{a} ; \mu_{a}, \Sigma_{a a}, \nu, p_{a}\right)
\end{aligned}
$$

This way we can marginalize any variable for $T$. Integration along a ray gives a 2 D t-distribution:

$$
T_{2 D}(x, \mu, \Sigma, \nu)=\left[1+\frac{1}{\nu}(x-\mu)^{T} \Sigma^{-1}(x-\mu)\right]^{-\frac{\nu+2}{2}}
$$

where $x$ now is in 2D space.
So far, we have derived all key equations for the forward pass when using t-distributions as the mixture components.

### 4.2. Backward Pass

The most important computation in the backward pass is to compute all the key gradients, $\frac{\partial L}{\partial \mu}, \frac{\partial L}{\partial S}, \frac{\partial L}{\partial R}, \frac{\partial L}{\partial c}, \frac{\partial L}{\partial o}$, and $\frac{\partial L}{\partial \nu}$, where $L$ is the loss, $\mu$ is the mean of a t-distribution. $S$ and $R$ are the scaling and rotation matrices for the covariance matrix of t-distribution. $c$ is the color represented in spherical harmonics. $o$ is the opacity. $\nu$ is the control parameter of the t-distribution.
$L$ is calculated between the ground-truth pixel colors and rendered colors. The rendered pixel colors are:

$$
c(x)=\sum_{i=1}^{N} c_{i} o_{i} T_{i}^{2 D}(x) \prod_{j=1}^{i-1}\left(1-o_{j} T_{j}^{2 D}(x)\right)
$$

where $x$ is the position of a pixel in 2D images.
For simplicity, we let $\alpha=o T$. Based on the chain rule:

$$
\frac{\partial_{L}}{\partial_{\mu}}=\frac{\partial_{L}}{\partial_{r g b}} \cdot \frac{\partial_{r g b}}{\partial_{\alpha}} \cdot \frac{\partial_{\alpha}}{\partial_{T^{2 D}}} \cdot \frac{\partial_{T^{2 D}}}{\partial_{h}} \cdot \frac{\partial_{h}}{\partial_{\mu^{\prime}}} \cdot \frac{\partial_{\mu^{\prime}}}{\partial_{\mu}}
$$

---

$$
\begin{gathered}
\frac{\partial_{\text {Loss }}}{\partial_{o}}=\frac{\partial_{\text {Loss }}}{\partial_{r g b}} \cdot \frac{\partial_{r g b}}{\partial_{\alpha}} \cdot \frac{\partial_{\alpha}}{\partial_{o}} \\
\frac{\partial_{L}}{\partial_{c}}=\frac{\partial_{L}}{\partial_{r g b}} \cdot \frac{\partial_{r g b}}{\partial_{c}} \\
\frac{\partial_{L}}{\partial_{S}}=\frac{\partial_{L}}{\partial_{r g b}} \cdot \frac{\partial_{r g b}}{\partial} \cdot \frac{\partial_{\alpha}}{\partial_{T^{2 D}}} \cdot \frac{\partial_{T^{2 D}}}{\partial_{h}} \cdot \frac{\partial_{h}}{\partial_{\Sigma^{\prime}}} \cdot \frac{\partial_{\Sigma^{\prime}}}{\partial_{\Sigma}} \cdot \frac{\partial_{\Sigma}}{\partial_{S}} \\
\frac{\partial_{L}}{\partial_{R}}=\frac{\partial_{L}}{\partial_{r g b}} \cdot \frac{\partial_{r g b}}{\partial_{\alpha}} \cdot \frac{\partial_{\alpha}}{\partial_{T^{2 D}}} \cdot \frac{\partial_{T^{2 D}}}{\partial_{h}} \cdot \frac{\partial_{h}}{\partial_{\Sigma^{\prime}}} \cdot \frac{\partial_{\Sigma^{\prime}}}{\partial_{\Sigma}} \cdot \frac{\partial_{\Sigma}}{\partial_{R}} \\
\frac{\partial_{L}}{\partial_{\nu}}=\frac{\partial_{L}}{\partial_{r g b}} \cdot \frac{\partial_{r g b}}{\partial_{\alpha}} \cdot \frac{\partial_{\alpha}}{\partial_{T^{2 D}}} \cdot \frac{\partial_{T^{2 D}}}{\partial_{\nu}}
\end{gathered}
$$

where $\mu^{\prime}$ and $\Sigma^{\prime}$ are the projected $\mu$ and $\Sigma$ in 2D space. $h$ is a predefined function explained shortly.

For these gradients, we only need to replace the calculation of Gaussian in vanilla 3DGS with the calculation of Student's $t$ distribution based on the chain rule, except for the gradient for $\nu$ in Eq. (15). In order to further simplify the calculation, we extract the same part of the 2D Gaussian and 2D Student's $t$ distributions, and define a function $h(x)$ :

$$
\begin{aligned}
& G^{2 D}\left(x, \mu^{\prime}, \Sigma^{\prime}\right)=\exp ^{-\frac{1}{2} h(x)} \\
& T^{2 D}\left(x, \mu^{\prime}, \Sigma^{\prime}, \nu\right)=\left[1+\frac{1}{\nu} h(x)\right]^{-\frac{\nu+2}{2}}
\end{aligned}
$$

where

$$
h(x)=\left(x-\mu^{\prime}\right)^{T} \Sigma^{\prime-1}\left(x-\mu^{\prime}\right)
$$

We have

$$
\begin{aligned}
& \frac{\partial T^{2 D}\left(x, \mu^{\prime}, \Sigma^{\prime}, \nu\right)}{\partial h(x)}=\left[1+\frac{1}{\nu} h(x)\right]^{-\frac{\nu+2}{2}} \quad d h(x) \\
& =-\frac{\nu+2}{2} \cdot \frac{1}{\nu} \cdot\left[1+\frac{1}{\nu} h(x)\right]^{-\frac{\nu+4}{2}} \quad d h(x)
\end{aligned}
$$

to replace every

$$
\frac{\partial G^{2 D}\left(x, \mu^{\prime}, \Sigma^{\prime}\right)}{\partial h(x)}=-\frac{1}{2} \exp ^{-\frac{1}{2} h(x)} \quad d h(x)
$$

Then we can use the derived calculations in 3DGS for the rest of the parts, we refer the readers to read [37] for more detailed mathematics if interested.

Finally, the gradient for optimizing $\nu$ needs to be calcu-
lated separately. Assuming $g(\nu)=1+\frac{1}{\nu} h(x)$, we have

$$
\begin{aligned}
\frac{\partial_{T^{2 D}\left(x, \mu^{\prime}, \Sigma^{\prime}, \nu\right)}}{\partial_{\nu}} & =\left[1+\frac{1}{\nu}\left(x-\mu^{\prime}\right)^{T} \Sigma^{\prime-1}\left(x-\mu^{\prime}\right)\right]^{-\frac{\nu+2}{2}} d \nu \\
& =\left[1+\frac{1}{\nu} h(x)\right]^{-\frac{\nu+2}{2}} d \nu \\
& =g(\nu)^{-\frac{\nu+2}{2}} d \nu \\
& =-\frac{\nu+2}{2} g(\nu)^{-\frac{\nu+4}{2}} \cdot g^{\prime}(\nu) \\
& =-\frac{\nu+2}{2}\left(1+\frac{1}{\nu} h(x)\right)^{-\frac{\nu+4}{2}} \\
& \cdot\left(\left(1+\frac{1}{\nu} h(x)\right) d \nu\right) \\
& =-\frac{\nu+2}{2}\left(1+\frac{1}{\nu} h(x)\right)^{-\frac{\nu+4}{2}} \cdot\left(-\frac{h(x)}{\nu^{2}}\right) \\
& =\frac{\nu+2}{2} \cdot \frac{h(x)}{\nu^{2}}\left[1+\frac{1}{\nu} h(x)\right]^{-\frac{\nu+4}{2}} \\
& =\frac{\nu+2}{2} \cdot \frac{\left(x-\mu^{\prime}\right)^{T} \Sigma^{\prime-1}\left(x-\mu^{\prime}\right)}{\nu^{2}} \\
& \cdot\left[1+\frac{1}{\nu}\left(x-\mu^{\prime}\right)^{T} \Sigma^{\prime-1}\left(x-\mu^{\prime}\right)\right]^{-\frac{\nu+4}{2}}
\end{aligned}
$$

## 5. Component Recycling

The key equation for relocating low opacity components to a high opacity component is to make sure the distribution before and after the relocation is not changed [15, 27]. If we move new components to the location of an old component $\mu_{\text {new }}=\mu_{\text {old }}$, this is ensured by separately handling the opacity and the covariance matrix. For opacity, it is simply:

$$
\left(1-O_{\text {new }}\right)^{N}=\left(1-O_{\text {old }}\right)
$$

For covariance:

$$
\begin{aligned}
& \operatorname{minimize} \int_{-\infty}^{\infty}\left\|C_{\text {new }}(x)-C_{\text {old }}(x)\right\| d x \text { or } \\
& \operatorname{minimize}\left\|\int_{-\infty}^{\infty} C_{\text {new }}(x)-\int_{-\infty}^{\infty} C_{\text {old }}(x)\right\| d x
\end{aligned}
$$

To solve Eq. (22), we first need to separately derive $\int_{-\infty}^{\infty} C_{\text {old }}(x)$ and $\int_{-\infty}^{\infty} C_{\text {new }}(x)$. For $\int_{-\infty}^{\infty} C_{\text {old }}(x)$, assuming $u=\frac{x}{\sqrt{\nu_{\text {old }} \Sigma_{\text {old }}}}, d u=\frac{1}{\sqrt{\nu_{\text {old }} \Sigma_{\text {old }}}} d x$, and $d x=$

---

$\sqrt{\nu_{o l d} \Sigma_{o l d}} d u$, then:

$$
\begin{aligned}
\int_{-\infty}^{\infty} C_{o l d}(x)= & \int_{-\infty}^{\infty} o_{o l d}\left[1+\frac{1}{\nu_{o l d}} \frac{x^{2}}{\Sigma_{o l d}}\right]^{-\frac{\nu_{o l d}+3}{2}} d x \\
& =o_{o l d} \int_{-\infty}^{\infty}\left[1+\frac{1}{\nu_{o l d}} \frac{x^{2}}{\Sigma_{o l d}}\right]^{-\frac{\nu_{o l d}+3}{2}} d x \\
& =o_{o l d} \int_{-\infty}^{\infty}\left[1+u^{2}\right]^{-\frac{\nu_{o l d}+3}{2}} \sqrt{\nu_{o l d} \Sigma_{o l d}} d u \\
& =o_{o l d} \sqrt{\nu_{o l d} \Sigma_{o l d}} \int_{-\infty}^{\infty}\left[1+u^{2}\right]^{-\frac{\nu_{o l d}+3}{2}} d u
\end{aligned}
$$

For the form $\int_{-\infty}^{\infty}\left[1+x^{2}\right]^{-\alpha} d x$, assuming $x=\tan (\theta)$, $d x=\sec ^{2}(\theta) d \theta$, and $1+x^{2}=1+\tan ^{2}(\theta)=\sec ^{2}(\theta)$, then:

$$
\begin{aligned}
& \int_{-\infty}^{\infty}\left[1+x^{2}\right]^{-\alpha} d x \\
& =2 \int_{0}^{\infty}\left[1+x^{2}\right]^{-\alpha} d x \\
& =2 \int_{0}^{\pi / 2}\left(\sec ^{2}(\theta)\right)^{-\alpha} \sec ^{2}(\theta) d \theta \\
& =2 \int_{0}^{\pi / 2}\left(\sec ^{2-2 \alpha}(\theta)\right) d \theta \\
& =2 \int_{0}^{\pi / 2}\left(\cos ^{2 \alpha-2}(\theta)\right) d \theta
\end{aligned}
$$

Further, according to the definition of the $\beta$ function:

$$
\beta(x, y)=2 \int_{0}^{\pi / 2} \sin ^{2 x-1}(\theta) \cos ^{2 y-1}(\theta) d \theta
$$

Eq. (24) becomes:

$$
\begin{aligned}
& 2 \int_{0}^{\pi / 2}\left(\cos ^{2 \alpha-2}(\theta)\right) d \theta \\
& 2 \int_{0}^{\pi / 2} \sin ^{2 \frac{1}{2}-1}(\theta) \cos ^{2\left(\alpha-\frac{1}{2}\right)-1}(\theta) d \theta \\
& =\beta\left(\frac{1}{2},\left(\alpha-\frac{1}{2}\right)\right)
\end{aligned}
$$

Then Eq. (23) becomes:

$$
\begin{aligned}
& o_{o l d} \sqrt{\nu_{o l d} \Sigma_{o l d}} \int_{-\infty}^{\infty}\left[1+u^{2}\right]^{-\frac{\nu_{o l d}+3}{2}} d u \\
& =o_{o l d} \sqrt{\nu_{o l d} \Sigma_{o l d}} \cdot \beta\left(\frac{1}{2}, \frac{\nu_{o l d}+2}{2}\right)
\end{aligned}
$$

For $\int_{-\infty}^{\infty} C_{\text {new }}(x)$,

$$
\begin{aligned}
& \int_{-\infty}^{\infty} C_{\text {new }}(x)=\int_{-\infty}^{\infty} \sum_{i=1}^{N} o_{\text {new }}\left[1+\frac{1}{\nu_{\text {new }}} \frac{x^{2}}{\Sigma_{\text {new }}}\right]^{-\frac{\nu_{\text {new }}+3}{2}} \\
& \cdot\left(1-o_{\text {new }}\left[1+\frac{1}{\nu_{\text {new }}} \frac{x^{2}}{\Sigma_{\text {new }}}\right]^{-\frac{\nu_{\text {new }}+3}{2}}\right)^{i-1} d x
\end{aligned}
$$

From Binomial theorem:

$$
(x+y)^{n}=\sum_{k=0}^{n}\binom{n}{k} x^{n-k} y^{k}=\sum_{k=0}^{n}\binom{n}{k} x^{k} y^{n-k}
$$

Eq. (28) becomes:

$$
\begin{aligned}
& \int_{-\infty}^{\infty} \sum_{i=1}^{N} o_{n e w}\left[1+\frac{1}{\nu_{n e w}} \frac{x^{2}}{\Sigma_{n e w}}\right]^{-\frac{\nu_{n e w}+3}{2}} \\
& \cdot\left(1-o_{n e w}\left[1+\frac{1}{\nu_{n e w}} \frac{x^{2}}{\Sigma_{n e w}}\right]^{-\frac{\nu_{n e w}+3}{2}}\right)^{i-1} d x \\
& =\int_{-\infty}^{\infty} \sum_{i=1}^{N} o_{n e w}\left[1+\frac{1}{\nu_{n e w}} \frac{x^{2}}{\Sigma_{n e w}}\right]^{-\frac{\nu_{n e w}+3}{2}} \\
& \cdot \sum_{k=0}^{i-1}\binom{i-1}{k}\left(-o_{n e w}\left[1+\frac{1}{\nu_{n e w}} \frac{x^{2}}{\Sigma_{n e w}}\right]^{-\frac{\nu_{n e w}+3}{2}}\right)^{k} d x \\
& =\int_{-\infty}^{\infty} \sum_{i=1}^{N} o_{n e w}\left[1+\frac{1}{\nu_{n e w}} \frac{x^{2}}{\Sigma_{n e w}}\right]^{-\frac{\nu_{n e w}+3}{2}} \\
& \cdot \sum_{k=0}^{i-1}\binom{i-1}{k}(-1)^{k}\left(o_{n e w}\right)^{k}\left(\left[1+\frac{1}{\nu_{n e w}} \frac{x^{2}}{\Sigma_{n e w}}\right]^{-\frac{\nu_{n e w}+3}{2}}\right)^{k} d x \\
& =\int_{-\infty}^{\infty} \sum_{i=1}^{N} \sum_{k=0}^{i-1}\binom{i-1}{k}(-1)^{k}\left(o_{n e w}\right)^{k+1} \\
& \cdot\left(\left[1+\frac{1}{\nu_{n e w}} \frac{x^{2}}{\Sigma_{n e w}}\right]^{-\frac{(k+1)\left(\nu_{n e w}+3\right)}{2}}\right) d x \\
& =\sum_{i=1}^{N} \sum_{k=0}^{i-1}\binom{i-1}{k}(-1)^{k}\left(o_{n e w}\right)^{k+1} \\
& \cdot \int_{-\infty}^{\infty}\left(\left[1+\frac{1}{\nu_{n e w}} \frac{x^{2}}{\Sigma_{n e w}}\right]^{-\frac{(k+1)\left(\nu_{n e w}+3\right)}{2}}\right) d x
\end{aligned}
$$

According to equations 24, 25 and 26, Eq. (30) becomes:

$$
\begin{aligned}
& \sum_{i=1}^{N} \sum_{k=0}^{i-1}\binom{i-1}{k}(-1)^{k}\left(o_{n e w}\right)^{k+1} \\
& \cdot \int_{-\infty}^{\infty}\left(\left[1+\frac{1}{\nu_{n e w}} \frac{x^{2}}{\Sigma_{n e w}}\right]^{-\frac{(k+1)\left(\nu_{n e w}+3\right)}{2}}\right) d x \\
& =\sum_{i=1}^{N} \sum_{k=0}^{i-1}\binom{i-1}{k}(-1)^{k}\left(o_{n e w}\right)^{k+1} \\
& \cdot \sqrt{\nu_{n e w} \Sigma_{n e w}} \cdot \beta\left(\frac{1}{2}, \frac{(k+1)\left(\nu_{n e w}+3\right)-1}{2}\right)
\end{aligned}
$$

Having derived $\int_{-\infty}^{\infty} C_{o l d}(x)$ and $\int_{-\infty}^{\infty} C_{\text {new }}(x)$, we mini-

---

mize Eq. (22) by setting:

$$
\begin{aligned}
& \int_{-\infty}^{\infty} C_{\text {new }}(x)=\int_{-\infty}^{\infty} C_{\text {old }}(x) \\
\Rightarrow & \sum_{i=1}^{N} \sum_{k=0}^{i-1}\binom{i-1}{k}(-1)^{k}\left(o_{\text {new }}\right)^{k+1} \\
& \cdot \sqrt{\nu_{\text {new }} \Sigma_{\text {new }}} \beta\left(\frac{1}{2}, \frac{(k+1)\left(\nu_{\text {new }}+3\right)-1}{2}\right) \\
& =o_{\text {old }} \sqrt{\nu_{\text {old }} \Sigma_{\text {old }}} \beta\left(\frac{1}{2}, \frac{\nu_{\text {old }}+2}{2}\right) \\
\Rightarrow & \sqrt{\nu_{\text {new }} \Sigma_{\text {new }}} \sum_{i=1}^{N} \sum_{k=0}^{i-1}\binom{i-1}{k}(-1)^{k}\left(o_{\text {new }}\right)^{k+1} \\
& \cdot \beta\left(\frac{1}{2}, \frac{(k+1)\left(\nu_{\text {new }}+3\right)-1}{2}\right) \\
& =o_{\text {old }} \sqrt{\nu_{\text {old }} \Sigma_{\text {old }}} \beta\left(\frac{1}{2}, \frac{\nu_{\text {old }}+2}{2}\right) \\
\Rightarrow & \Sigma_{\text {new }}=\left(o_{\text {old }}\right)^{2} \frac{\nu_{\text {old }}}{\nu_{\text {new }}} \\
& \left(\frac{\beta\left(\frac{1}{2}, \frac{\nu_{\text {old }}+2}{2}\right)}{\sum_{i=1}^{N} \sum_{k=0}^{i-1}\binom{i-1}{k}(-1)^{k}\left(o_{\text {new }}\right)^{k+1} \beta\left(\frac{1}{2}, \frac{(k+1)\left(\nu_{\text {new }}+3\right)-1}{2}\right)}\right)^{2} \\
& \cdot \Sigma_{\text {old }}
\end{aligned}
$$

So at the end, we can compute $\Sigma_{\text {new }}$ based on $\Sigma_{\text {old }}$ :

$$
\begin{aligned}
& \Sigma_{\text {new }}=\left(o_{\text {old }}\right)^{2} \frac{\nu_{\text {old }}}{\nu_{\text {new }}} \\
& \left(\frac{\beta\left(\frac{1}{2}, \frac{\nu_{\text {old }}+2}{2}\right)}{\sum_{i=1}^{N} \sum_{k=0}^{i-1}\binom{i-1}{k}(-1)^{k}\left(o_{\text {new }}\right)^{k+1} \beta\left(\frac{1}{2}, \frac{(k+1)\left(\nu_{\text {new }}+3\right)-1}{2}\right)}\right)^{2} \\
& \cdot \Sigma_{\text {old }}
\end{aligned}
$$

Furthermore, since $\beta$ function can be represented by $\Gamma$ functions:

$$
\beta(x, y)=\frac{\Gamma(x) \Gamma(y)}{\Gamma(x+y)}
$$

We can use the $\Gamma$ function or $\ln (\Gamma)$ function instead of $\beta$ function in practice:

$$
\begin{aligned}
\beta(x, y) & =\frac{\Gamma(x) \Gamma(y)}{\Gamma(x+y)} \\
\Rightarrow \ln (\beta(x, y)) & =\ln (\Gamma(x))+\ln (\Gamma(y))-\ln (\Gamma(x+y)) \\
\Rightarrow \beta(x, y) & =\exp (\ln (\Gamma(x))+\ln (\Gamma(y))-\ln (\Gamma(x+y)))
\end{aligned}
$$

## 6. SGHMC Sampling

In SGHMC [4], the posterior distribution of model parameters $\theta$ given a set of independent observations $x \in D$ is defined as

$$
\pi(\theta, r) \propto \exp (-U(\theta))
$$

where $U(\theta)$ is a potential energy function which is $-\sum_{x \in D} \log p(x \mid \theta)-\log p(\theta)$.

To sample from $p(\theta \mid D)$, The Hamiltonian (Hybrid) Monte Carlo (HMC) considers generating samples from a joint distribution of $\pi(\theta, r)$ defined by

$$
\pi(\theta, r) \propto \exp \left(-U(\theta)-\frac{1}{2} r^{T} M r\right)
$$

where the Hamiltonian function is defined by $H(\theta, r)=$ $U(\theta)+\frac{1}{2} r^{T} M r . M$ is the mass matrix and $r$ is the auxiliary momentum variables. Further, to introduce stochastic gradients into the sampling, the Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) is proposed in [4] where an additional friction is introduced. We refer the readers to [4] for detailed mathematical derivation.

To employ SGHMC for our SSS sampling, we start by parameterizing a joint distribution:

$$
P(\theta, r) \propto \exp \left(-L_{\theta}(x)-\frac{1}{2} r^{T} I r\right)
$$

By defining a similar Hamiltonian function as in Eq. (37) and adding a friction term as in [4], we derive

$$
\begin{aligned}
d \theta & =M^{-1} r d t \\
d r & =-\nabla U(\theta) d t-C M^{-1} r d t+\mathcal{N}(0,2 C d t)
\end{aligned}
$$

Next, we further modify the updating equations in Eq. (39) to:

$$
\begin{aligned}
\mu_{t+1} & =\mu_{t}-\varepsilon^{2}\left[\frac{\partial L}{\partial \mu}\right]_{t}+F+N \\
F & =\sigma(o) \varepsilon(1-\varepsilon C) r_{t-1} \\
N & =\sigma(o) \mathcal{N}\left(0,2 \varepsilon^{\frac{3}{2}} C\right) \\
r_{t+1} & =r_{t}-\varepsilon\left[\frac{\partial L}{\partial \mu}\right]_{t+1}-\varepsilon C r_{t-1}+\mathcal{N}(0,2 \varepsilon C) \\
& \text { where } \sigma(o)=\sigma(-k(o-t))
\end{aligned}
$$

where $\varepsilon$ is the learning rate and decays during learning. $\mathcal{N}$ is Gaussian noise. $o$ is the opacity. To further clarify the relation between the learnable parameter and the momentum, we first show the updating rule for the learnable parameter in the original SGHMC:

$$
\begin{aligned}
\mu_{t+1} & =\mu_{t}+\varepsilon *\left(r_{t}-\varepsilon * G_{t}-\varepsilon * C * r_{t}+N(0,2 * \varepsilon * C)\right) \\
& =\mu_{t}+\varepsilon * r_{t}-\varepsilon^{2} * G_{t}-\varepsilon^{2} * C * r_{t}+N\left(0,2 * \varepsilon^{\frac{3}{2}} * C\right) \\
& =\mu_{t}-\varepsilon^{2} * G_{t}+\varepsilon *(1-\varepsilon * C) * r_{t}+N\left(0,2 * \varepsilon^{\frac{3}{2}} * C\right)
\end{aligned}
$$

where $G$ is the gradient $\left[\frac{\partial L}{\partial \mu}\right]$.
Before Stochastic Gradient Hamiltonian Monte Carlo (SGHMC), we first attempted the Stochastic Gradient Langevin Dynamics (SGLD) sampling in 3DGSMCMC [15]. Although it outperforms the standard optimization employed in the original 3DGS and its variants, it

---

still sometimes generates suboptimal results. Other than the randomness in the optimization itself, we suspected the core reason is the increased model complexity, especially the introduction of $\nu$ in t-distribution, which brings tight coupling between many parameters, e.g. $\nu$ greatly influencing $\mu$ and $\Sigma$. On the high level, the optimization of $\mu$ and $\Sigma$ can be seen as seeking the optimal distribution within a family of distributions. In 3DGS, this family is Gaussians. However, when $\nu$ is also optimized, the family itself changes during optimization. This is the core reason we resort to SGHMC which shows better sampling behaviors given tightly coupled parameters [4].

Furthermore, we leave the learning of $\nu$ and other parameters to Adam, as this can help further decouple the parameters. This is a similar strategy to the 3DGS-MCMC. This is complemented by using SGHMC on the location of t-distribution $\mu$. Also, for components with high opacity, we tend to think that they are near their local optima, so no further random perturbation is needed. This is achieved by adding a sigmoid switch:

$$
\mu_{t+1}=\mu_{t}-\varepsilon^{2} * G_{t}+\sigma\left(\varepsilon *(1-\varepsilon * C) * r_{t}\right)+\sigma\left(N\left(0,2 * \varepsilon^{\frac{3}{2}} * C\right)\right)
$$

$\sigma$ is the customized sigmoid function. Note we add the sigmoid switch to both the friction and the noise, partially to keep the integrity of the sampler and partially to remove the friction for nearly optimal components. Also, when the friction and noise are removed, the parameter is updated by $\varepsilon^{2} * G_{t}$, i.e. the gradient scaled by $\varepsilon^{2}$ which is much smaller than the learning rate $\varepsilon$, encouraging local search.

Finally, we conducted experiments between our SGHMC and SGLD in 3DGS-MCMC. We found that while SGLD can explore large spaces, SGHMC is better at local exploitation. To achieve the best results, we finally performed a burn-in stage with the friction removed during training for large exploration. In order to maintain the anisotropy of $\Sigma$ of Student's t distribution after the friction is removed, we multiply the noise by $\Sigma$ following 3DGS-MCMC. After the burn-in stage, we add the friction back and restore the noise (no longer multiplied by $\Sigma$ ).

## 7. Representation Limitation

Although we demonstrate the strength of our approach in both qualitative and quantitative evaluations, we do acknowledge that our approach is not perfect in every scenario. We have discussed the limitations of SSS in the main context. For example, the Student's $t$ distribution is limited by symmetric and smooth representation, which makes it difficult to handle sharp shapes perfectly. Student's $t$ distribution combined with negative components can increase the representation ability, but the range of representation is still limited. In addition, although the randomness of SGHMC brings more exploration of space, it still sometimes suffer
from the floating artifact problem commonly observed in 3DGS.